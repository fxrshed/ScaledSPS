{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"colon-cancer\"\n",
    "scale_data = False\n",
    "\n",
    "if dataset_name == \"a9a\":\n",
    "    url_train = f\"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/{dataset_name}\"\n",
    "\n",
    "    trainset = urllib.request.urlopen(url_train)\n",
    "\n",
    "    trainX, trainY = load_svmlight_file(trainset)\n",
    "\n",
    "    # train_data = torch.tensor(normalize(a1a_trainX.toarray(), norm='l2', axis=1), dtype=torch.float, device=device)\n",
    "    train_data = torch.tensor(trainX.toarray(), dtype=torch.float, device=device)\n",
    "    train_target = torch.tensor(trainY, dtype=torch.float, device=device)\n",
    "\n",
    "elif dataset_name == \"colon-cancer\":\n",
    "    # with open(f\"datasets/{dataset_name}\", \"r\") as f_:\n",
    "    #     filelines = f_.readlines()\n",
    "\n",
    "    # targets = []\n",
    "    # dataset = []\n",
    "\n",
    "    # for line in filelines:\n",
    "    #     split_line = line.split(sep=\" \")\n",
    "    #     targets.append(split_line[0])\n",
    "    #     data = []\n",
    "    #     for i in split_line[2:]:\n",
    "    #         data.append(i.split(sep=\":\")[1])\n",
    "    #     dataset.append(data)\n",
    "\n",
    "    # trainX = np.array(dataset, dtype=np.float32)\n",
    "    # trainY = np.array(targets, dtype=np.float32)\n",
    "\n",
    "    trainX, trainY = load_svmlight_file(f\"datasets/{dataset_name}\") \n",
    "\n",
    "\n",
    "    train_data = torch.tensor(trainX.toarray(), dtype=torch.float, device=device)\n",
    "    train_target = torch.tensor(trainY, dtype=torch.float, device=device)\n",
    "\n",
    "elif dataset_name == \"covtype.libsvm.binary.scale\" or dataset_name == \"covtype.libsvm.binary\":\n",
    "    trainX, trainY = load_svmlight_file(f\"datasets/{dataset_name}\")\n",
    "\n",
    "    sample = np.random.choice(trainX.shape[0], round(trainX.shape[0] * 0.05), replace=False)\n",
    "\n",
    "    assert sample.shape == np.unique(sample).shape\n",
    "\n",
    "    trainX = trainX[sample]\n",
    "    trainY = trainY[sample]\n",
    "\n",
    "    train_data = torch.tensor(trainX.toarray(), dtype=torch.float, device=device)\n",
    "    train_target = torch.tensor(trainY, dtype=torch.float, device=device)\n",
    "\n",
    "elif dataset_name == \"mushrooms\":\n",
    "    trainX, trainY = load_svmlight_file(f\"datasets/{dataset_name}\")\n",
    "\n",
    "    train_data = torch.tensor(trainX.toarray(), dtype=torch.float, device=device)\n",
    "    train_target = torch.tensor(trainY, dtype=torch.float, device=device)\n",
    "\n",
    "\n",
    "if scale_data:\n",
    "    r1 = -10\n",
    "    r2 = 10\n",
    "    scaling_vec = (r1 - r2) * torch.rand(train_data.shape[1], device=device) + r2\n",
    "    scaling_vec = torch.pow(torch.e, scaling_vec)\n",
    "    train_data = scaling_vec * train_data\n",
    "\n",
    "train_load = data_utils.TensorDataset(train_data, train_target)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD with $lr=1e^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.zeros(train_data.shape[1], device=device).requires_grad_()\n",
    "hist_sgd = []\n",
    "\n",
    "loss = F(w)\n",
    "g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "\n",
    "hist_sgd.append([loss.item(), (torch.linalg.norm(g) ** 2).item()])\n",
    "\n",
    "\n",
    "for step in range(STEPS):\n",
    "    \n",
    "    for i, (batch_data, batch_target) in enumerate(train_dataloader):\n",
    "        loss = F(w, X=batch_data, y=batch_target)\n",
    "        g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "        precond = eta\n",
    "        w = w - ( precond * g )\n",
    "\n",
    "    print(f\"Loss: {F(w).item()} | GradNorm^2: {(torch.linalg.norm(g) ** 2).item()}\")\n",
    "\n",
    "    hist_sgd.append([F(w).item(), (torch.linalg.norm(g) ** 2).item()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diag_estimate_old(data_shape_cols, grad, weights, iters):\n",
    "    Ds = []\n",
    "    for j in range(iters):\n",
    "        z = rademacher(data_shape_cols).to(device=device)\n",
    "        with torch.no_grad():\n",
    "            hvp = torch.autograd.grad(grad, weights, grad_outputs=z, retain_graph=True)[0]\n",
    "        Ds.append((hvp*z))\n",
    "\n",
    "    return torch.mean(torch.stack(Ds), 0)\n",
    "\n",
    "def diag_estimate(weights, iters):\n",
    "    Ds = []\n",
    "    for j in range(iters):\n",
    "        z = rademacher(weights.shape[0]).to(device=weights.get_device())\n",
    "        with torch.no_grad():\n",
    "            hvp = torch.autograd.grad(weights.grad, weights, grad_outputs=z, retain_graph=True)[0]\n",
    "        Ds.append((hvp*z))\n",
    "\n",
    "    return torch.mean(torch.stack(Ds), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931473016738892 | GradNorm^2: 1.7407530546188354\n",
      "Loss: 0.0 | GradNorm^2: 0.0\n",
      "Loss: 0.0 | GradNorm^2: 0.0\n",
      "Loss: 0.0 | GradNorm^2: 0.0\n",
      "Loss: 0.0 | GradNorm^2: 0.0\n",
      "Loss: 0.0 | GradNorm^2: 0.0\n",
      "Loss: 0.0 | GradNorm^2: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32m/home/farshed.abdukhakimov/projects/sps2/main.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.14/home/farshed.abdukhakimov/projects/sps2/main.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (batch_data, batch_target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.14/home/farshed.abdukhakimov/projects/sps2/main.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m         \n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.14/home/farshed.abdukhakimov/projects/sps2/main.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m# print(f\"Loss: {loss.item()} | GradNorm^2: {(torch.linalg.norm(g) ** 2).item()}\")\u001b[39;00m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.14/home/farshed.abdukhakimov/projects/sps2/main.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.14/home/farshed.abdukhakimov/projects/sps2/main.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39m# if i%1 == 0: print(f\"Accuracy: {np.mean(trainX.dot(w.cpu().data.numpy())*trainY > 0)}\")\u001b[39;00m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.14/home/farshed.abdukhakimov/projects/sps2/main.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     loss \u001b[39m=\u001b[39m F(w, X\u001b[39m=\u001b[39mbatch_data, y\u001b[39m=\u001b[39mbatch_target)\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.14/home/farshed.abdukhakimov/projects/sps2/main.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     g, \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(loss, w, create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.14/home/farshed.abdukhakimov/projects/sps2/main.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     vk \u001b[39m=\u001b[39m diag_estimate_old(w\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], g, w, \u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.14/home/farshed.abdukhakimov/projects/sps2/main.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39m# Smoothing and Truncation        \u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/.conda/envs/sps2/lib/python3.10/site-packages/torch/autograd/__init__.py:275\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n",
      "\u001b[1;32m    273\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs)\n",
      "\u001b[1;32m    274\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 275\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n",
      "\u001b[1;32m    276\u001b[0m         outputs, grad_outputs_, retain_graph, create_graph, inputs,\n",
      "\u001b[1;32m    277\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w = torch.zeros(train_data.shape[1], device=device).requires_grad_()\n",
    "\n",
    "alpha = 1e-1\n",
    "beta = 0.999\n",
    "\n",
    "loss = F(w)\n",
    "g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "\n",
    "hist_sps_d = [[loss.item(), (torch.linalg.norm(g) ** 2).item()]]\n",
    "\n",
    "Dk = diag_estimate_old(w.shape[0], g, w, 100)\n",
    "\n",
    "for step in range(STEPS):\n",
    "    \n",
    "    loss = F(w)\n",
    "    g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "    hist_sps_d.append([F(w).item(), (torch.linalg.norm(g) ** 2).item()])\n",
    "\n",
    "    print(f\"Loss: {loss.item()} | GradNorm^2: {(torch.linalg.norm(g) ** 2).item()}\")\n",
    "\n",
    "    \n",
    "    for i, (batch_data, batch_target) in enumerate(train_dataloader):\n",
    "            \n",
    "        # print(f\"Loss: {loss.item()} | GradNorm^2: {(torch.linalg.norm(g) ** 2).item()}\")\n",
    "\n",
    "        # if i%1 == 0: print(f\"Accuracy: {np.mean(trainX.dot(w.cpu().data.numpy())*trainY > 0)}\")\n",
    "\n",
    "        loss = F(w, X=batch_data, y=batch_target)\n",
    "        g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "\n",
    "        vk = diag_estimate_old(w.shape[0], g, w, 1)\n",
    "\n",
    "        # Smoothing and Truncation        \n",
    "        Dk = beta * Dk + (1 - beta) * vk\n",
    "        Dk_hat = torch.abs(Dk)\n",
    "        Dk_hat[Dk_hat < alpha] = alpha\n",
    "        Dk_hat_inv = 1 / Dk_hat\n",
    "\n",
    "        gnorm = (g * Dk_hat_inv).dot(g)\n",
    "        \n",
    "        if gnorm.item() < 1e-13:\n",
    "            continue\n",
    "        precond = (loss / (gnorm)) * Dk_hat_inv\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            w.sub_(precond * g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SPSD(Optimizer):\n",
    "    def __init__(self, params, preconditioner=\"none\", default_kwargs=None):\n",
    "        defaults = dict()\n",
    "        if default_kwargs:\n",
    "            defaults.update(**default_kwargs)\n",
    "\n",
    "        self.preconditioner = None\n",
    "\n",
    "        if preconditioner == \"hutch\":\n",
    "            self.preconditioner = Hutch()\n",
    "            \n",
    "        self.preconditioner_initialized = False\n",
    "\n",
    "        # TO-DO: Think of something better\n",
    "        self.replay_buffer = []\n",
    "\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        for group in self.param_groups:\n",
    "            weights = list(group[\"params\"])\n",
    "            \n",
    "            loss = closure()\n",
    "            grad = torch.autograd.grad(loss, weights, create_graph=True)\n",
    "            loss = loss.item()\n",
    "\n",
    "            if self.preconditioner:\n",
    "                if not self.preconditioner_initialized:\n",
    "                    self.preconditioner.init(self, grad, 100)\n",
    "                    self.preconditioner_initialized = True\n",
    "                self.preconditioner.step(self, grad, 1)\n",
    "            else:\n",
    "                self.init_empty_precond()\n",
    "                self.preconditioner_initialized = True\n",
    "            \n",
    "            self.update(grad, loss)\n",
    "                          \n",
    "        return loss\n",
    "\n",
    "    def init_empty_precond(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                self.state[p]['Dk'] = torch.ones_like(p) \n",
    "                self.state[p]['DkhatInv'] = torch.ones_like(p)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, grad, loss):\n",
    "        for group in self.param_groups: \n",
    "            gnorm_square = 0.\n",
    "            for p, g in zip(group['params'], grad):\n",
    "                g_detached = g.detach().clone()\n",
    "                self.state[p]['scaled_grad'] = self.state[p]['DkhatInv'].mul(g_detached)\n",
    "                gnorm_sq = self.state[p]['scaled_grad'].mul(g_detached).sum()\n",
    "                gnorm_square += gnorm_sq\n",
    "\n",
    "            if gnorm_square.item() < 1e-13:\n",
    "                return \n",
    "            step_size = loss / gnorm_square\n",
    "\n",
    "            for p in group['params']:\n",
    "                p.sub_(self.state[p]['scaled_grad'].mul(step_size))\n",
    "\n",
    "            self.replay_buffer.append({\n",
    "                \"loss\" : loss,\n",
    "                \"grad_norm_sq\" : gnorm_square.item(),\n",
    "            })\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('sps2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35d98a01b571b9c5855d990661b06f31354de19c91463a2a0e2c023e458877c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
