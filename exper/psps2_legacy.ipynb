{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farshed.abdukhakimov/.conda/envs/sps2/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import optuna \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "from datasets import get_dataset\n",
    "from loss_fns import get_loss\n",
    "from utils import solve\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_reg(w, X, y):\n",
    "    return torch.mean(torch.log(1 + torch.exp(-y * (X @ w))))\n",
    "\n",
    "def nllsq(w, X, y):\n",
    "    return torch.mean( ( y - (1/(1 + torch.exp(-X @ w ))) )**2 )\n",
    "\n",
    "def rademacher_old(weights):\n",
    "    return torch.round(torch.rand_like(weights)) * 2 - 1\n",
    "\n",
    "def diag_estimate_old(weights, grad, iters):\n",
    "    Ds = []\n",
    "    for j in range(iters):\n",
    "        z = rademacher_old(weights)\n",
    "        with torch.no_grad():\n",
    "            hvp = torch.autograd.grad(grad, weights, grad_outputs=z, retain_graph=True)[0]\n",
    "        Ds.append((hvp*z))\n",
    "\n",
    "    return torch.mean(torch.stack(Ds), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/farshed.abdukhakimov/projects/sps2/datasets\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "batch_size = 64\n",
    "dataset_name = \"mushrooms\"\n",
    "percentage = 1.0\n",
    "\n",
    "# training \n",
    "STEPS = 300\n",
    "loss_name = \"logreg\"\n",
    "loss_class = get_loss(loss_name)\n",
    "gamma = 1.0 \n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "scale_k = 0\n",
    "scale_range = [-scale_k, scale_k] # [-value, value]\n",
    "train_data, train_target = get_dataset(dataset_name, batch_size, percentage, scale_range, loss_class.y_range)\n",
    "train_data = train_data.to(torch.get_default_dtype())\n",
    "train_target = train_target.to(torch.get_default_dtype())\n",
    "train_load = data_utils.TensorDataset(train_data, train_target)\n",
    "train_dataloader = DataLoader(train_load, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931471805599453 | GradNorm^2: 0.3195669607542947\n",
      "Loss: 0.16668382211443877 | GradNorm^2: 0.006134417731569834\n",
      "Loss: 0.12045682781964252 | GradNorm^2: 0.002328108859702026\n",
      "Loss: 0.09966336687539232 | GradNorm^2: 0.0012205034950301488\n",
      "Loss: 0.08716118420808967 | GradNorm^2: 0.0009358335916332891\n",
      "Loss: 0.07812815748680701 | GradNorm^2: 0.0006071497872857561\n",
      "Loss: 0.07126480284284004 | GradNorm^2: 0.00048009678411869993\n",
      "Loss: 0.06573379161270573 | GradNorm^2: 0.00039824453669382704\n",
      "Loss: 0.06111803873864498 | GradNorm^2: 0.00033293426528437764\n",
      "Loss: 0.057195072820730816 | GradNorm^2: 0.00028907912910852804\n",
      "Loss: 0.05379499479938606 | GradNorm^2: 0.0002493395444248656\n",
      "Loss: 0.05082921393916707 | GradNorm^2: 0.000224283205772718\n",
      "Loss: 0.04822257353335311 | GradNorm^2: 0.00021325320697692442\n",
      "Loss: 0.04586555172921916 | GradNorm^2: 0.00018325201400722275\n",
      "Loss: 0.04374984939160306 | GradNorm^2: 0.00015959331431272022\n",
      "Loss: 0.04184138103814873 | GradNorm^2: 0.0001433361864275968\n",
      "Loss: 0.04011057501942087 | GradNorm^2: 0.00013133404284409604\n",
      "Loss: 0.03852726794642761 | GradNorm^2: 0.00012100029799427383\n",
      "Loss: 0.037134565856609694 | GradNorm^2: 0.00014489979833534546\n",
      "Loss: 0.03574153773749705 | GradNorm^2: 0.00010870013632005853\n",
      "Loss: 0.034486821244687244 | GradNorm^2: 9.423580907666907e-05\n",
      "Loss: 0.033363247753750026 | GradNorm^2: 0.00010228253647139394\n",
      "Loss: 0.03225944936400451 | GradNorm^2: 8.190806107834311e-05\n",
      "Loss: 0.03126625649362308 | GradNorm^2: 8.163225901905706e-05\n",
      "Loss: 0.030319207498242014 | GradNorm^2: 7.370306801975862e-05\n",
      "Loss: 0.029456646421927368 | GradNorm^2: 7.812267139739895e-05\n",
      "Loss: 0.028601916741778654 | GradNorm^2: 6.360986642257472e-05\n",
      "Loss: 0.02781848923791579 | GradNorm^2: 5.9976019407976775e-05\n",
      "Loss: 0.027091234607290054 | GradNorm^2: 6.199905939187496e-05\n",
      "Loss: 0.026378843500708755 | GradNorm^2: 5.39516118660042e-05\n",
      "Loss: 0.025735191072691104 | GradNorm^2: 5.8816349596814965e-05\n",
      "Loss: 0.02508746344688773 | GradNorm^2: 4.9248363848915454e-05\n",
      "Loss: 0.02448683246806428 | GradNorm^2: 4.620568356841881e-05\n",
      "Loss: 0.02392776411744963 | GradNorm^2: 4.816649653883329e-05\n",
      "Loss: 0.023376431949190615 | GradNorm^2: 4.317455373798329e-05\n",
      "Loss: 0.022853816640200985 | GradNorm^2: 3.984368258128461e-05\n",
      "Loss: 0.022358778685526288 | GradNorm^2: 3.811560351799694e-05\n",
      "Loss: 0.021885141931091666 | GradNorm^2: 3.6474599219427844e-05\n",
      "Loss: 0.021438759072402 | GradNorm^2: 3.73261961259833e-05\n",
      "Loss: 0.020997831609930732 | GradNorm^2: 3.371735232845692e-05\n",
      "Loss: 0.02058157060362816 | GradNorm^2: 3.256133911953221e-05\n",
      "Loss: 0.020190004352450074 | GradNorm^2: 3.413644731020182e-05\n",
      "Loss: 0.01979656768618261 | GradNorm^2: 3.0181008511662727e-05\n",
      "Loss: 0.019432548561473396 | GradNorm^2: 3.097963039784241e-05\n",
      "Loss: 0.019068999601266883 | GradNorm^2: 2.761539312473244e-05\n",
      "Loss: 0.01872646201269815 | GradNorm^2: 2.6772623449252978e-05\n",
      "Loss: 0.01839486452382083 | GradNorm^2: 2.5621047566713437e-05\n",
      "Loss: 0.018076547222411282 | GradNorm^2: 2.5012046114295898e-05\n",
      "Loss: 0.01776735808251667 | GradNorm^2: 2.3856617511873212e-05\n",
      "Loss: 0.017470579808519432 | GradNorm^2: 2.3367189572081653e-05\n",
      "Loss: 0.017183291430190404 | GradNorm^2: 2.2756336859673077e-05\n",
      "Loss: 0.016903113768025495 | GradNorm^2: 2.1532207857342222e-05\n",
      "Loss: 0.016633724714912396 | GradNorm^2: 2.0865138561740055e-05\n",
      "Loss: 0.016375672605527664 | GradNorm^2: 2.101101171258178e-05\n",
      "Loss: 0.016119952165028636 | GradNorm^2: 1.960142068336964e-05\n",
      "Loss: 0.015874757811813262 | GradNorm^2: 1.8983901828529613e-05\n",
      "Loss: 0.015637395457517342 | GradNorm^2: 1.8504916715077466e-05\n",
      "Loss: 0.015408033026246567 | GradNorm^2: 1.8330500482735478e-05\n",
      "Loss: 0.015185003032478335 | GradNorm^2: 1.8102990059951103e-05\n",
      "Loss: 0.014967910337256538 | GradNorm^2: 1.7745986333641654e-05\n",
      "Loss: 0.014754166931014804 | GradNorm^2: 1.675240302711361e-05\n",
      "Loss: 0.014547458797062469 | GradNorm^2: 1.6052802222109046e-05\n",
      "Loss: 0.014347052514297536 | GradNorm^2: 1.551301037312537e-05\n",
      "Loss: 0.014153010320665283 | GradNorm^2: 1.5252466571115197e-05\n",
      "Loss: 0.013962982120463882 | GradNorm^2: 1.4702180486010866e-05\n",
      "Loss: 0.01378538009517189 | GradNorm^2: 1.5976113220392826e-05\n",
      "Loss: 0.013598808770021386 | GradNorm^2: 1.3995448520469718e-05\n",
      "Loss: 0.013428419578400873 | GradNorm^2: 1.4766242665108198e-05\n",
      "Loss: 0.013254776730391639 | GradNorm^2: 1.3780336350582183e-05\n",
      "Loss: 0.01308608629135793 | GradNorm^2: 1.2955051902605566e-05\n",
      "Loss: 0.01292373321458253 | GradNorm^2: 1.2651854942672282e-05\n",
      "Loss: 0.012765138166904565 | GradNorm^2: 1.2329889929518485e-05\n",
      "Loss: 0.01261022336818639 | GradNorm^2: 1.2031038974498009e-05\n",
      "Loss: 0.012459030490374369 | GradNorm^2: 1.174668809583068e-05\n",
      "Loss: 0.01231255055351086 | GradNorm^2: 1.1713580434974728e-05\n",
      "Loss: 0.012167457161319992 | GradNorm^2: 1.1269609935343897e-05\n",
      "Loss: 0.012026348438793993 | GradNorm^2: 1.0989149678583033e-05\n",
      "Loss: 0.011893435800440121 | GradNorm^2: 1.1756448806287548e-05\n",
      "Loss: 0.01175409602756575 | GradNorm^2: 1.054348569256166e-05\n",
      "Loss: 0.011622056046225644 | GradNorm^2: 1.0249866173194411e-05\n",
      "Loss: 0.011493157672891151 | GradNorm^2: 1.002699621735136e-05\n",
      "Loss: 0.011367257587682103 | GradNorm^2: 9.859035541876496e-06\n",
      "Loss: 0.011243559737331373 | GradNorm^2: 9.606043372348731e-06\n",
      "Loss: 0.011122975175277456 | GradNorm^2: 9.46037752536678e-06\n",
      "Loss: 0.011004355636263192 | GradNorm^2: 9.210600090898529e-06\n",
      "Loss: 0.010888846796182789 | GradNorm^2: 9.10442591405706e-06\n",
      "Loss: 0.010775101659639361 | GradNorm^2: 8.888094675050225e-06\n",
      "Loss: 0.010664142996577938 | GradNorm^2: 8.765809515116043e-06\n",
      "Loss: 0.010554801487015486 | GradNorm^2: 8.531990558609386e-06\n",
      "Loss: 0.010448110236011342 | GradNorm^2: 8.403972012109115e-06\n",
      "Loss: 0.010342848696550264 | GradNorm^2: 8.160772955054498e-06\n",
      "Loss: 0.010241273932578296 | GradNorm^2: 8.20985731918721e-06\n",
      "Loss: 0.010140863883582485 | GradNorm^2: 8.119190181333882e-06\n",
      "Loss: 0.010040825700934088 | GradNorm^2: 7.754841276167013e-06\n",
      "Loss: 0.009943681425549154 | GradNorm^2: 7.585407999465441e-06\n",
      "Loss: 0.009848399215920619 | GradNorm^2: 7.429515718157851e-06\n",
      "Loss: 0.0097574696455765 | GradNorm^2: 7.736402665680423e-06\n",
      "Loss: 0.009663426510952899 | GradNorm^2: 7.183473318757238e-06\n",
      "Loss: 0.009573888021018611 | GradNorm^2: 7.136280225210411e-06\n",
      "Loss: 0.009484836469895041 | GradNorm^2: 6.896792241710465e-06\n",
      "Loss: 0.009397875610149667 | GradNorm^2: 6.767509991025919e-06\n",
      "Loss: 0.009313474066600559 | GradNorm^2: 6.796381169260244e-06\n",
      "Loss: 0.009230625413394897 | GradNorm^2: 6.840744519178203e-06\n",
      "Loss: 0.009147564910737213 | GradNorm^2: 6.595666139213445e-06\n",
      "Loss: 0.00906576887954172 | GradNorm^2: 6.329512968082909e-06\n",
      "Loss: 0.008986205169734458 | GradNorm^2: 6.216235843925206e-06\n",
      "Loss: 0.008907969090144897 | GradNorm^2: 6.098012118191936e-06\n",
      "Loss: 0.008831583707531173 | GradNorm^2: 6.066207728857257e-06\n",
      "Loss: 0.008756126606952753 | GradNorm^2: 5.985312166019595e-06\n",
      "Loss: 0.008681355817102 | GradNorm^2: 5.80921737619083e-06\n",
      "Loss: 0.008608494602187601 | GradNorm^2: 5.749160970200744e-06\n",
      "Loss: 0.008536718625606666 | GradNorm^2: 5.682013381415477e-06\n",
      "Loss: 0.008465936281347076 | GradNorm^2: 5.576397607431572e-06\n",
      "Loss: 0.008396485148675976 | GradNorm^2: 5.505994061357196e-06\n",
      "Loss: 0.008327550453231136 | GradNorm^2: 5.343545262666924e-06\n",
      "Loss: 0.008260202800510413 | GradNorm^2: 5.260692988729055e-06\n",
      "Loss: 0.008193862466135455 | GradNorm^2: 5.174435335407943e-06\n",
      "Loss: 0.008128829813804968 | GradNorm^2: 5.128899626554598e-06\n",
      "Loss: 0.008064891275973703 | GradNorm^2: 5.093706241393873e-06\n",
      "Loss: 0.008001925784827295 | GradNorm^2: 5.059430634232948e-06\n",
      "Loss: 0.00793891719192533 | GradNorm^2: 4.87372055490841e-06\n",
      "Loss: 0.007877491685194787 | GradNorm^2: 4.788737597956087e-06\n",
      "Loss: 0.00781706866425056 | GradNorm^2: 4.716142265392712e-06\n",
      "Loss: 0.007757573349380884 | GradNorm^2: 4.6460852560366225e-06\n",
      "Loss: 0.007699960942514644 | GradNorm^2: 4.7213860257340504e-06\n",
      "Loss: 0.007641221467034519 | GradNorm^2: 4.511026059768164e-06\n",
      "Loss: 0.007584266160541872 | GradNorm^2: 4.443408245889151e-06\n",
      "Loss: 0.0075284426042038655 | GradNorm^2: 4.416111866348717e-06\n",
      "Loss: 0.007472899325751391 | GradNorm^2: 4.315658195975453e-06\n",
      "Loss: 0.007418585880796969 | GradNorm^2: 4.27491384806808e-06\n",
      "Loss: 0.00736567324732985 | GradNorm^2: 4.323740547439707e-06\n",
      "Loss: 0.007311910696829608 | GradNorm^2: 4.14545855902026e-06\n",
      "Loss: 0.007259649793710015 | GradNorm^2: 4.076991863185212e-06\n",
      "Loss: 0.007208554599141396 | GradNorm^2: 4.068066387697071e-06\n",
      "Loss: 0.007157423362207928 | GradNorm^2: 3.965399744688618e-06\n",
      "Loss: 0.0071073746342431195 | GradNorm^2: 3.909245397904056e-06\n",
      "Loss: 0.00705809229713789 | GradNorm^2: 3.865125537377344e-06\n",
      "Loss: 0.007009680462862542 | GradNorm^2: 3.849593166384258e-06\n",
      "Loss: 0.006961861898873506 | GradNorm^2: 3.825742812693109e-06\n",
      "Loss: 0.0069139419812337365 | GradNorm^2: 3.7015641536099117e-06\n",
      "Loss: 0.006867569879739404 | GradNorm^2: 3.7024987692087265e-06\n",
      "Loss: 0.0068210963001686265 | GradNorm^2: 3.6066204023488946e-06\n",
      "Loss: 0.006775563242820809 | GradNorm^2: 3.5573927992136576e-06\n",
      "Loss: 0.006730819571797662 | GradNorm^2: 3.533841741833956e-06\n",
      "Loss: 0.006686609951091975 | GradNorm^2: 3.5024051546273507e-06\n",
      "Loss: 0.006642576457436283 | GradNorm^2: 3.420281199410137e-06\n",
      "Loss: 0.006599669001929704 | GradNorm^2: 3.4121883636180956e-06\n",
      "Loss: 0.006556827593221866 | GradNorm^2: 3.3432098156592218e-06\n",
      "Loss: 0.006515359489562148 | GradNorm^2: 3.3780251861795424e-06\n",
      "Loss: 0.006474336267696389 | GradNorm^2: 3.401392544205612e-06\n",
      "Loss: 0.006433265489384743 | GradNorm^2: 3.3553054721608213e-06\n",
      "Loss: 0.00639163564797038 | GradNorm^2: 3.176039907382848e-06\n",
      "Loss: 0.006351559702126417 | GradNorm^2: 3.1296530467065436e-06\n",
      "Loss: 0.00631202515851929 | GradNorm^2: 3.0904441680151943e-06\n",
      "Loss: 0.00627303960384141 | GradNorm^2: 3.0595405757988016e-06\n",
      "Loss: 0.006234400337886478 | GradNorm^2: 3.0153039151657513e-06\n",
      "Loss: 0.006196303111046754 | GradNorm^2: 2.9788419719781596e-06\n",
      "Loss: 0.006158890357003543 | GradNorm^2: 2.9701516440215166e-06\n",
      "Loss: 0.006121589562587338 | GradNorm^2: 2.9265905475381126e-06\n",
      "Loss: 0.006084698343515773 | GradNorm^2: 2.8732218588950535e-06\n",
      "Loss: 0.006048399384253128 | GradNorm^2: 2.841577985107869e-06\n",
      "Loss: 0.006012498248071231 | GradNorm^2: 2.805895328414632e-06\n",
      "Loss: 0.005977125183397146 | GradNorm^2: 2.7823071338167523e-06\n",
      "Loss: 0.005942001791525623 | GradNorm^2: 2.7418766315253303e-06\n",
      "Loss: 0.005907587121296777 | GradNorm^2: 2.735820756726818e-06\n",
      "Loss: 0.00587311534789345 | GradNorm^2: 2.6788008879492093e-06\n",
      "Loss: 0.0058392574010999445 | GradNorm^2: 2.6477464704433835e-06\n",
      "Loss: 0.005805813791364761 | GradNorm^2: 2.6214392324279088e-06\n",
      "Loss: 0.0057727440264436596 | GradNorm^2: 2.594100421353839e-06\n",
      "Loss: 0.005740090108944738 | GradNorm^2: 2.571242782125724e-06\n",
      "Loss: 0.005707685619769885 | GradNorm^2: 2.5361755490179716e-06\n",
      "Loss: 0.005675641616962799 | GradNorm^2: 2.5022838255693538e-06\n",
      "Loss: 0.005644011424967291 | GradNorm^2: 2.474683762767905e-06\n",
      "Loss: 0.005613209276625229 | GradNorm^2: 2.499526105525248e-06\n",
      "Loss: 0.005581837684367175 | GradNorm^2: 2.423200476204528e-06\n",
      "Loss: 0.005551493629891346 | GradNorm^2: 2.4242343166245767e-06\n",
      "Loss: 0.005521309921534045 | GradNorm^2: 2.4062677600622346e-06\n",
      "Loss: 0.005491399600358659 | GradNorm^2: 2.3837039672442956e-06\n",
      "Loss: 0.005461495575989039 | GradNorm^2: 2.326765628369968e-06\n",
      "Loss: 0.005432186916728023 | GradNorm^2: 2.3009025673534037e-06\n",
      "Loss: 0.005403366789120389 | GradNorm^2: 2.294743625418031e-06\n",
      "Loss: 0.00537455295131731 | GradNorm^2: 2.2560032228644997e-06\n",
      "Loss: 0.00534613750782818 | GradNorm^2: 2.2274928699109087e-06\n",
      "Loss: 0.005317994523948297 | GradNorm^2: 2.19776849635662e-06\n",
      "Loss: 0.005290258881285039 | GradNorm^2: 2.1805030601675654e-06\n",
      "Loss: 0.005262722057360225 | GradNorm^2: 2.154938878232228e-06\n",
      "Loss: 0.005235587239241453 | GradNorm^2: 2.1418233837896564e-06\n",
      "Loss: 0.005208804801918953 | GradNorm^2: 2.1363614445898122e-06\n",
      "Loss: 0.0051819399006859425 | GradNorm^2: 2.0951768084236342e-06\n",
      "Loss: 0.005155477652081457 | GradNorm^2: 2.0666826378406824e-06\n",
      "Loss: 0.005129359541526058 | GradNorm^2: 2.04713628967582e-06\n",
      "Loss: 0.005103487498477036 | GradNorm^2: 2.02610565630952e-06\n",
      "Loss: 0.005078001373202035 | GradNorm^2: 2.017813991916357e-06\n",
      "Loss: 0.005052670812429926 | GradNorm^2: 2.00064209305942e-06\n",
      "Loss: 0.005027489453677565 | GradNorm^2: 1.9731814918268065e-06\n",
      "Loss: 0.005002612844541963 | GradNorm^2: 1.950265889557729e-06\n",
      "Loss: 0.004977954391521392 | GradNorm^2: 1.9265257052470245e-06\n",
      "Loss: 0.004953672098002672 | GradNorm^2: 1.916386625652584e-06\n",
      "Loss: 0.004929495559094215 | GradNorm^2: 1.8925938322466136e-06\n",
      "Loss: 0.004905672005865458 | GradNorm^2: 1.8843103534539956e-06\n",
      "Loss: 0.004881954332085889 | GradNorm^2: 1.8606604230537205e-06\n",
      "Loss: 0.004858666046400235 | GradNorm^2: 1.85706951988274e-06\n",
      "Loss: 0.004835329192294262 | GradNorm^2: 1.8286011329023137e-06\n",
      "Loss: 0.004812234472211095 | GradNorm^2: 1.801538498548086e-06\n",
      "Loss: 0.00478943255343368 | GradNorm^2: 1.7842491513391894e-06\n",
      "Loss: 0.004766867356645298 | GradNorm^2: 1.7670635689833216e-06\n",
      "Loss: 0.004744557236701437 | GradNorm^2: 1.7539184317356529e-06\n",
      "Loss: 0.004722441090442917 | GradNorm^2: 1.7394361835194674e-06\n",
      "Loss: 0.004700446362415072 | GradNorm^2: 1.717807413964188e-06\n",
      "Loss: 0.004678744769480613 | GradNorm^2: 1.7047630393889343e-06\n",
      "Loss: 0.004657224958423947 | GradNorm^2: 1.6902491097442856e-06\n",
      "Loss: 0.004635868501540606 | GradNorm^2: 1.6722612965804728e-06\n",
      "Loss: 0.004614886741978259 | GradNorm^2: 1.671858484002636e-06\n",
      "Loss: 0.004593871703887793 | GradNorm^2: 1.6504106457966034e-06\n",
      "Loss: 0.004573044759106142 | GradNorm^2: 1.6295251536781351e-06\n",
      "Loss: 0.004552432357868341 | GradNorm^2: 1.611254155153127e-06\n",
      "Loss: 0.004532090920184902 | GradNorm^2: 1.6014706509677649e-06\n",
      "Loss: 0.004511888075473613 | GradNorm^2: 1.5871573445212008e-06\n",
      "Loss: 0.004491911621947409 | GradNorm^2: 1.5787627712675834e-06\n",
      "Loss: 0.004471967407259638 | GradNorm^2: 1.5561341502465404e-06\n",
      "Loss: 0.004452254892935728 | GradNorm^2: 1.5411019948052573e-06\n",
      "Loss: 0.0044327717725570264 | GradNorm^2: 1.5290956335705847e-06\n",
      "Loss: 0.0044134559267379435 | GradNorm^2: 1.5172684735084785e-06\n",
      "Loss: 0.0043942652438568605 | GradNorm^2: 1.5013840509111691e-06\n",
      "Loss: 0.004375291795104513 | GradNorm^2: 1.49022947459285e-06\n",
      "Loss: 0.004356447724878219 | GradNorm^2: 1.4771596832340244e-06\n",
      "Loss: 0.004337901791788727 | GradNorm^2: 1.4760776264807925e-06\n",
      "Loss: 0.004319272816256132 | GradNorm^2: 1.4534970514675187e-06\n",
      "Loss: 0.004300887643017259 | GradNorm^2: 1.4395052080368394e-06\n",
      "Loss: 0.004282679834065303 | GradNorm^2: 1.4264553299039297e-06\n",
      "Loss: 0.0042646246509660335 | GradNorm^2: 1.4138837195962435e-06\n",
      "Loss: 0.004246756586338025 | GradNorm^2: 1.4042164786360371e-06\n",
      "Loss: 0.00422904807133145 | GradNorm^2: 1.3958840737164262e-06\n",
      "Loss: 0.004211430587986819 | GradNorm^2: 1.3820599352315266e-06\n",
      "Loss: 0.004194009733406177 | GradNorm^2: 1.3735663942920785e-06\n",
      "Loss: 0.0041766820205513255 | GradNorm^2: 1.360628015695181e-06\n",
      "Loss: 0.004159467124358828 | GradNorm^2: 1.3449766080493688e-06\n",
      "Loss: 0.004142477029726092 | GradNorm^2: 1.3364255533403904e-06\n",
      "Loss: 0.004125573497888178 | GradNorm^2: 1.3239232282698682e-06\n",
      "Loss: 0.004108910319364502 | GradNorm^2: 1.3202122829238658e-06\n",
      "Loss: 0.004092295406344932 | GradNorm^2: 1.309347185104878e-06\n",
      "Loss: 0.004075731990884568 | GradNorm^2: 1.2912254990348997e-06\n",
      "Loss: 0.00405944826995766 | GradNorm^2: 1.2857841412598515e-06\n",
      "Loss: 0.004043193872809337 | GradNorm^2: 1.271971450319945e-06\n",
      "Loss: 0.0040270994549292775 | GradNorm^2: 1.260777545523764e-06\n",
      "Loss: 0.004011153130083865 | GradNorm^2: 1.251678247081782e-06\n",
      "Loss: 0.003995329719489122 | GradNorm^2: 1.2421915633201686e-06\n",
      "Loss: 0.003979617841842856 | GradNorm^2: 1.2318377957428009e-06\n",
      "Loss: 0.0039641115843242055 | GradNorm^2: 1.228954019924016e-06\n",
      "Loss: 0.0039486352246257085 | GradNorm^2: 1.2178656447633869e-06\n",
      "Loss: 0.003933255497401958 | GradNorm^2: 1.205273018040992e-06\n",
      "Loss: 0.003918038762856006 | GradNorm^2: 1.1971693988226533e-06\n",
      "Loss: 0.0039029023486739645 | GradNorm^2: 1.185673691545703e-06\n",
      "Loss: 0.0038878983892038214 | GradNorm^2: 1.175188749923028e-06\n",
      "Loss: 0.0038730225615478363 | GradNorm^2: 1.1661428847033233e-06\n",
      "Loss: 0.003858267885362105 | GradNorm^2: 1.157818673799813e-06\n",
      "Loss: 0.0038436127930382255 | GradNorm^2: 1.1483663760717902e-06\n",
      "Loss: 0.003829097970712 | GradNorm^2: 1.1414462416692025e-06\n",
      "Loss: 0.003814646603220029 | GradNorm^2: 1.1307411379216626e-06\n",
      "Loss: 0.0038003255418339915 | GradNorm^2: 1.1229067388266361e-06\n",
      "Loss: 0.0037861157504359495 | GradNorm^2: 1.114530978642869e-06\n",
      "Loss: 0.0037721313191970054 | GradNorm^2: 1.1151753352969934e-06\n",
      "Loss: 0.00375802572709925 | GradNorm^2: 1.0983023114996092e-06\n",
      "Loss: 0.0037441226609953924 | GradNorm^2: 1.0894476403498404e-06\n",
      "Loss: 0.003730326374712889 | GradNorm^2: 1.081228452857552e-06\n",
      "Loss: 0.003716650864728858 | GradNorm^2: 1.074363040260292e-06\n",
      "Loss: 0.0037030492719841725 | GradNorm^2: 1.0658328546419798e-06\n",
      "Loss: 0.0036895567262380928 | GradNorm^2: 1.0576013098380741e-06\n",
      "Loss: 0.003676183460508496 | GradNorm^2: 1.0512398426216429e-06\n",
      "Loss: 0.0036628730245821368 | GradNorm^2: 1.0425263832266616e-06\n",
      "Loss: 0.0036496801097345923 | GradNorm^2: 1.0352179979174381e-06\n",
      "Loss: 0.003636562884142246 | GradNorm^2: 1.0276359607918751e-06\n",
      "Loss: 0.0036235534132386983 | GradNorm^2: 1.0205136335129314e-06\n",
      "Loss: 0.0036106661387486786 | GradNorm^2: 1.0151804202410048e-06\n",
      "Loss: 0.0035978327180218386 | GradNorm^2: 1.0072860736190856e-06\n",
      "Loss: 0.0035850778003735395 | GradNorm^2: 9.983857920601361e-07\n",
      "Loss: 0.0035724515690476574 | GradNorm^2: 9.92389373109909e-07\n",
      "Loss: 0.003559881328270749 | GradNorm^2: 9.84342312977893e-07\n",
      "Loss: 0.0035474242333257354 | GradNorm^2: 9.780268224506924e-07\n",
      "Loss: 0.0035350402621641326 | GradNorm^2: 9.705983821699791e-07\n",
      "Loss: 0.0035227554406770385 | GradNorm^2: 9.643540693316326e-07\n",
      "Loss: 0.003510547646645214 | GradNorm^2: 9.575723335490479e-07\n",
      "Loss: 0.003498421050846875 | GradNorm^2: 9.508757299196708e-07\n",
      "Loss: 0.0034863895817111934 | GradNorm^2: 9.447731147003689e-07\n",
      "Loss: 0.0034744207167714553 | GradNorm^2: 9.375288807400781e-07\n",
      "Loss: 0.003462550923668083 | GradNorm^2: 9.311497372417133e-07\n",
      "Loss: 0.0034507686572651164 | GradNorm^2: 9.252890822971432e-07\n",
      "Loss: 0.003439057355908435 | GradNorm^2: 9.196042817745143e-07\n",
      "Loss: 0.003427406579268201 | GradNorm^2: 9.12185756711498e-07\n",
      "Loss: 0.0034158557242322243 | GradNorm^2: 9.061573844822391e-07\n",
      "Loss: 0.003404376425098721 | GradNorm^2: 8.999220037469104e-07\n",
      "Loss: 0.0033929859186543647 | GradNorm^2: 8.944360652387671e-07\n",
      "Loss: 0.003381670992155301 | GradNorm^2: 8.889925631228909e-07\n",
      "Loss: 0.003370429830309963 | GradNorm^2: 8.832657680275528e-07\n",
      "Loss: 0.0033592452358139286 | GradNorm^2: 8.763898071343415e-07\n",
      "Loss: 0.00334816428479191 | GradNorm^2: 8.724222161395418e-07\n",
      "Loss: 0.003337122488515189 | GradNorm^2: 8.653432714708208e-07\n",
      "Loss: 0.0033261633771976967 | GradNorm^2: 8.588934096889055e-07\n",
      "Loss: 0.00331530400381173 | GradNorm^2: 8.546017302301877e-07\n",
      "Loss: 0.003304477067998231 | GradNorm^2: 8.476979640549762e-07\n",
      "Loss: 0.0032937399898106765 | GradNorm^2: 8.422660656282232e-07\n"
     ]
    }
   ],
   "source": [
    "# study_name = f\"sgd-{dataset_name}-{batch_size}-{percentage}-{STEPS}-{loss_name}-{scale_k}\"\n",
    "# storage_name = \"sqlite:///optuna_results/{}.db\".format(study_name)\n",
    "# study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "# lr = study.best_params[\"lr\"]\n",
    "lr = 0.1\n",
    "\n",
    "# parameters\n",
    "w = torch.zeros(train_data.shape[1], device=device).requires_grad_()\n",
    "optimizer = SGD([w], lr=lr)\n",
    "loss_function = loss_class(w)\n",
    "\n",
    "# save loss and grad size to history\n",
    "hist_sgd = []\n",
    "loss = loss_function(train_data.to(device), train_target.to(device))\n",
    "g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "print(f\"Loss: {loss.item()} | GradNorm^2: {(torch.linalg.norm(g) ** 2 ).item()}\")\n",
    "hist_sgd.append([loss.item(), (torch.linalg.norm(g) ** 2).item()])\n",
    "\n",
    "for step in range(STEPS):\n",
    "    for i, (batch_data, batch_target) in enumerate(train_dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_target = batch_target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(batch_data, batch_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss = loss_function(train_data.to(device), train_target.to(device))\n",
    "    g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "    print(f\"Loss: {loss.item()} | GradNorm^2: {(torch.linalg.norm(g) ** 2 ).item()}\")\n",
    "    hist_sgd.append([loss.item(), (torch.linalg.norm(g) ** 2).item()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931471805599453 | GradNorm^2: 0.3195669607542947\n",
      "Loss: 0.007451683507338987 | GradNorm^2: 6.77209635151883e-05\n",
      "Loss: 0.0030237306847070086 | GradNorm^2: 2.0378028737299437e-06\n",
      "Loss: 0.0017588199343609911 | GradNorm^2: 9.738784772443405e-07\n",
      "Loss: 0.0011627772543538863 | GradNorm^2: 1.818614540107884e-07\n",
      "Loss: 0.0008308262710892498 | GradNorm^2: 6.811354814526095e-08\n",
      "Loss: 0.0006436656414065681 | GradNorm^2: 1.2197930804677897e-07\n",
      "Loss: 0.0005216601601498042 | GradNorm^2: 2.1559517620680685e-07\n",
      "Loss: 0.00041703377114478814 | GradNorm^2: 8.872391719205328e-08\n",
      "Loss: 0.0003414416628798521 | GradNorm^2: 1.7086761118074086e-08\n",
      "Loss: 0.00029295256507511883 | GradNorm^2: 4.7659498759754054e-08\n",
      "Loss: 0.00024767622735338623 | GradNorm^2: 2.1550591778265113e-08\n",
      "Loss: 0.00021140142124728684 | GradNorm^2: 4.487284698447492e-09\n",
      "Loss: 0.00018944216949432044 | GradNorm^2: 2.3665297665058975e-08\n",
      "Loss: 0.00016286925342249157 | GradNorm^2: 4.656227761564756e-09\n",
      "Loss: 0.00014669636521912784 | GradNorm^2: 1.3575485186728253e-08\n",
      "Loss: 0.00012813599090740485 | GradNorm^2: 2.8789272973957282e-09\n",
      "Loss: 0.00011459491846195448 | GradNorm^2: 1.551470432083856e-09\n",
      "Loss: 0.00010342838048137148 | GradNorm^2: 2.7113633172243797e-09\n",
      "Loss: 9.26410686825272e-05 | GradNorm^2: 1.0037739739110955e-09\n",
      "Loss: 8.386842211326943e-05 | GradNorm^2: 7.911958692639096e-10\n",
      "Loss: 7.832224486066782e-05 | GradNorm^2: 4.6729598049693615e-09\n",
      "Loss: 7.006659247439019e-05 | GradNorm^2: 1.6832708733146877e-09\n",
      "Loss: 6.422616079044055e-05 | GradNorm^2: 1.8764984264909577e-09\n",
      "Loss: 5.783870147526038e-05 | GradNorm^2: 3.172704763629912e-10\n",
      "Loss: 5.301132336564358e-05 | GradNorm^2: 3.5833255594213804e-10\n",
      "Loss: 4.932013380212231e-05 | GradNorm^2: 9.784339462977934e-10\n",
      "Loss: 4.483753327403702e-05 | GradNorm^2: 1.8949138131079763e-10\n",
      "Loss: 4.180023306491828e-05 | GradNorm^2: 5.696797298172108e-10\n",
      "Loss: 3.81022785491545e-05 | GradNorm^2: 1.4886496178718245e-10\n",
      "Loss: 3.5344262452463014e-05 | GradNorm^2: 2.2383919350202467e-10\n",
      "Loss: 3.279184597256031e-05 | GradNorm^2: 2.583356726219449e-10\n",
      "Loss: 3.0191228490879306e-05 | GradNorm^2: 9.546578318215614e-11\n",
      "Loss: 2.7950343395124207e-05 | GradNorm^2: 7.352580307441368e-11\n",
      "Loss: 2.6087078252214397e-05 | GradNorm^2: 1.676375061239823e-10\n",
      "Loss: 2.4180425424709155e-05 | GradNorm^2: 1.137161441162265e-10\n",
      "Loss: 2.2414842053263166e-05 | GradNorm^2: 6.264974723758875e-11\n",
      "Loss: 2.0954796772098223e-05 | GradNorm^2: 9.007996352909784e-11\n",
      "Loss: 1.966229209213057e-05 | GradNorm^2: 1.6819238368768842e-10\n",
      "Loss: 1.8137296873351625e-05 | GradNorm^2: 6.922050109139099e-11\n",
      "Loss: 1.699141644766578e-05 | GradNorm^2: 1.0217233058559983e-10\n",
      "Loss: 1.5711586189459575e-05 | GradNorm^2: 2.7932570110600017e-11\n",
      "Loss: 1.4671338078373211e-05 | GradNorm^2: 2.9099440609126638e-11\n",
      "Loss: 1.3801429384658026e-05 | GradNorm^2: 6.077892279534169e-11\n",
      "Loss: 1.2746971573435572e-05 | GradNorm^2: 1.5523348503853323e-11\n",
      "Loss: 1.190948434035768e-05 | GradNorm^2: 1.3165102412277662e-11\n",
      "Loss: 1.1114934739567479e-05 | GradNorm^2: 1.1811132882721498e-11\n",
      "Loss: 1.0426871086502454e-05 | GradNorm^2: 2.0691678402155805e-11\n",
      "Loss: 9.705749795238582e-06 | GradNorm^2: 9.791257178438434e-12\n",
      "Loss: 9.076271071693867e-06 | GradNorm^2: 8.353434134209695e-12\n",
      "Loss: 8.493265535122432e-06 | GradNorm^2: 7.80950726690139e-12\n",
      "Loss: 7.939397799411448e-06 | GradNorm^2: 6.31951590323496e-12\n",
      "Loss: 7.439205890722445e-06 | GradNorm^2: 5.2043399578588895e-12\n",
      "Loss: 6.9658953784718e-06 | GradNorm^2: 5.950703699262848e-12\n",
      "Loss: 6.527047315685533e-06 | GradNorm^2: 5.778316329049488e-12\n",
      "Loss: 6.1291308097300125e-06 | GradNorm^2: 7.808171527392536e-12\n",
      "Loss: 5.739620681591829e-06 | GradNorm^2: 6.466374832852597e-12\n",
      "Loss: 5.362944219454387e-06 | GradNorm^2: 4.1670824356397046e-12\n",
      "Loss: 5.039447217471449e-06 | GradNorm^2: 5.228599927652062e-12\n",
      "Loss: 4.7775740586692275e-06 | GradNorm^2: 9.302099133547897e-12\n",
      "Loss: 4.412535425359565e-06 | GradNorm^2: 2.4712197479642272e-12\n",
      "Loss: 4.196342691509776e-06 | GradNorm^2: 8.05130401740749e-12\n",
      "Loss: 3.878394167103839e-06 | GradNorm^2: 1.4200042972907889e-12\n",
      "Loss: 3.633679027328691e-06 | GradNorm^2: 1.5585501569917895e-12\n",
      "Loss: 3.445927687362133e-06 | GradNorm^2: 4.548142916298258e-12\n",
      "Loss: 3.1949554042605714e-06 | GradNorm^2: 1.1672051865754284e-12\n",
      "Loss: 3.0640909866414802e-06 | GradNorm^2: 6.093354856612771e-12\n",
      "Loss: 2.808555013005003e-06 | GradNorm^2: 8.852880118046619e-13\n",
      "Loss: 2.6365270089874937e-06 | GradNorm^2: 6.572934938928567e-13\n",
      "Loss: 2.4767260104782526e-06 | GradNorm^2: 9.276709971700954e-13\n",
      "Loss: 2.32177772967344e-06 | GradNorm^2: 6.161698885756983e-13\n",
      "Loss: 2.1694874118642594e-06 | GradNorm^2: 4.344925739423539e-13\n",
      "Loss: 2.0508369406081808e-06 | GradNorm^2: 1.136897991756419e-12\n",
      "Loss: 1.913824163919855e-06 | GradNorm^2: 5.598606300789231e-13\n",
      "Loss: 1.7978132757465473e-06 | GradNorm^2: 4.0823397640197374e-13\n",
      "Loss: 1.686431540143169e-06 | GradNorm^2: 3.662850183314121e-13\n",
      "Loss: 1.5952561731124537e-06 | GradNorm^2: 8.236693893898695e-13\n",
      "Loss: 1.4854314509399433e-06 | GradNorm^2: 2.730946951903201e-13\n",
      "Loss: 1.3915023771333412e-06 | GradNorm^2: 2.2014409220137365e-13\n",
      "Loss: 1.3115703222345612e-06 | GradNorm^2: 3.4923121136744354e-13\n",
      "Loss: 1.232777457399649e-06 | GradNorm^2: 3.906152864233671e-13\n",
      "Loss: 1.150498828716223e-06 | GradNorm^2: 1.760740392771695e-13\n",
      "Loss: 1.088319428524248e-06 | GradNorm^2: 3.0824761495687153e-13\n",
      "Loss: 1.013124451717989e-06 | GradNorm^2: 1.1271477089925353e-13\n",
      "Loss: 9.542950971108853e-07 | GradNorm^2: 1.3585542785771136e-13\n",
      "Loss: 8.942130245932371e-07 | GradNorm^2: 1.2298746280600895e-13\n",
      "Loss: 8.524034686613907e-07 | GradNorm^2: 3.6895900242772436e-13\n",
      "Loss: 7.887394837349215e-07 | GradNorm^2: 1.1089507146629395e-13\n",
      "Loss: 7.376753209392124e-07 | GradNorm^2: 5.066377148358494e-14\n",
      "Loss: 6.936018385867512e-07 | GradNorm^2: 6.9875348724241e-14\n",
      "Loss: 6.50048035861103e-07 | GradNorm^2: 4.543365577215261e-14\n",
      "Loss: 6.140154133919108e-07 | GradNorm^2: 1.017676091579439e-13\n",
      "Loss: 5.741470778842079e-07 | GradNorm^2: 4.9750642541671007e-14\n",
      "Loss: 5.377045769546943e-07 | GradNorm^2: 3.460183164542081e-14\n",
      "Loss: 5.06801653549786e-07 | GradNorm^2: 4.415862213052706e-14\n",
      "Loss: 4.750060065034597e-07 | GradNorm^2: 4.146670360046662e-14\n",
      "Loss: 4.464282770912461e-07 | GradNorm^2: 4.7745573399180456e-14\n",
      "Loss: 4.174948511171238e-07 | GradNorm^2: 1.590980454778435e-14\n",
      "Loss: 3.9126772239641383e-07 | GradNorm^2: 1.3864353758274273e-14\n",
      "Loss: 3.6841041140082045e-07 | GradNorm^2: 2.1065835583484292e-14\n",
      "Loss: 3.486334631333041e-07 | GradNorm^2: 3.8732642615545274e-14\n",
      "Loss: 3.2383715413575627e-07 | GradNorm^2: 9.644784297460854e-15\n",
      "Loss: 3.056137317929061e-07 | GradNorm^2: 1.7321034049949848e-14\n",
      "Loss: 2.864072063938135e-07 | GradNorm^2: 1.2562907108994868e-14\n",
      "Loss: 2.6801830960266335e-07 | GradNorm^2: 7.084216169871973e-15\n",
      "Loss: 2.5164912824092965e-07 | GradNorm^2: 7.0132562005723294e-15\n",
      "Loss: 2.3620173125632698e-07 | GradNorm^2: 6.4530841744916055e-15\n",
      "Loss: 2.2170127483460988e-07 | GradNorm^2: 4.589914877006463e-15\n",
      "Loss: 2.0836694034900865e-07 | GradNorm^2: 4.678532108648724e-15\n",
      "Loss: 1.954746663980806e-07 | GradNorm^2: 4.875993436732187e-15\n",
      "Loss: 1.8357997424211547e-07 | GradNorm^2: 3.894276121840572e-15\n",
      "Loss: 1.7551915137937416e-07 | GradNorm^2: 1.6026993368063437e-14\n",
      "Loss: 1.617987629233489e-07 | GradNorm^2: 2.5086538926230658e-15\n",
      "Loss: 1.5231989524048877e-07 | GradNorm^2: 3.424452601798896e-15\n",
      "Loss: 1.4368573693791982e-07 | GradNorm^2: 4.628194227461325e-15\n",
      "Loss: 1.3451946297715547e-07 | GradNorm^2: 3.3420964513438844e-15\n",
      "Loss: 1.2623291553309474e-07 | GradNorm^2: 2.4379025102804262e-15\n",
      "Loss: 1.187243208610596e-07 | GradNorm^2: 2.8900045523185876e-15\n",
      "Loss: 1.1166701789976657e-07 | GradNorm^2: 2.5476165767095144e-15\n",
      "Loss: 1.0435396895267631e-07 | GradNorm^2: 1.3331288450870706e-15\n",
      "Loss: 9.801877019320117e-08 | GradNorm^2: 1.3193776238409267e-15\n",
      "Loss: 9.229862205312669e-08 | GradNorm^2: 1.5000214357376786e-15\n",
      "Loss: 8.644561000076987e-08 | GradNorm^2: 1.180938213278892e-15\n",
      "Loss: 8.224296700993214e-08 | GradNorm^2: 3.024517885229593e-15\n",
      "Loss: 7.763124675615843e-08 | GradNorm^2: 3.4078821833646567e-15\n",
      "Loss: 7.2228398162216e-08 | GradNorm^2: 1.87664249697542e-15\n",
      "Loss: 6.73062843982406e-08 | GradNorm^2: 7.605188250949156e-16\n",
      "Loss: 6.449082453871129e-08 | GradNorm^2: 2.637726616096947e-15\n",
      "Loss: 5.961317912472792e-08 | GradNorm^2: 1.0282164072614069e-15\n",
      "Loss: 5.688412697260259e-08 | GradNorm^2: 1.9988413213094094e-15\n",
      "Loss: 5.2140675014219985e-08 | GradNorm^2: 2.638643076712293e-16\n",
      "Loss: 4.917160677904985e-08 | GradNorm^2: 3.6452928690590097e-16\n",
      "Loss: 4.6134532906244176e-08 | GradNorm^2: 2.7375430721792683e-16\n",
      "Loss: 4.330728694412607e-08 | GradNorm^2: 2.4132636919261426e-16\n",
      "Loss: 4.075214788631885e-08 | GradNorm^2: 2.789479808468783e-16\n",
      "Loss: 3.82361182639382e-08 | GradNorm^2: 1.9433943703203496e-16\n",
      "Loss: 3.5864522770571194e-08 | GradNorm^2: 1.3068434101854855e-16\n",
      "Loss: 3.37942149297764e-08 | GradNorm^2: 2.24259690255422e-16\n",
      "Loss: 3.161123613288674e-08 | GradNorm^2: 9.275267875978286e-17\n",
      "Loss: 2.9797090412906642e-08 | GradNorm^2: 1.6162413863281742e-16\n",
      "Loss: 2.8042306347036054e-08 | GradNorm^2: 1.6064474708199436e-16\n",
      "Loss: 2.6220654886902748e-08 | GradNorm^2: 6.400802721865966e-17\n",
      "Loss: 2.4636609005622762e-08 | GradNorm^2: 6.150162974177583e-17\n",
      "Loss: 2.315371117133994e-08 | GradNorm^2: 5.655957737558156e-17\n",
      "Loss: 2.1854957628306464e-08 | GradNorm^2: 8.922835672684653e-17\n",
      "Loss: 2.0968975723283336e-08 | GradNorm^2: 2.993416795946293e-16\n",
      "Loss: 1.935667013398839e-08 | GradNorm^2: 9.279779426665266e-17\n",
      "Loss: 1.8110309214242754e-08 | GradNorm^2: 4.991228094336992e-17\n",
      "Loss: 1.7035651280476775e-08 | GradNorm^2: 5.073989913958146e-17\n",
      "Loss: 1.5981402616718163e-08 | GradNorm^2: 2.780014593512028e-17\n",
      "Loss: 1.5019556800217445e-08 | GradNorm^2: 2.1583410077044424e-17\n",
      "Loss: 1.4322645053701887e-08 | GradNorm^2: 8.851694393539606e-17\n",
      "Loss: 1.3385583095035941e-08 | GradNorm^2: 4.986222572704811e-17\n",
      "Loss: 1.254526796924185e-08 | GradNorm^2: 1.7767160574123017e-17\n",
      "Loss: 1.1760672481821791e-08 | GradNorm^2: 1.2858192389372587e-17\n",
      "Loss: 1.1064497951094778e-08 | GradNorm^2: 1.1244084369245032e-17\n",
      "Loss: 1.0399057200064799e-08 | GradNorm^2: 9.703785011802207e-18\n",
      "Loss: 9.830879703926795e-09 | GradNorm^2: 1.94583643556345e-17\n",
      "Loss: 9.225702977734223e-09 | GradNorm^2: 9.996816810088299e-18\n",
      "Loss: 8.743700545917874e-09 | GradNorm^2: 2.3710100284674267e-17\n",
      "Loss: 8.159247797244776e-09 | GradNorm^2: 6.9359785263474616e-18\n",
      "Loss: 7.779073715627227e-09 | GradNorm^2: 2.3583082646011604e-17\n",
      "Loss: 7.220732024727625e-09 | GradNorm^2: 4.68238963584732e-18\n",
      "Loss: 6.822964112656542e-09 | GradNorm^2: 6.21489082326777e-18\n",
      "Loss: 6.569234458186703e-09 | GradNorm^2: 2.68539128968871e-17\n",
      "Loss: 6.0566316433677594e-09 | GradNorm^2: 3.541569948180321e-18\n",
      "Loss: 5.724029100570909e-09 | GradNorm^2: 5.510873472935888e-18\n",
      "Loss: 5.3835372156935445e-09 | GradNorm^2: 2.8979522028317975e-18\n",
      "Loss: 5.129806041763246e-09 | GradNorm^2: 8.908875965588986e-18\n",
      "Loss: 4.822327873692002e-09 | GradNorm^2: 6.6542288242928214e-18\n",
      "Loss: 4.5426453134352544e-09 | GradNorm^2: 4.943411805070354e-18\n",
      "Loss: 4.2576606253582285e-09 | GradNorm^2: 1.605315100731001e-18\n",
      "Loss: 4.025478386702977e-09 | GradNorm^2: 1.6876086830525533e-18\n",
      "Loss: 3.838884493384186e-09 | GradNorm^2: 5.236204659521992e-18\n",
      "Loss: 3.6031960765552903e-09 | GradNorm^2: 2.175240832596192e-18\n",
      "Loss: 3.3923463922247337e-09 | GradNorm^2: 1.137816977027908e-18\n",
      "Loss: 3.2079331434233656e-09 | GradNorm^2: 8.852342368381408e-19\n",
      "Loss: 3.0403823071084603e-09 | GradNorm^2: 1.1951639454219935e-18\n",
      "Loss: 2.8710737283569066e-09 | GradNorm^2: 7.386258868200175e-19\n",
      "Loss: 2.7144990211454883e-09 | GradNorm^2: 8.473630970776418e-19\n",
      "Loss: 2.5667507189350533e-09 | GradNorm^2: 6.099859032393961e-19\n",
      "Loss: 2.432883985254466e-09 | GradNorm^2: 5.762528858245709e-19\n",
      "Loss: 2.338761951742403e-09 | GradNorm^2: 2.27027397160534e-18\n",
      "Loss: 2.182335989016185e-09 | GradNorm^2: 4.0163481529584857e-19\n",
      "Loss: 2.073797349819664e-09 | GradNorm^2: 4.0684828053867044e-19\n",
      "Loss: 1.966044804239382e-09 | GradNorm^2: 3.3402451856497834e-19\n",
      "Loss: 1.868536051889899e-09 | GradNorm^2: 4.20041102449477e-19\n",
      "Loss: 1.7765325982574147e-09 | GradNorm^2: 4.730455765433604e-19\n",
      "Loss: 1.6850643681247665e-09 | GradNorm^2: 2.8282412242045596e-19\n",
      "Loss: 1.602457483929066e-09 | GradNorm^2: 2.7524006434877664e-19\n",
      "Loss: 1.5226533329302185e-09 | GradNorm^2: 1.9545899781248163e-19\n",
      "Loss: 1.4513330940611263e-09 | GradNorm^2: 2.7376033535908915e-19\n",
      "Loss: 1.3853131196031902e-09 | GradNorm^2: 3.2439749459221193e-19\n",
      "Loss: 1.3273003252310845e-09 | GradNorm^2: 5.166738940019102e-19\n",
      "Loss: 1.262461832723775e-09 | GradNorm^2: 3.543241457399319e-19\n",
      "Loss: 1.1947655594652994e-09 | GradNorm^2: 1.3934669097327542e-19\n",
      "Loss: 1.1396407876759527e-09 | GradNorm^2: 1.0754647967084241e-19\n",
      "Loss: 1.0878630548902305e-09 | GradNorm^2: 9.720733966161875e-20\n",
      "Loss: 1.061274415380264e-09 | GradNorm^2: 6.088446198732637e-19\n",
      "Loss: 9.960345777077264e-10 | GradNorm^2: 1.1176865094951605e-19\n",
      "Loss: 9.542703399159211e-10 | GradNorm^2: 1.3516811049967508e-19\n",
      "Loss: 9.160750545644584e-10 | GradNorm^2: 1.8734884367349452e-19\n",
      "Loss: 8.72223894951818e-10 | GradNorm^2: 6.367572163771442e-20\n",
      "Loss: 8.41742807448928e-10 | GradNorm^2: 1.7602197364278598e-19\n",
      "Loss: 8.02572112574226e-10 | GradNorm^2: 8.110841119503704e-20\n",
      "Loss: 7.721214149232564e-10 | GradNorm^2: 1.104124823990992e-19\n",
      "Loss: 7.401573088064288e-10 | GradNorm^2: 7.845837520000218e-20\n",
      "Loss: 7.11233386930047e-10 | GradNorm^2: 7.393707131848787e-20\n",
      "Loss: 6.813072207982769e-10 | GradNorm^2: 3.7802678355396664e-20\n",
      "Loss: 6.911585935478137e-10 | GradNorm^2: 6.025663299979855e-19\n",
      "Loss: 6.307615872914454e-10 | GradNorm^2: 3.095955284004411e-20\n",
      "Loss: 6.087855882131638e-10 | GradNorm^2: 5.2305585337894954e-20\n",
      "Loss: 5.853894462631021e-10 | GradNorm^2: 2.7916296569792615e-20\n",
      "Loss: 5.649838589895965e-10 | GradNorm^2: 2.7549716860898204e-20\n",
      "Loss: 5.460996095974221e-10 | GradNorm^2: 3.275368329660407e-20\n",
      "Loss: 5.261569564709127e-10 | GradNorm^2: 2.176386305818124e-20\n",
      "Loss: 5.085365038634493e-10 | GradNorm^2: 2.5490312120571374e-20\n",
      "Loss: 4.913950508693159e-10 | GradNorm^2: 1.9407536893752344e-20\n",
      "Loss: 4.760572498887059e-10 | GradNorm^2: 2.2201877688873152e-20\n",
      "Loss: 4.6003445752992984e-10 | GradNorm^2: 1.8733224456693937e-20\n",
      "Loss: 4.6116784305760186e-10 | GradNorm^2: 1.8564771656962007e-19\n",
      "Loss: 4.31566391444216e-10 | GradNorm^2: 1.4128900180750378e-20\n",
      "Loss: 4.180796417159245e-10 | GradNorm^2: 1.3652179177411725e-20\n",
      "Loss: 4.054596013574397e-10 | GradNorm^2: 1.432570551296635e-20\n",
      "Loss: 3.9371165502213335e-10 | GradNorm^2: 1.2363311207053824e-20\n",
      "Loss: 3.8219438802334394e-10 | GradNorm^2: 1.1497641469627479e-20\n",
      "Loss: 3.7367591209777323e-10 | GradNorm^2: 3.2150982520832044e-20\n",
      "Loss: 3.6098296176818943e-10 | GradNorm^2: 9.724845633818931e-21\n",
      "Loss: 3.509652507059565e-10 | GradNorm^2: 9.415098127203513e-21\n",
      "Loss: 3.4154529233858593e-10 | GradNorm^2: 8.650744914173442e-21\n",
      "Loss: 3.3473748496158706e-10 | GradNorm^2: 2.467189407532491e-20\n",
      "Loss: 3.2399534423197863e-10 | GradNorm^2: 7.879433030503428e-21\n",
      "Loss: 3.1564919460003917e-10 | GradNorm^2: 7.438537159328801e-21\n",
      "Loss: 3.077537748294277e-10 | GradNorm^2: 7.009128212182278e-21\n",
      "Loss: 3.001429205586028e-10 | GradNorm^2: 6.772521275731791e-21\n",
      "Loss: 2.9313977098885874e-10 | GradNorm^2: 8.809406426474219e-21\n",
      "Loss: 2.861892145133644e-10 | GradNorm^2: 7.787629442820255e-21\n",
      "Loss: 2.7967976201542916e-10 | GradNorm^2: 8.459583124280128e-21\n",
      "Loss: 2.728789050083788e-10 | GradNorm^2: 5.911122110899202e-21\n",
      "Loss: 2.6756930707572614e-10 | GradNorm^2: 1.0727793350117626e-20\n",
      "Loss: 2.61638424207462e-10 | GradNorm^2: 9.535180022366222e-21\n",
      "Loss: 2.558356919050514e-10 | GradNorm^2: 8.151715649372626e-21\n",
      "Loss: 2.4994055120142916e-10 | GradNorm^2: 5.362345106278748e-21\n",
      "Loss: 2.4520950095716684e-10 | GradNorm^2: 8.250096549532167e-21\n",
      "Loss: 2.4030029404944577e-10 | GradNorm^2: 8.701939152477095e-21\n",
      "Loss: 2.347215637488512e-10 | GradNorm^2: 4.096646487043516e-21\n",
      "Loss: 2.30902739751974e-10 | GradNorm^2: 8.462770649649434e-21\n",
      "Loss: 2.2554573515970468e-10 | GradNorm^2: 3.9272331274917215e-21\n",
      "Loss: 2.2115609280299342e-10 | GradNorm^2: 3.78556002835849e-21\n",
      "Loss: 2.1709837258539254e-10 | GradNorm^2: 3.405718677911221e-21\n",
      "Loss: 2.1323997197543247e-10 | GradNorm^2: 4.9937095428763115e-21\n",
      "Loss: 2.1112390275510194e-10 | GradNorm^2: 1.3529205532670619e-20\n",
      "Loss: 2.0555575104831032e-10 | GradNorm^2: 4.381633462926313e-21\n",
      "Loss: 2.0176669193710315e-10 | GradNorm^2: 3.39187767294145e-21\n",
      "Loss: 1.9908402050338399e-10 | GradNorm^2: 7.023347809743395e-21\n",
      "Loss: 1.9473467877420105e-10 | GradNorm^2: 2.790646211232054e-21\n",
      "Loss: 1.9146411267410474e-10 | GradNorm^2: 2.7453744544979833e-21\n",
      "Loss: 1.887911455453816e-10 | GradNorm^2: 5.04565549403613e-21\n",
      "Loss: 1.854466780419807e-10 | GradNorm^2: 3.806977414608751e-21\n",
      "Loss: 1.822815606827603e-10 | GradNorm^2: 2.88272585418411e-21\n",
      "Loss: 1.7933584529812238e-10 | GradNorm^2: 2.614659491344609e-21\n",
      "Loss: 1.7663268262273398e-10 | GradNorm^2: 2.9504306373253053e-21\n",
      "Loss: 1.737671003430057e-10 | GradNorm^2: 2.2049848224768543e-21\n",
      "Loss: 1.711228316753333e-10 | GradNorm^2: 2.235654875979032e-21\n",
      "Loss: 1.6865708015808984e-10 | GradNorm^2: 2.610542739310219e-21\n",
      "Loss: 1.659670411941781e-10 | GradNorm^2: 1.9606944095827513e-21\n",
      "Loss: 1.6375414196911267e-10 | GradNorm^2: 2.2033856542725487e-21\n",
      "Loss: 1.6135126375372705e-10 | GradNorm^2: 2.4467345607940978e-21\n",
      "Loss: 1.5933399611673885e-10 | GradNorm^2: 3.2580994962292695e-21\n",
      "Loss: 1.56799842276593e-10 | GradNorm^2: 2.1078854340532222e-21\n",
      "Loss: 1.545705475005628e-10 | GradNorm^2: 1.855625030983258e-21\n",
      "Loss: 1.5251206923789856e-10 | GradNorm^2: 2.0000472590076736e-21\n",
      "Loss: 1.5071319187244513e-10 | GradNorm^2: 2.684810096346724e-21\n",
      "Loss: 1.4845787495537598e-10 | GradNorm^2: 1.8100857225308702e-21\n",
      "Loss: 1.4646504099519456e-10 | GradNorm^2: 1.5219173237246656e-21\n",
      "Loss: 1.44843219856628e-10 | GradNorm^2: 2.415112044121969e-21\n",
      "Loss: 1.4269535349115905e-10 | GradNorm^2: 1.4766942654691333e-21\n",
      "Loss: 1.4091424041428382e-10 | GradNorm^2: 1.4845262022050937e-21\n",
      "Loss: 1.3919393560319886e-10 | GradNorm^2: 1.5458501539428386e-21\n",
      "Loss: 1.374581486031803e-10 | GradNorm^2: 1.4126577065913559e-21\n",
      "Loss: 1.3580322932120592e-10 | GradNorm^2: 1.3664469157130163e-21\n",
      "Loss: 1.3415928563690968e-10 | GradNorm^2: 1.3039042306408227e-21\n",
      "Loss: 1.3258426321755042e-10 | GradNorm^2: 1.2738829837161724e-21\n",
      "Loss: 1.3117608210325433e-10 | GradNorm^2: 1.6920026957781287e-21\n",
      "Loss: 1.295083980918895e-10 | GradNorm^2: 1.2215400181966891e-21\n",
      "Loss: 1.2800803594914298e-10 | GradNorm^2: 1.1528378656591084e-21\n",
      "Loss: 1.265648148419212e-10 | GradNorm^2: 1.2046728598695193e-21\n",
      "Loss: 1.2524232633404217e-10 | GradNorm^2: 1.3551666292141151e-21\n",
      "Loss: 1.2379139006167322e-10 | GradNorm^2: 1.1527847959974877e-21\n",
      "Loss: 1.2256187176653397e-10 | GradNorm^2: 1.4791823671302394e-21\n",
      "Loss: 1.2110105744142316e-10 | GradNorm^2: 1.0878514409662235e-21\n",
      "Loss: 1.1983626933453604e-10 | GradNorm^2: 1.1335857074507271e-21\n",
      "Loss: 1.1857371131677454e-10 | GradNorm^2: 1.1248955816629484e-21\n",
      "Loss: 1.173031454929085e-10 | GradNorm^2: 1.0407345791840902e-21\n",
      "Loss: 1.1610400942776326e-10 | GradNorm^2: 9.462577083109416e-22\n",
      "Loss: 1.1490548790493576e-10 | GradNorm^2: 9.295149555631451e-22\n",
      "Loss: 1.1378699980892343e-10 | GradNorm^2: 1.0899753217977498e-21\n",
      "Loss: 1.1258193697499592e-10 | GradNorm^2: 9.333861239699497e-22\n",
      "Loss: 1.1146509009160512e-10 | GradNorm^2: 9.363441280605186e-22\n",
      "Loss: 1.1040412828427748e-10 | GradNorm^2: 9.444702265422455e-22\n",
      "Loss: 1.0954400373109504e-10 | GradNorm^2: 1.537438407917248e-21\n"
     ]
    }
   ],
   "source": [
    "# study_name = f\"adam-{dataset_name}-{batch_size}-{percentage}-{STEPS}-{loss_name}-{scale_k}\"\n",
    "# storage_name = \"sqlite:///optuna_results/{}.db\".format(study_name)\n",
    "# study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "# lr = study.best_params[\"lr\"]\n",
    "lr = 0.1\n",
    "# parameters\n",
    "w = torch.zeros(train_data.shape[1], device=device).requires_grad_()\n",
    "optimizer = Adam([w], lr=lr)\n",
    "loss_function = loss_class(w)\n",
    "\n",
    "# save loss and grad size to history\n",
    "hist_adam = []\n",
    "loss = loss_function(train_data.to(device), train_target.to(device))\n",
    "g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "print(f\"Loss: {loss.item()} | GradNorm^2: {(torch.linalg.norm(g) ** 2 ).item()}\")\n",
    "hist_adam.append([loss.item(), (torch.linalg.norm(g) ** 2).item()])\n",
    "\n",
    "for step in range(STEPS):\n",
    "    for i, (batch_data, batch_target) in enumerate(train_dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_target = batch_target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(batch_data, batch_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss = loss_function(train_data.to(device), train_target.to(device))\n",
    "    g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "    print(f\"Loss: {loss.item()} | GradNorm^2: {(torch.linalg.norm(g) ** 2 ).item()}\")\n",
    "    hist_adam.append([loss.item(), (torch.linalg.norm(g) ** 2).item()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSPS2\n",
    "$w^* = \\arg\\min_{w\\in\\mathbb{R} ^d}\\frac{1}{2} \\|w - w_t\\|_{D}^2 \\nonumber \\\\\n",
    "\\textit{s.t.} \\quad f_i(w_t) +  \\langle  \\nabla  f_i(w_t), w-w_t \\rangle + \\frac{1}{2} \\langle D(w-w^t), w - w^t \\rangle = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931471805599453 | GradNorm^2: 0.3195669607542947\n",
      "Loss: 0.01273632985658148 | GradNorm^2: 4.8047184393537585e-05\n",
      "Loss: 0.011085717156693133 | GradNorm^2: 7.900462273791814e-05\n",
      "Loss: 0.011570226701282608 | GradNorm^2: 0.0005507484889951745\n",
      "Loss: 0.006942764910307914 | GradNorm^2: 9.319190015826484e-05\n",
      "Loss: 0.005942196459627987 | GradNorm^2: 7.085505782630452e-05\n",
      "Loss: 0.004286356016590375 | GradNorm^2: 3.763538784024932e-05\n",
      "Loss: 0.004215666015102859 | GradNorm^2: 3.74723944443412e-05\n",
      "Loss: 0.0038063931156914125 | GradNorm^2: 6.94737066862079e-05\n",
      "Loss: 0.002831080588959166 | GradNorm^2: 4.673096638043763e-06\n",
      "Loss: 0.002831080588959166 | GradNorm^2: 4.673096638043763e-06\n",
      "Loss: 0.0023829953969756864 | GradNorm^2: 1.4857996916270562e-05\n",
      "Loss: 0.0020960760876296398 | GradNorm^2: 4.786921326293024e-06\n",
      "Loss: 0.0021211250939524597 | GradNorm^2: 1.5368743628067295e-05\n",
      "Loss: 0.0015620632739325921 | GradNorm^2: 1.8387322542095513e-06\n",
      "Loss: 0.0015620632739325921 | GradNorm^2: 1.8387322542095513e-06\n",
      "Loss: 0.0015620632739325921 | GradNorm^2: 1.8387322542095513e-06\n",
      "Loss: 0.0015620632739325921 | GradNorm^2: 1.8387322542095513e-06\n",
      "Loss: 0.0010625021533203726 | GradNorm^2: 3.5073554076046377e-06\n",
      "Loss: 0.0008907668537727632 | GradNorm^2: 6.306472044326084e-07\n",
      "Loss: 0.000848662319326799 | GradNorm^2: 2.477112900157101e-06\n",
      "Loss: 0.0005817425896840057 | GradNorm^2: 5.072443639583191e-07\n",
      "Loss: 0.00050801016377069 | GradNorm^2: 1.256353282234916e-07\n",
      "Loss: 0.00050801016377069 | GradNorm^2: 1.256353282234916e-07\n",
      "Loss: 0.00042499202573024374 | GradNorm^2: 2.1691764643094926e-07\n",
      "Loss: 0.0003955106451125708 | GradNorm^2: 1.0961821044957174e-07\n",
      "Loss: 0.0003955106451125708 | GradNorm^2: 1.0961821044957174e-07\n",
      "Loss: 0.0002725915466614735 | GradNorm^2: 3.2324038827918204e-08\n",
      "Loss: 0.00025907636447080587 | GradNorm^2: 4.923670358237947e-08\n",
      "Loss: 0.00025636386500166606 | GradNorm^2: 3.81031611323396e-08\n",
      "Loss: 0.00023059340243536814 | GradNorm^2: 3.480693519659263e-08\n",
      "Loss: 0.00023059340243536814 | GradNorm^2: 3.480693519659263e-08\n",
      "Loss: 0.00017498438084999595 | GradNorm^2: 2.6607862476738394e-08\n",
      "Loss: 0.00016998909087824205 | GradNorm^2: 1.5268253195549235e-08\n",
      "Loss: 0.00016998909087824205 | GradNorm^2: 1.5268253195549235e-08\n",
      "Loss: 0.00016998909087824205 | GradNorm^2: 1.5268253195549235e-08\n",
      "Loss: 0.00013250306942208702 | GradNorm^2: 2.1886835273361487e-08\n",
      "Loss: 0.00011251091783487786 | GradNorm^2: 5.867670610441586e-08\n",
      "Loss: 8.249242814092786e-05 | GradNorm^2: 3.167078969703775e-09\n",
      "Loss: 8.249242814092786e-05 | GradNorm^2: 3.167078969703775e-09\n",
      "Loss: 8.249242814092786e-05 | GradNorm^2: 3.167078969703775e-09\n",
      "Loss: 8.249242814092786e-05 | GradNorm^2: 3.167078969703775e-09\n",
      "Loss: 7.316108344569587e-05 | GradNorm^2: 8.158129677105706e-09\n",
      "Loss: 6.676635923290615e-05 | GradNorm^2: 4.557280997122695e-09\n",
      "Loss: 6.717473914790646e-05 | GradNorm^2: 3.009310330888253e-08\n",
      "Loss: 3.904952689558978e-05 | GradNorm^2: 2.6188439378460604e-09\n",
      "Loss: 3.722666933631381e-05 | GradNorm^2: 4.861492774958599e-09\n",
      "Loss: 3.13054714792904e-05 | GradNorm^2: 9.312031552418655e-10\n",
      "Loss: 3.13054714792904e-05 | GradNorm^2: 9.312031552418655e-10\n",
      "Loss: 3.13054714792904e-05 | GradNorm^2: 9.312031552418655e-10\n",
      "Loss: 3.13054714792904e-05 | GradNorm^2: 9.312031552418655e-10\n",
      "Loss: 2.3468072763011216e-05 | GradNorm^2: 3.220300527213202e-10\n",
      "Loss: 2.425063035547576e-05 | GradNorm^2: 1.2977952869194528e-09\n",
      "Loss: 2.425063035547576e-05 | GradNorm^2: 1.2977952869194528e-09\n",
      "Loss: 1.7571140461500522e-05 | GradNorm^2: 7.247754630559779e-11\n",
      "Loss: 1.5604143392963602e-05 | GradNorm^2: 5.4414197175274346e-11\n",
      "Loss: 1.5799972712602494e-05 | GradNorm^2: 3.712920813077147e-10\n",
      "Loss: 1.4598446844800734e-05 | GradNorm^2: 1.2200357941161655e-10\n",
      "Loss: 1.2376402316506918e-05 | GradNorm^2: 8.053593972083494e-11\n",
      "Loss: 1.1181278874079116e-05 | GradNorm^2: 3.109881155979687e-11\n",
      "Loss: 1.135770351177738e-05 | GradNorm^2: 2.981149829870079e-11\n",
      "Loss: 1.135770351177738e-05 | GradNorm^2: 2.981149829870079e-11\n",
      "Loss: 9.973850845630077e-06 | GradNorm^2: 4.789013061393238e-11\n",
      "Loss: 1.0114438355069206e-05 | GradNorm^2: 7.844372076646791e-11\n",
      "Loss: 1.0106169684292626e-05 | GradNorm^2: 5.734769512463249e-11\n",
      "Loss: 9.626919981776558e-06 | GradNorm^2: 7.649050872284427e-11\n",
      "Loss: 9.626919981776558e-06 | GradNorm^2: 7.649050872284427e-11\n",
      "Loss: 8.516215364327544e-06 | GradNorm^2: 2.1423234082390615e-11\n",
      "Loss: 8.18258945111654e-06 | GradNorm^2: 3.7830577746159626e-11\n",
      "Loss: 8.18258945111654e-06 | GradNorm^2: 3.7830577746159626e-11\n",
      "Loss: 8.18258945111654e-06 | GradNorm^2: 3.7830577746159626e-11\n",
      "Loss: 8.18258945111654e-06 | GradNorm^2: 3.7830577746159626e-11\n",
      "Loss: 8.18258945111654e-06 | GradNorm^2: 3.7830577746159626e-11\n",
      "Loss: 8.18258945111654e-06 | GradNorm^2: 3.7830577746159626e-11\n",
      "Loss: 8.18258945111654e-06 | GradNorm^2: 3.7830577746159626e-11\n",
      "Loss: 8.18258945111654e-06 | GradNorm^2: 3.7830577746159626e-11\n",
      "Loss: 8.18258945111654e-06 | GradNorm^2: 3.7830577746159626e-11\n",
      "Loss: 8.18258945111654e-06 | GradNorm^2: 3.7830577746159626e-11\n",
      "Loss: 8.18258945111654e-06 | GradNorm^2: 3.7830577746159626e-11\n",
      "Loss: 8.18258945111654e-06 | GradNorm^2: 3.7830577746159626e-11\n",
      "Loss: 8.18258945111654e-06 | GradNorm^2: 3.7830577746159626e-11\n",
      "Loss: 8.18258945111654e-06 | GradNorm^2: 3.7830577746159626e-11\n",
      "Loss: 8.18258945111654e-06 | GradNorm^2: 3.7830577746159626e-11\n",
      "Loss: 8.18258945111654e-06 | GradNorm^2: 3.7830577746159626e-11\n",
      "Loss: 8.18258945111654e-06 | GradNorm^2: 3.7830577746159626e-11\n",
      "Loss: 8.18258945111654e-06 | GradNorm^2: 3.7830577746159626e-11\n",
      "Loss: 8.18258945111654e-06 | GradNorm^2: 3.7830577746159626e-11\n",
      "Loss: 8.27584445152029e-06 | GradNorm^2: 1.416176882761005e-10\n",
      "Loss: 8.393552935040347e-06 | GradNorm^2: 1.6459147415020378e-10\n",
      "Loss: 7.752470955298758e-06 | GradNorm^2: 6.942323883661035e-11\n",
      "Loss: 7.752470955298758e-06 | GradNorm^2: 6.942323883661035e-11\n",
      "Loss: 7.752470955298758e-06 | GradNorm^2: 6.942323883661035e-11\n",
      "Loss: 7.752470955298758e-06 | GradNorm^2: 6.942323883661035e-11\n",
      "Loss: 7.752470955298758e-06 | GradNorm^2: 6.942323883661035e-11\n",
      "Loss: 7.752470955298758e-06 | GradNorm^2: 6.942323883661035e-11\n",
      "Loss: 7.752470955298758e-06 | GradNorm^2: 6.942323883661035e-11\n",
      "Loss: 7.752470955298758e-06 | GradNorm^2: 6.942323883661035e-11\n",
      "Loss: 7.752470955298758e-06 | GradNorm^2: 6.942323883661035e-11\n",
      "Loss: 7.752470955298758e-06 | GradNorm^2: 6.942323883661035e-11\n",
      "Loss: 7.752470955298758e-06 | GradNorm^2: 6.942323883661035e-11\n",
      "Loss: 7.752470955298758e-06 | GradNorm^2: 6.942323883661035e-11\n",
      "Loss: 7.752470955298758e-06 | GradNorm^2: 6.942323883661035e-11\n",
      "Loss: 7.752470955298758e-06 | GradNorm^2: 6.942323883661035e-11\n",
      "Loss: 7.980867304659697e-06 | GradNorm^2: 1.0789791971833487e-10\n",
      "Loss: 9.823092825328184e-06 | GradNorm^2: 4.2958335168413224e-10\n",
      "Loss: 8.160210452803925e-06 | GradNorm^2: 9.833673960838903e-11\n",
      "Loss: 8.160210452803925e-06 | GradNorm^2: 9.833673960838903e-11\n",
      "Loss: 8.160210452803925e-06 | GradNorm^2: 9.833673960838903e-11\n",
      "Loss: 8.160210452803925e-06 | GradNorm^2: 9.833673960838903e-11\n",
      "Loss: 8.160210452803925e-06 | GradNorm^2: 9.833673960838903e-11\n",
      "Loss: 8.326709242795075e-06 | GradNorm^2: 1.9269377685517028e-10\n",
      "Loss: 8.326709242795075e-06 | GradNorm^2: 1.9269377685517028e-10\n",
      "Loss: 8.326709242795075e-06 | GradNorm^2: 1.9269377685517028e-10\n",
      "Loss: 7.939243490191563e-06 | GradNorm^2: 1.1461747868977117e-10\n",
      "Loss: 7.833456887121946e-06 | GradNorm^2: 1.4967127862322203e-10\n",
      "Loss: 6.795537166979202e-06 | GradNorm^2: 2.2023523510653754e-11\n",
      "Loss: 6.795537166979202e-06 | GradNorm^2: 2.2023523510653754e-11\n",
      "Loss: 6.795537166979202e-06 | GradNorm^2: 2.2023523510653754e-11\n",
      "Loss: 6.795537166979202e-06 | GradNorm^2: 2.2023523510653754e-11\n",
      "Loss: 6.795537166979202e-06 | GradNorm^2: 2.2023523510653754e-11\n",
      "Loss: 6.795537166979202e-06 | GradNorm^2: 2.2023523510653754e-11\n",
      "Loss: 6.795537166979202e-06 | GradNorm^2: 2.2023523510653754e-11\n",
      "Loss: 6.795537166979202e-06 | GradNorm^2: 2.2023523510653754e-11\n",
      "Loss: 6.795537166979202e-06 | GradNorm^2: 2.2023523510653754e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 7.5383206253444435e-06 | GradNorm^2: 9.14008752606281e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 6.821687619602917e-06 | GradNorm^2: 3.7629823604935294e-11\n",
      "Loss: 8.436289587532707e-06 | GradNorm^2: 3.454730823360501e-10\n",
      "Loss: 6.945054241531549e-06 | GradNorm^2: 9.577464914567401e-11\n",
      "Loss: 6.73654922569359e-06 | GradNorm^2: 6.672355116428816e-11\n",
      "Loss: 6.73654922569359e-06 | GradNorm^2: 6.672355116428816e-11\n",
      "Loss: 6.73654922569359e-06 | GradNorm^2: 6.672355116428816e-11\n",
      "Loss: 6.73654922569359e-06 | GradNorm^2: 6.672355116428816e-11\n",
      "Loss: 6.73654922569359e-06 | GradNorm^2: 6.672355116428816e-11\n",
      "Loss: 6.188295336990728e-06 | GradNorm^2: 1.055137922689475e-11\n",
      "Loss: 6.188295336990728e-06 | GradNorm^2: 1.055137922689475e-11\n",
      "Loss: 6.188295336990728e-06 | GradNorm^2: 1.055137922689475e-11\n",
      "Loss: 6.188295336990728e-06 | GradNorm^2: 1.055137922689475e-11\n",
      "Loss: 6.188295336990728e-06 | GradNorm^2: 1.055137922689475e-11\n",
      "Loss: 6.188295336990728e-06 | GradNorm^2: 1.055137922689475e-11\n",
      "Loss: 6.188295336990728e-06 | GradNorm^2: 1.055137922689475e-11\n",
      "Loss: 6.188295336990728e-06 | GradNorm^2: 1.055137922689475e-11\n",
      "Loss: 6.188295336990728e-06 | GradNorm^2: 1.055137922689475e-11\n",
      "Loss: 6.188295336990728e-06 | GradNorm^2: 1.055137922689475e-11\n",
      "Loss: 6.188295336990728e-06 | GradNorm^2: 1.055137922689475e-11\n",
      "Loss: 6.188295336990728e-06 | GradNorm^2: 1.055137922689475e-11\n",
      "Loss: 6.188295336990728e-06 | GradNorm^2: 1.055137922689475e-11\n",
      "Loss: 6.188295336990728e-06 | GradNorm^2: 1.055137922689475e-11\n",
      "Loss: 6.188295336990728e-06 | GradNorm^2: 1.055137922689475e-11\n",
      "Loss: 6.188295336990728e-06 | GradNorm^2: 1.055137922689475e-11\n",
      "Loss: 6.188295336990728e-06 | GradNorm^2: 1.055137922689475e-11\n",
      "Loss: 6.188295336990728e-06 | GradNorm^2: 1.055137922689475e-11\n",
      "Loss: 6.188295336990728e-06 | GradNorm^2: 1.055137922689475e-11\n",
      "Loss: 6.188295336990728e-06 | GradNorm^2: 1.055137922689475e-11\n",
      "Loss: 6.5925341523203176e-06 | GradNorm^2: 8.544438074967592e-11\n",
      "Loss: 6.5925341523203176e-06 | GradNorm^2: 8.544438074967592e-11\n",
      "Loss: 6.5925341523203176e-06 | GradNorm^2: 8.544438074967592e-11\n",
      "Loss: 6.911785571214529e-06 | GradNorm^2: 1.4678232066281236e-10\n",
      "Loss: 6.911785571214529e-06 | GradNorm^2: 1.4678232066281236e-10\n",
      "Loss: 6.260965980172931e-06 | GradNorm^2: 7.414821985969421e-11\n",
      "Loss: 6.260965980172931e-06 | GradNorm^2: 7.414821985969421e-11\n",
      "Loss: 6.260965980172931e-06 | GradNorm^2: 7.414821985969421e-11\n",
      "Loss: 6.260965980172931e-06 | GradNorm^2: 7.414821985969421e-11\n",
      "Loss: 6.260965980172931e-06 | GradNorm^2: 7.414821985969421e-11\n",
      "Loss: 6.260965980172931e-06 | GradNorm^2: 7.414821985969421e-11\n",
      "Loss: 6.260965980172931e-06 | GradNorm^2: 7.414821985969421e-11\n",
      "Loss: 6.260965980172931e-06 | GradNorm^2: 7.414821985969421e-11\n",
      "Loss: 6.260965980172931e-06 | GradNorm^2: 7.414821985969421e-11\n",
      "Loss: 6.260965980172931e-06 | GradNorm^2: 7.414821985969421e-11\n",
      "Loss: 6.260965980172931e-06 | GradNorm^2: 7.414821985969421e-11\n",
      "Loss: 6.260965980172931e-06 | GradNorm^2: 7.414821985969421e-11\n",
      "Loss: 6.260965980172931e-06 | GradNorm^2: 7.414821985969421e-11\n",
      "Loss: 6.260965980172931e-06 | GradNorm^2: 7.414821985969421e-11\n",
      "Loss: 6.260965980172931e-06 | GradNorm^2: 7.414821985969421e-11\n",
      "Loss: 6.260965980172931e-06 | GradNorm^2: 7.414821985969421e-11\n",
      "Loss: 6.260965980172931e-06 | GradNorm^2: 7.414821985969421e-11\n",
      "Loss: 6.260965980172931e-06 | GradNorm^2: 7.414821985969421e-11\n",
      "Loss: 6.260965980172931e-06 | GradNorm^2: 7.414821985969421e-11\n",
      "Loss: 6.260965980172931e-06 | GradNorm^2: 7.414821985969421e-11\n",
      "Loss: 6.260965980172931e-06 | GradNorm^2: 7.414821985969421e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.740142751775349e-06 | GradNorm^2: 2.0016186635847285e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n",
      "Loss: 5.710584996138496e-06 | GradNorm^2: 3.4181490959664024e-11\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "alpha = 1e-4\n",
    "beta=0.999\n",
    "w = torch.zeros(train_data.shape[1], device=device).requires_grad_()\n",
    "\n",
    "loss_function = loss_class(w)\n",
    "\n",
    "# save loss and grad size to history\n",
    "hist_psps2 = []\n",
    "loss = loss_function(train_data.to(device), train_target.to(device))\n",
    "g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "print(f\"Loss: {loss.item()} | GradNorm^2: {(torch.linalg.norm(g) ** 2 ).item()}\")\n",
    "hist_psps2.append([loss.item(), (torch.linalg.norm(g) ** 2).item()])\n",
    "\n",
    "# preconditioninig matrix\n",
    "Dk = diag_estimate_old(w, g, 100)\n",
    "Dk_hat_inv = torch.ones_like(w)\n",
    "\n",
    "\n",
    "for step in range(STEPS):\n",
    "\n",
    "    for i, (batch_data, batch_target) in enumerate(train_dataloader):\n",
    "\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_target = batch_target.to(device)\n",
    "\n",
    "        loss = loss_function(batch_data, batch_target)\n",
    "        g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "\n",
    "        vk = diag_estimate_old(w, g, 1)\n",
    "\n",
    "        # Smoothing and Truncation \n",
    "        Dk = beta * Dk + (1 - beta) * vk\n",
    "        Dk_hat = torch.abs(Dk)\n",
    "        Dk_hat[Dk_hat < alpha] = alpha\n",
    "\n",
    "        Dk_hat_inv = 1 / Dk_hat\n",
    "\n",
    "        gnorm = (g * Dk_hat_inv).dot(g)\n",
    "\n",
    "        f_grad = g.clone().detach()\n",
    "        \n",
    "        a = 2 * loss.item() + 3 * gnorm\n",
    "        b = 4 * loss.item() - 2 * gnorm\n",
    "        c = 2 * loss.item()\n",
    "\n",
    "        det = 1 - (2 * loss.item() / gnorm )\n",
    "\n",
    "        if det < 1e-15:\n",
    "            # print(\"no real solution\")\n",
    "            continue\n",
    "        else:\n",
    "            t = torch.sqrt(det)/det\n",
    "            root1 = -1 + t\n",
    "            root2 = -1 - t\n",
    "            \n",
    "            root = torch.maximum(root1, root2)\n",
    "\n",
    "        precond = root/(1 + root) * Dk_hat_inv\n",
    "        with torch.no_grad():\n",
    "            w.sub_(precond * f_grad)\n",
    "\n",
    "    loss = loss_function(train_data.to(device), train_target.to(device))\n",
    "    g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "    print(f\"Loss: {loss.item()} | GradNorm^2: {(torch.linalg.norm(g) ** 2 ).item()}\")\n",
    "    hist_psps2.append([loss.item(), (torch.linalg.norm(g) ** 2).item()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSPS2-L1\n",
    "$w_{t+1}, \\; s_{t+1} = {\\arg\\min}_{w\\in\\mathbb{R}^d,s\\geq\n",
    "    0}\\frac{1}{2}\\|w-w_t\\|_{D}^2+\\mu(s-s_t)^2+\\lambda s \\nonumber \\\\\n",
    "    \\textit{s.t.} \\quad f_i(w_t) +  \\langle  \\nabla  f_i(w_t), w-w_t\\rangle +\\frac{1}{2}\\langle D(w-w^t), w - w^t \\rangle \\leq s \\nonumber.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931471805599453 | GradNorm^2: 0.3195669607542947\n",
      "Loss: 0.04319362217204191 | GradNorm^2: 0.000695710077354812 | s: 0.04605122658256474 | s_nil + z = 0.04613842921629544\n",
      "Loss: 0.029801985379582133 | GradNorm^2: 0.00035695849121140976 | s: 0.037211649691514484 | s_nil + z = 0.03735630166475626\n",
      "Loss: 0.022423024203195106 | GradNorm^2: 0.00014168968915972597 | s: 0.028501710642390217 | s_nil + z = 0.028495870536985896\n",
      "Loss: 0.017675602757730637 | GradNorm^2: 9.067517599499067e-05 | s: 0.02198859855713457 | s_nil + z = 0.021948729514747082\n",
      "Loss: 0.014327681732857707 | GradNorm^2: 5.212456030260763e-05 | s: 0.017395247354635685 | s_nil + z = 0.01745097704478775\n",
      "Loss: 0.011889782767921863 | GradNorm^2: 3.7404675760447914e-05 | s: 0.013941430742712672 | s_nil + z = 0.01396672173785539\n",
      "Loss: 0.010015060695152048 | GradNorm^2: 2.4387564270917618e-05 | s: 0.011516611275693698 | s_nil + z = 0.01156383192918023\n",
      "Loss: 0.008546059209057402 | GradNorm^2: 1.669923030975711e-05 | s: 0.009506467069025885 | s_nil + z = 0.009533011853660851\n",
      "Loss: 0.007404857951373558 | GradNorm^2: 2.012025622933619e-05 | s: 0.00846191489611007 | s_nil + z = 0.008495707929203379\n",
      "Loss: 0.006393827124210497 | GradNorm^2: 1.0208819146240645e-05 | s: 0.007190396986322478 | s_nil + z = 0.007220385894662604\n",
      "Loss: 0.0055713811714897185 | GradNorm^2: 6.404408618432779e-06 | s: 0.006065702936827105 | s_nil + z = 0.0060809791537588045\n",
      "Loss: 0.00487700559071608 | GradNorm^2: 4.744152426700274e-06 | s: 0.00528749646478413 | s_nil + z = 0.00529331079422999\n",
      "Loss: 0.004286041621157041 | GradNorm^2: 3.4956841632769257e-06 | s: 0.0046577839754665005 | s_nil + z = 0.004671167968894725\n",
      "Loss: 0.0037749849822720093 | GradNorm^2: 2.7408589210543374e-06 | s: 0.004085546211831036 | s_nil + z = 0.004094523039604156\n",
      "Loss: 0.003330049547990629 | GradNorm^2: 2.124343878350255e-06 | s: 0.0036469411675852156 | s_nil + z = 0.003653629463381644\n",
      "Loss: 0.002945089795915817 | GradNorm^2: 1.9380048426109165e-06 | s: 0.0032359565531881467 | s_nil + z = 0.00323952727261214\n",
      "Loss: 0.002600168684963952 | GradNorm^2: 1.2439598932141898e-06 | s: 0.002880491507348565 | s_nil + z = 0.0028928516327708653\n",
      "Loss: 0.0023028716898820033 | GradNorm^2: 9.627123467175445e-07 | s: 0.0024697075733821393 | s_nil + z = 0.002477081967799659\n",
      "Loss: 0.0020434413158905286 | GradNorm^2: 9.507354787164772e-07 | s: 0.00219082151548469 | s_nil + z = 0.0021991185735116756\n",
      "Loss: 0.0018098397352868363 | GradNorm^2: 5.909399761637208e-07 | s: 0.0019338784062447746 | s_nil + z = 0.001935109417642307\n",
      "Loss: 0.0016116007103288198 | GradNorm^2: 6.893183617863527e-07 | s: 0.0017380777352596552 | s_nil + z = 0.0017398896784458634\n",
      "Loss: 0.001441417491042533 | GradNorm^2: 8.507736465293356e-07 | s: 0.0015815176632060855 | s_nil + z = 0.0015899092279433105\n",
      "Loss: 0.001273892996695186 | GradNorm^2: 4.53664669338012e-07 | s: 0.001400585695368629 | s_nil + z = 0.0013979476521945242\n",
      "Loss: 0.0011382293260888095 | GradNorm^2: 4.5461784757793365e-07 | s: 0.001235239291407315 | s_nil + z = 0.001236545460416745\n",
      "Loss: 0.0010092661824477169 | GradNorm^2: 2.2004298248991946e-07 | s: 0.0010862265268358705 | s_nil + z = 0.001090080025143235\n",
      "Loss: 0.0009021160253054034 | GradNorm^2: 2.0517741918379177e-07 | s: 0.0009770965923223181 | s_nil + z = 0.0009787831510991718\n",
      "Loss: 0.0008039560861763634 | GradNorm^2: 1.340275566283791e-07 | s: 0.0008636811085126256 | s_nil + z = 0.0008646431914751534\n",
      "Loss: 0.0007182592848240882 | GradNorm^2: 1.0642091192558099e-07 | s: 0.0007672516244232309 | s_nil + z = 0.0007695287156681274\n",
      "Loss: 0.000641159244885699 | GradNorm^2: 7.324653692158908e-08 | s: 0.0006919782644075262 | s_nil + z = 0.0006765460115564989\n",
      "Loss: 0.0005745436903480157 | GradNorm^2: 7.53762545832068e-08 | s: 0.0006211124678705144 | s_nil + z = 0.0006236152600529038\n",
      "Loss: 0.0005142654319032372 | GradNorm^2: 5.1984990536744935e-08 | s: 0.0005417966678607314 | s_nil + z = 0.0005433715052013052\n",
      "Loss: 0.0004596231019913803 | GradNorm^2: 3.5385424428634774e-08 | s: 0.0004829227146292062 | s_nil + z = 0.00048577990708111723\n",
      "Loss: 0.0004115415102950922 | GradNorm^2: 2.7593170824281555e-08 | s: 0.0004361927916329087 | s_nil + z = 0.0004338017395381574\n",
      "Loss: 0.0003689289462779969 | GradNorm^2: 2.1610865357646223e-08 | s: 0.0004032923993541907 | s_nil + z = 0.00040303239593128537\n",
      "Loss: 0.0003311031411797927 | GradNorm^2: 1.6892965783175325e-08 | s: 0.00035308931210250785 | s_nil + z = 0.00034982423235226795\n",
      "Loss: 0.0002987826485116274 | GradNorm^2: 2.1914103136151348e-08 | s: 0.0003110120591312897 | s_nil + z = 0.0003115977370233419\n",
      "Loss: 0.0002681332894148517 | GradNorm^2: 1.0728776436323206e-08 | s: 0.0002804759119715624 | s_nil + z = 0.0002807726888268275\n",
      "Loss: 0.0002420356104274904 | GradNorm^2: 9.795203795472321e-09 | s: 0.0002514794530592837 | s_nil + z = 0.0002516527980763808\n",
      "Loss: 0.00021818708031879562 | GradNorm^2: 7.196327759280979e-09 | s: 0.000235506879504671 | s_nil + z = 0.0002362572449939454\n",
      "Loss: 0.00019742530425323297 | GradNorm^2: 6.110271981046776e-09 | s: 0.00020903731683469208 | s_nil + z = 0.00021055427637402951\n",
      "Loss: 0.00017999475240888942 | GradNorm^2: 9.203104401325185e-09 | s: 0.0001946814265153906 | s_nil + z = 0.00019419675503336287\n",
      "Loss: 0.00016522500856516723 | GradNorm^2: 1.3397352172153071e-08 | s: 0.00017422915986309125 | s_nil + z = 0.0001749001459181651\n",
      "Loss: 0.00014835161390096665 | GradNorm^2: 2.907909022021408e-09 | s: 0.00015737635642529055 | s_nil + z = 0.00015614367889985048\n",
      "Loss: 0.00013567287661433772 | GradNorm^2: 2.5755876278013123e-09 | s: 0.00014020129209301458 | s_nil + z = 0.00014013983739730868\n",
      "Loss: 0.00012578630994185475 | GradNorm^2: 6.1205038230845865e-09 | s: 0.00013185540834312006 | s_nil + z = 0.00013193959766309715\n",
      "Loss: 0.0001144832879334828 | GradNorm^2: 1.5503471156234134e-09 | s: 0.00012101736558212306 | s_nil + z = 0.00012115673477085948\n",
      "Loss: 0.00010595707898594017 | GradNorm^2: 2.1035792508843653e-09 | s: 0.00011358207795548892 | s_nil + z = 0.00011433487086460331\n",
      "Loss: 9.774917711441622e-05 | GradNorm^2: 1.2118830589302182e-09 | s: 0.00010413904600971129 | s_nil + z = 0.00010476642284819315\n",
      "Loss: 9.053259379139196e-05 | GradNorm^2: 9.38361431409842e-10 | s: 9.419511193368765e-05 | s_nil + z = 9.449993202995804e-05\n",
      "Loss: 8.465028374065005e-05 | GradNorm^2: 1.6669192766992284e-09 | s: 8.79984774914912e-05 | s_nil + z = 8.778959535878095e-05\n",
      "Loss: 7.917842351796733e-05 | GradNorm^2: 1.803830937690301e-09 | s: 8.185241635896822e-05 | s_nil + z = 8.18978160119152e-05\n",
      "Loss: 7.345824368746336e-05 | GradNorm^2: 6.32130128809477e-10 | s: 7.74141875560901e-05 | s_nil + z = 7.716039331158442e-05\n",
      "Loss: 6.938410307245697e-05 | GradNorm^2: 1.2927464148131864e-09 | s: 7.236278077134449e-05 | s_nil + z = 7.199445995956927e-05\n",
      "Loss: 6.508004496298271e-05 | GradNorm^2: 9.230289959492942e-10 | s: 6.799719882883258e-05 | s_nil + z = 6.848173172482874e-05\n",
      "Loss: 6.1017342162464224e-05 | GradNorm^2: 3.6105381606294197e-10 | s: 6.271448731154724e-05 | s_nil + z = 6.287023484430332e-05\n",
      "Loss: 5.761914536810617e-05 | GradNorm^2: 3.140392802155594e-10 | s: 5.723491015258459e-05 | s_nil + z = 5.750661957375225e-05\n",
      "Loss: 5.469313238939992e-05 | GradNorm^2: 5.294975690106992e-10 | s: 5.470894659630175e-05 | s_nil + z = 5.511975388477336e-05\n",
      "Loss: 5.186069618429266e-05 | GradNorm^2: 4.635968541177558e-10 | s: 5.221671851098563e-05 | s_nil + z = 5.233174078666386e-05\n",
      "Loss: 4.906996076315226e-05 | GradNorm^2: 2.464572363058319e-10 | s: 4.8933401553471064e-05 | s_nil + z = 4.8390104200296546e-05\n",
      "Loss: 4.663373821627422e-05 | GradNorm^2: 1.9067044940447032e-10 | s: 4.747315736386008e-05 | s_nil + z = 4.765613890613668e-05\n",
      "Loss: 4.460249325090795e-05 | GradNorm^2: 3.19857026155771e-10 | s: 4.444877536038489e-05 | s_nil + z = 4.478835236869372e-05\n",
      "Loss: 4.243431231734175e-05 | GradNorm^2: 1.5605254594639598e-10 | s: 4.233757038343327e-05 | s_nil + z = 4.2439939399056825e-05\n",
      "Loss: 4.0591772445748103e-05 | GradNorm^2: 1.4020243663101078e-10 | s: 4.140811505367607e-05 | s_nil + z = 4.1156760313618146e-05\n",
      "Loss: 3.90444457651887e-05 | GradNorm^2: 2.490166620947967e-10 | s: 3.9169774459124215e-05 | s_nil + z = 3.929954079521375e-05\n",
      "Loss: 3.736541215896407e-05 | GradNorm^2: 1.1766715010279967e-10 | s: 3.720155690162736e-05 | s_nil + z = 3.745682702274017e-05\n",
      "Loss: 3.594838329344629e-05 | GradNorm^2: 1.0658202132528476e-10 | s: 3.53481349991568e-05 | s_nil + z = 3.5417349449284874e-05\n",
      "Loss: 3.4838843951157935e-05 | GradNorm^2: 2.56090800995016e-10 | s: 3.479117643788002e-05 | s_nil + z = 3.480477220896283e-05\n",
      "Loss: 3.393224734489184e-05 | GradNorm^2: 4.787448960423321e-10 | s: 3.2858199475008955e-05 | s_nil + z = 3.307101610457858e-05\n",
      "Loss: 3.2388319956112774e-05 | GradNorm^2: 1.361876214801055e-10 | s: 3.209493133498915e-05 | s_nil + z = 3.192520085613243e-05\n",
      "Loss: 3.144316865318371e-05 | GradNorm^2: 2.0119240728487196e-10 | s: 3.093004189317614e-05 | s_nil + z = 3.0673860969825106e-05\n",
      "Loss: 3.0713862159384984e-05 | GradNorm^2: 3.662084408150825e-10 | s: 3.0191294009531672e-05 | s_nil + z = 3.0238604440284422e-05\n",
      "Loss: 2.939312213795486e-05 | GradNorm^2: 7.087756820002042e-11 | s: 2.9541198781170666e-05 | s_nil + z = 2.966120343020351e-05\n",
      "Loss: 2.872766630022944e-05 | GradNorm^2: 1.935156090568441e-10 | s: 2.768553333690326e-05 | s_nil + z = 2.7818817659230888e-05\n",
      "Loss: 2.7752432150625605e-05 | GradNorm^2: 7.457780265280989e-11 | s: 2.785884404740792e-05 | s_nil + z = 2.786788414450153e-05\n",
      "Loss: 2.697888541320619e-05 | GradNorm^2: 6.210986025079537e-11 | s: 2.7951618696220785e-05 | s_nil + z = 2.7892350633842988e-05\n",
      "Loss: 2.6373700577951615e-05 | GradNorm^2: 1.277057453821798e-10 | s: 2.7072234427111286e-05 | s_nil + z = 2.6841257766335724e-05\n",
      "Loss: 2.557794094144064e-05 | GradNorm^2: 5.2066702106971214e-11 | s: 2.561554624882461e-05 | s_nil + z = 2.539785393362859e-05\n",
      "Loss: 2.4945954114163652e-05 | GradNorm^2: 5.440198589861188e-11 | s: 2.5355957195243596e-05 | s_nil + z = 2.5310894616110968e-05\n",
      "Loss: 2.4331880081719305e-05 | GradNorm^2: 4.8849254044208436e-11 | s: 2.481094429865203e-05 | s_nil + z = 2.435535949763673e-05\n",
      "Loss: 2.3903355944603105e-05 | GradNorm^2: 1.285643209609775e-10 | s: 2.414164163890675e-05 | s_nil + z = 2.425942391495739e-05\n",
      "Loss: 2.3210781213303282e-05 | GradNorm^2: 4.749508832716382e-11 | s: 2.3604148275211512e-05 | s_nil + z = 2.3406211753868034e-05\n",
      "Loss: 2.2722640588658162e-05 | GradNorm^2: 6.500404603650629e-11 | s: 2.3305776859192336e-05 | s_nil + z = 2.3150506505642543e-05\n",
      "Loss: 2.2176280682455098e-05 | GradNorm^2: 3.8752906755498646e-11 | s: 2.255591920121403e-05 | s_nil + z = 2.2598443863749984e-05\n",
      "Loss: 2.1717439032997358e-05 | GradNorm^2: 4.6945592209733846e-11 | s: 2.1431928035527856e-05 | s_nil + z = 2.140186462702004e-05\n",
      "Loss: 2.124079945304977e-05 | GradNorm^2: 3.5304477190330055e-11 | s: 2.139523189645e-05 | s_nil + z = 2.109891837595579e-05\n",
      "Loss: 2.080873493230898e-05 | GradNorm^2: 3.643368851969184e-11 | s: 2.024176293815695e-05 | s_nil + z = 2.03633057482e-05\n",
      "Loss: 2.038752400597616e-05 | GradNorm^2: 3.33751871460706e-11 | s: 1.9860624922014262e-05 | s_nil + z = 1.9758175366105246e-05\n",
      "Loss: 2.0023460986799414e-05 | GradNorm^2: 4.9892481253187894e-11 | s: 1.969381582769864e-05 | s_nil + z = 1.9772245953453282e-05\n",
      "Loss: 1.971528923853867e-05 | GradNorm^2: 8.343678519914637e-11 | s: 1.9424352733055057e-05 | s_nil + z = 1.954112261891988e-05\n",
      "Loss: 1.924142964852381e-05 | GradNorm^2: 3.4530475514971034e-11 | s: 1.8814886904460832e-05 | s_nil + z = 1.8918376822711636e-05\n",
      "Loss: 1.8872376260909563e-05 | GradNorm^2: 2.7578786737308496e-11 | s: 1.8948178862842264e-05 | s_nil + z = 1.9029801552196413e-05\n",
      "Loss: 1.8551504061085344e-05 | GradNorm^2: 3.631429127884081e-11 | s: 1.8553179394868792e-05 | s_nil + z = 1.8511875088569885e-05\n",
      "Loss: 1.820805158869375e-05 | GradNorm^2: 2.94792722151942e-11 | s: 1.8351806158271507e-05 | s_nil + z = 1.835169927343755e-05\n",
      "Loss: 1.7882521340327943e-05 | GradNorm^2: 2.4873174470982796e-11 | s: 1.788434046881896e-05 | s_nil + z = 1.803402174705565e-05\n",
      "Loss: 1.757449345788454e-05 | GradNorm^2: 2.379209413365042e-11 | s: 1.759438892294533e-05 | s_nil + z = 1.73356309604647e-05\n",
      "Loss: 1.728684109830909e-05 | GradNorm^2: 2.629038016671159e-11 | s: 1.7770140324937777e-05 | s_nil + z = 1.78799210669503e-05\n",
      "Loss: 1.7003054892641573e-05 | GradNorm^2: 2.63229806085204e-11 | s: 1.7109784605153615e-05 | s_nil + z = 1.7234104834472352e-05\n",
      "Loss: 1.6767757365024442e-05 | GradNorm^2: 4.1418715946717464e-11 | s: 1.6702789805146913e-05 | s_nil + z = 1.6795602509160623e-05\n",
      "Loss: 1.6490342967689736e-05 | GradNorm^2: 3.650498364506587e-11 | s: 1.6294957679847764e-05 | s_nil + z = 1.6388007707222874e-05\n",
      "Loss: 1.6191038700332948e-05 | GradNorm^2: 2.0180488932621793e-11 | s: 1.6312170024896603e-05 | s_nil + z = 1.6353569271279622e-05\n",
      "Loss: 1.5943682175298603e-05 | GradNorm^2: 2.0588025810293576e-11 | s: 1.63912624584487e-05 | s_nil + z = 1.646632401677145e-05\n",
      "Loss: 1.570174597187407e-05 | GradNorm^2: 1.997367770365683e-11 | s: 1.5932790222983246e-05 | s_nil + z = 1.5985323510950278e-05\n",
      "Loss: 1.546489764273436e-05 | GradNorm^2: 1.874310425551317e-11 | s: 1.56895274364627e-05 | s_nil + z = 1.569855263994918e-05\n",
      "Loss: 1.5235994113708908e-05 | GradNorm^2: 1.7739577222232628e-11 | s: 1.5215287230151551e-05 | s_nil + z = 1.5133134679316818e-05\n",
      "Loss: 1.5025417103421163e-05 | GradNorm^2: 2.0910511345305193e-11 | s: 1.528512866251078e-05 | s_nil + z = 1.531569379588371e-05\n",
      "Loss: 1.4829754068837877e-05 | GradNorm^2: 2.6807320779405862e-11 | s: 1.5020354578075089e-05 | s_nil + z = 1.5119483620665286e-05\n",
      "Loss: 1.4621147221753308e-05 | GradNorm^2: 2.5998637076021222e-11 | s: 1.434233031245341e-05 | s_nil + z = 1.4203543141466657e-05\n",
      "Loss: 1.4395407441978235e-05 | GradNorm^2: 1.7453583811928845e-11 | s: 1.4192519237996087e-05 | s_nil + z = 1.4281620481393364e-05\n",
      "Loss: 1.4216351163534443e-05 | GradNorm^2: 2.2788534325980916e-11 | s: 1.450416624741676e-05 | s_nil + z = 1.4413938657746594e-05\n",
      "Loss: 1.4015815314750772e-05 | GradNorm^2: 1.89369675960989e-11 | s: 1.3983891060183919e-05 | s_nil + z = 1.4096864162226053e-05\n",
      "Loss: 1.3826087330883735e-05 | GradNorm^2: 1.7352522011439994e-11 | s: 1.3642721135477704e-05 | s_nil + z = 1.371498176432756e-05\n",
      "Loss: 1.3667010524997479e-05 | GradNorm^2: 2.3883152306913003e-11 | s: 1.413077807780439e-05 | s_nil + z = 1.4143054446365266e-05\n",
      "Loss: 1.3460686518042359e-05 | GradNorm^2: 1.3810215622200743e-11 | s: 1.3711356853041825e-05 | s_nil + z = 1.359914466271539e-05\n",
      "Loss: 1.3301996037084092e-05 | GradNorm^2: 1.7419980354333156e-11 | s: 1.3289422748791436e-05 | s_nil + z = 1.3364468939541796e-05\n",
      "Loss: 1.3131023160724122e-05 | GradNorm^2: 1.5804023558487517e-11 | s: 1.283809249739124e-05 | s_nil + z = 1.2795918993897815e-05\n",
      "Loss: 1.2959495102761329e-05 | GradNorm^2: 1.2746456810852462e-11 | s: 1.278229663658142e-05 | s_nil + z = 1.2854098859267638e-05\n",
      "Loss: 1.2805233040317922e-05 | GradNorm^2: 1.3743203319108783e-11 | s: 1.270782942063181e-05 | s_nil + z = 1.2779650369589002e-05\n",
      "Loss: 1.264626613347345e-05 | GradNorm^2: 1.2198462955324565e-11 | s: 1.2325148122137206e-05 | s_nil + z = 1.2364649212594225e-05\n",
      "Loss: 1.249547011975583e-05 | GradNorm^2: 1.1933842824108332e-11 | s: 1.2592883771470911e-05 | s_nil + z = 1.2679801108465838e-05\n",
      "Loss: 1.2348873451487811e-05 | GradNorm^2: 1.1819556621943212e-11 | s: 1.2202690719612523e-05 | s_nil + z = 1.2293991099886215e-05\n",
      "Loss: 1.2204811283453869e-05 | GradNorm^2: 1.1410353917446271e-11 | s: 1.2144308383518745e-05 | s_nil + z = 1.217258857513781e-05\n",
      "Loss: 1.2063865177898455e-05 | GradNorm^2: 1.0971365004841989e-11 | s: 1.1904462296477109e-05 | s_nil + z = 1.193080766578869e-05\n",
      "Loss: 1.1933102598626549e-05 | GradNorm^2: 1.2425464592898911e-11 | s: 1.179997886176143e-05 | s_nil + z = 1.183986450856156e-05\n",
      "Loss: 1.1793115064872307e-05 | GradNorm^2: 1.0488299496194022e-11 | s: 1.1822167577120525e-05 | s_nil + z = 1.1913283484564496e-05\n",
      "Loss: 1.1664198673823561e-05 | GradNorm^2: 1.0793628386552875e-11 | s: 1.170197725359859e-05 | s_nil + z = 1.1705915227526453e-05\n",
      "Loss: 1.1534507426122611e-05 | GradNorm^2: 1.0080864558267698e-11 | s: 1.142363869972078e-05 | s_nil + z = 1.1494417223097131e-05\n",
      "Loss: 1.1416032693914464e-05 | GradNorm^2: 1.1633917483527676e-11 | s: 1.090345218858218e-05 | s_nil + z = 1.0920814093755282e-05\n",
      "Loss: 1.1286734178275302e-05 | GradNorm^2: 9.587531114614812e-12 | s: 1.1187523417388331e-05 | s_nil + z = 1.1253469129696159e-05\n",
      "Loss: 1.1170838902420935e-05 | GradNorm^2: 1.0358904830831519e-11 | s: 1.1030887699042118e-05 | s_nil + z = 1.1043509844009536e-05\n",
      "Loss: 1.1050083978767124e-05 | GradNorm^2: 9.233740832745173e-12 | s: 1.1211592116898691e-05 | s_nil + z = 1.1262529720536263e-05\n",
      "Loss: 1.0938130209110949e-05 | GradNorm^2: 9.723992351355557e-12 | s: 1.1004167781412778e-05 | s_nil + z = 1.1056559639524566e-05\n",
      "Loss: 1.0837505910515608e-05 | GradNorm^2: 1.2486537250080616e-11 | s: 1.0662072010795519e-05 | s_nil + z = 1.060072993964295e-05\n",
      "Loss: 1.071402967990618e-05 | GradNorm^2: 8.817524611185415e-12 | s: 1.0418037000290233e-05 | s_nil + z = 1.0458664720831845e-05\n",
      "Loss: 1.0609871773462105e-05 | GradNorm^2: 9.5015555195854e-12 | s: 1.0439978230095048e-05 | s_nil + z = 1.0478533787640354e-05\n",
      "Loss: 1.0504978260174157e-05 | GradNorm^2: 9.458369285176193e-12 | s: 1.0211140775087155e-05 | s_nil + z = 1.022394902262802e-05\n",
      "Loss: 1.039680692784171e-05 | GradNorm^2: 8.088787831831468e-12 | s: 1.0462566989895729e-05 | s_nil + z = 1.0379660553574976e-05\n",
      "Loss: 1.0295725655430612e-05 | GradNorm^2: 7.937802892678748e-12 | s: 1.041110257795769e-05 | s_nil + z = 1.04835818234561e-05\n",
      "Loss: 1.0196839050992453e-05 | GradNorm^2: 7.841140184818093e-12 | s: 1.0133082116643285e-05 | s_nil + z = 1.0195846220223403e-05\n",
      "Loss: 1.0102262646650698e-05 | GradNorm^2: 8.316671416287015e-12 | s: 1.0140578551249261e-05 | s_nil + z = 1.0201293659188593e-05\n",
      "Loss: 1.0007336671869246e-05 | GradNorm^2: 8.275683086691701e-12 | s: 1.0022123022485167e-05 | s_nil + z = 9.986720525941017e-06\n",
      "Loss: 9.912690980869195e-06 | GradNorm^2: 7.882164971440591e-12 | s: 9.838857341653804e-06 | s_nil + z = 9.911303038727393e-06\n",
      "Loss: 9.81862712672434e-06 | GradNorm^2: 7.212737902684824e-12 | s: 9.861822526068629e-06 | s_nil + z = 9.917218316356224e-06\n",
      "Loss: 9.733347344773073e-06 | GradNorm^2: 8.165563196515353e-12 | s: 1.0112269909167061e-05 | s_nil + z = 1.0172137157843375e-05\n",
      "Loss: 9.647292114783278e-06 | GradNorm^2: 8.577731069370726e-12 | s: 9.68524022208877e-06 | s_nil + z = 9.640787016908237e-06\n",
      "Loss: 9.55500180208646e-06 | GradNorm^2: 7.197804393284494e-12 | s: 9.71886433980624e-06 | s_nil + z = 9.763457021978728e-06\n",
      "Loss: 9.472946464985832e-06 | GradNorm^2: 7.777352967277522e-12 | s: 9.793319314313784e-06 | s_nil + z = 9.767631839283086e-06\n",
      "Loss: 9.38770954187529e-06 | GradNorm^2: 7.286165004921701e-12 | s: 9.466611955935423e-06 | s_nil + z = 9.526831360670562e-06\n",
      "Loss: 9.302358000179584e-06 | GradNorm^2: 6.443642137422469e-12 | s: 9.29229943269714e-06 | s_nil + z = 9.36049500638303e-06\n",
      "Loss: 9.226509096876505e-06 | GradNorm^2: 7.36873625725596e-12 | s: 9.46520307354602e-06 | s_nil + z = 9.42165592415084e-06\n",
      "Loss: 9.14731188704453e-06 | GradNorm^2: 7.260558967225094e-12 | s: 9.1938046938591e-06 | s_nil + z = 9.18276584919997e-06\n",
      "Loss: 9.064950942178148e-06 | GradNorm^2: 6.179729573159923e-12 | s: 9.14851532126759e-06 | s_nil + z = 9.197740903008067e-06\n",
      "Loss: 8.988807227258732e-06 | GradNorm^2: 6.164646972071273e-12 | s: 8.793041448235569e-06 | s_nil + z = 8.72951764932517e-06\n",
      "Loss: 8.919042057609835e-06 | GradNorm^2: 7.193253282113231e-12 | s: 9.147032921952317e-06 | s_nil + z = 9.179942338857772e-06\n",
      "Loss: 8.842279188822227e-06 | GradNorm^2: 6.504142873645574e-12 | s: 9.029358253642292e-06 | s_nil + z = 9.038038810305071e-06\n",
      "Loss: 8.770596332091326e-06 | GradNorm^2: 6.596378014891094e-12 | s: 8.59561684374204e-06 | s_nil + z = 8.655167970453292e-06\n",
      "Loss: 8.696536956792151e-06 | GradNorm^2: 5.986730976930354e-12 | s: 8.414909825525779e-06 | s_nil + z = 8.421420928945988e-06\n",
      "Loss: 8.624949474489394e-06 | GradNorm^2: 5.632006377027453e-12 | s: 8.561778643449373e-06 | s_nil + z = 8.571059949287474e-06\n",
      "Loss: 8.555195177447292e-06 | GradNorm^2: 5.435768130714878e-12 | s: 8.288804330622923e-06 | s_nil + z = 8.34688229187086e-06\n",
      "Loss: 8.487315364370108e-06 | GradNorm^2: 5.3858083737774876e-12 | s: 8.440102419602007e-06 | s_nil + z = 8.497648052878393e-06\n",
      "Loss: 8.420111663298442e-06 | GradNorm^2: 5.256632177646464e-12 | s: 8.345000590334373e-06 | s_nil + z = 8.276361993776737e-06\n",
      "Loss: 8.35415873860487e-06 | GradNorm^2: 5.174358072112479e-12 | s: 8.158926008924212e-06 | s_nil + z = 8.176337655707577e-06\n",
      "Loss: 8.291094033157468e-06 | GradNorm^2: 5.441493217940166e-12 | s: 8.242144268512679e-06 | s_nil + z = 8.239622486062859e-06\n",
      "Loss: 8.225653010729908e-06 | GradNorm^2: 5.062566859354698e-12 | s: 8.329725369489298e-06 | s_nil + z = 8.238605818944311e-06\n",
      "Loss: 8.163466658577184e-06 | GradNorm^2: 5.1082255547024125e-12 | s: 8.173475427004339e-06 | s_nil + z = 8.139692657248287e-06\n",
      "Loss: 8.106140149019506e-06 | GradNorm^2: 5.882292935332363e-12 | s: 8.026539217049035e-06 | s_nil + z = 8.04463451612586e-06\n",
      "Loss: 8.04005123431435e-06 | GradNorm^2: 4.811196646853702e-12 | s: 8.167996224966946e-06 | s_nil + z = 8.223718814544256e-06\n",
      "Loss: 7.980169706162022e-06 | GradNorm^2: 4.7592707584195134e-12 | s: 8.027005049137736e-06 | s_nil + z = 8.068568634563011e-06\n",
      "Loss: 7.920795564235621e-06 | GradNorm^2: 4.641082966047627e-12 | s: 7.831114882538476e-06 | s_nil + z = 7.825952086044918e-06\n",
      "Loss: 7.8632692771931e-06 | GradNorm^2: 4.7038305169515335e-12 | s: 7.84712252559358e-06 | s_nil + z = 7.829430653839842e-06\n",
      "Loss: 7.808897581352326e-06 | GradNorm^2: 5.168067726324143e-12 | s: 7.72224582652845e-06 | s_nil + z = 7.768973500660303e-06\n",
      "Loss: 7.74879289066758e-06 | GradNorm^2: 4.438135585830968e-12 | s: 7.592954441490979e-06 | s_nil + z = 7.582764117795848e-06\n",
      "Loss: 7.69319493965401e-06 | GradNorm^2: 4.388522407651056e-12 | s: 7.6751168198137e-06 | s_nil + z = 7.656903149702576e-06\n",
      "Loss: 7.639663284024907e-06 | GradNorm^2: 4.554269717815872e-12 | s: 7.97971748441563e-06 | s_nil + z = 7.990755005481132e-06\n",
      "Loss: 7.5842715799844385e-06 | GradNorm^2: 4.248687946025228e-12 | s: 7.771644012301928e-06 | s_nil + z = 7.817879330983268e-06\n",
      "Loss: 7.5333759990700865e-06 | GradNorm^2: 4.599155066819367e-12 | s: 7.641465628995185e-06 | s_nil + z = 7.692696912073305e-06\n",
      "Loss: 7.480569718777911e-06 | GradNorm^2: 4.492098808114205e-12 | s: 7.539924958972538e-06 | s_nil + z = 7.460300117237417e-06\n",
      "Loss: 7.426834245745468e-06 | GradNorm^2: 4.0872112833966375e-12 | s: 7.370492485759446e-06 | s_nil + z = 7.361366452122986e-06\n",
      "Loss: 7.376256812462827e-06 | GradNorm^2: 4.106780102424471e-12 | s: 7.19046384435348e-06 | s_nil + z = 7.224054652914258e-06\n",
      "Loss: 7.325442828366697e-06 | GradNorm^2: 3.965081355974047e-12 | s: 7.219710133127145e-06 | s_nil + z = 7.2820533396009135e-06\n",
      "Loss: 7.2767736340691994e-06 | GradNorm^2: 4.0757441310483924e-12 | s: 7.341227735651594e-06 | s_nil + z = 7.341857847571742e-06\n",
      "Loss: 7.229736950606343e-06 | GradNorm^2: 4.3444827709824606e-12 | s: 7.393816523309062e-06 | s_nil + z = 7.421129233797785e-06\n",
      "Loss: 7.17859144560232e-06 | GradNorm^2: 3.807505251066194e-12 | s: 6.946542108186805e-06 | s_nil + z = 6.975970407774312e-06\n",
      "Loss: 7.130862071398691e-06 | GradNorm^2: 3.7482661768894735e-12 | s: 7.144478084312535e-06 | s_nil + z = 7.1611668461049605e-06\n",
      "Loss: 7.083891745050852e-06 | GradNorm^2: 3.708989813783746e-12 | s: 7.087919702500816e-06 | s_nil + z = 7.117991490522557e-06\n",
      "Loss: 7.038456522247595e-06 | GradNorm^2: 3.814050389489284e-12 | s: 6.899640912735921e-06 | s_nil + z = 6.9058349349282455e-06\n",
      "Loss: 6.993010842562168e-06 | GradNorm^2: 3.822279491200421e-12 | s: 6.862485714939251e-06 | s_nil + z = 6.81623135748155e-06\n",
      "Loss: 6.946502346435285e-06 | GradNorm^2: 3.559322485315331e-12 | s: 7.080250766063624e-06 | s_nil + z = 7.033363022157513e-06\n",
      "Loss: 6.9021278668383174e-06 | GradNorm^2: 3.542137820745064e-12 | s: 6.823112597616333e-06 | s_nil + z = 6.861706303339099e-06\n",
      "Loss: 6.8583561671624035e-06 | GradNorm^2: 3.528819625830212e-12 | s: 6.967901669697716e-06 | s_nil + z = 6.995206372324703e-06\n",
      "Loss: 6.814648995533219e-06 | GradNorm^2: 3.4360867700941787e-12 | s: 6.795652221881044e-06 | s_nil + z = 6.847374938813433e-06\n",
      "Loss: 6.771723202179478e-06 | GradNorm^2: 3.3813379962950846e-12 | s: 6.625289989986794e-06 | s_nil + z = 6.597230689742162e-06\n",
      "Loss: 6.730289665278212e-06 | GradNorm^2: 3.4790583750234512e-12 | s: 6.6647344054480255e-06 | s_nil + z = 6.63737202444975e-06\n",
      "Loss: 6.689612790121565e-06 | GradNorm^2: 3.6113452637776843e-12 | s: 6.878618767796518e-06 | s_nil + z = 6.919709280163444e-06\n",
      "Loss: 6.646340630702497e-06 | GradNorm^2: 3.2503931344407057e-12 | s: 6.651518253302268e-06 | s_nil + z = 6.688831290353747e-06\n",
      "Loss: 6.606160277578163e-06 | GradNorm^2: 3.2980616660094643e-12 | s: 6.8704547580995985e-06 | s_nil + z = 6.892894084765955e-06\n",
      "Loss: 6.569105680414711e-06 | GradNorm^2: 3.74891534483048e-12 | s: 6.900565938526006e-06 | s_nil + z = 6.925053192746722e-06\n",
      "Loss: 6.5257043306681196e-06 | GradNorm^2: 3.1419209342205384e-12 | s: 6.529707681010978e-06 | s_nil + z = 6.566375704453176e-06\n",
      "Loss: 6.486576129067233e-06 | GradNorm^2: 3.1265719999489394e-12 | s: 6.430043754709867e-06 | s_nil + z = 6.380024588206745e-06\n",
      "Loss: 6.449466165211691e-06 | GradNorm^2: 3.347220958655873e-12 | s: 6.575155993889891e-06 | s_nil + z = 6.525012100903893e-06\n",
      "Loss: 6.409962511708747e-06 | GradNorm^2: 3.1277256587918357e-12 | s: 6.357450227075441e-06 | s_nil + z = 6.383199957785135e-06\n",
      "Loss: 6.372165830370576e-06 | GradNorm^2: 3.0940779586526664e-12 | s: 6.277755911711415e-06 | s_nil + z = 6.319618403347049e-06\n",
      "Loss: 6.334750624215469e-06 | GradNorm^2: 3.0540059209511176e-12 | s: 6.177040834788208e-06 | s_nil + z = 6.222539239812697e-06\n",
      "Loss: 6.297411262639209e-06 | GradNorm^2: 2.9604561594024185e-12 | s: 6.294538896134975e-06 | s_nil + z = 6.31730140601694e-06\n",
      "Loss: 6.2607081582498326e-06 | GradNorm^2: 2.899949577938132e-12 | s: 6.126011395107952e-06 | s_nil + z = 6.0699983000442326e-06\n",
      "Loss: 6.224491857662542e-06 | GradNorm^2: 2.8500642310317066e-12 | s: 6.225272902515542e-06 | s_nil + z = 6.1781640464872445e-06\n",
      "Loss: 6.189107680468798e-06 | GradNorm^2: 2.860960825329465e-12 | s: 6.104940920858381e-06 | s_nil + z = 6.058418420669398e-06\n",
      "Loss: 6.153538399811533e-06 | GradNorm^2: 2.7837179214597318e-12 | s: 6.202999123282537e-06 | s_nil + z = 6.214317805626479e-06\n",
      "Loss: 6.118776149757617e-06 | GradNorm^2: 2.7648608800289044e-12 | s: 6.135155635271291e-06 | s_nil + z = 6.1109964192909705e-06\n",
      "Loss: 6.085123724287654e-06 | GradNorm^2: 2.845567801888588e-12 | s: 6.28266718794364e-06 | s_nil + z = 6.29204586186094e-06\n",
      "Loss: 6.05018176916882e-06 | GradNorm^2: 2.688039646726054e-12 | s: 6.190764018229897e-06 | s_nil + z = 6.207904551552845e-06\n",
      "Loss: 6.016493621070389e-06 | GradNorm^2: 2.657367724278135e-12 | s: 6.109472745893658e-06 | s_nil + z = 6.096835603042194e-06\n",
      "Loss: 5.984708036105387e-06 | GradNorm^2: 2.8378188857888766e-12 | s: 6.0865834741374695e-06 | s_nil + z = 6.103004654381405e-06\n",
      "Loss: 5.9513149811844144e-06 | GradNorm^2: 2.7394501495392665e-12 | s: 6.058900190954559e-06 | s_nil + z = 6.091890723168569e-06\n",
      "Loss: 5.9179948289580334e-06 | GradNorm^2: 2.603784104897546e-12 | s: 6.043501977050177e-06 | s_nil + z = 6.042227707024617e-06\n",
      "Loss: 5.885759238955288e-06 | GradNorm^2: 2.5720061976113818e-12 | s: 5.854811245010548e-06 | s_nil + z = 5.882854911869184e-06\n",
      "Loss: 5.853786323399764e-06 | GradNorm^2: 2.522198942916553e-12 | s: 5.838437249750449e-06 | s_nil + z = 5.878163021597049e-06\n",
      "Loss: 5.822251070129143e-06 | GradNorm^2: 2.4867179405104517e-12 | s: 5.697949018612687e-06 | s_nil + z = 5.736227786837245e-06\n",
      "Loss: 5.79131047796404e-06 | GradNorm^2: 2.4861102381819696e-12 | s: 5.662607269324047e-06 | s_nil + z = 5.7041481387257176e-06\n",
      "Loss: 5.760368692330106e-06 | GradNorm^2: 2.441367869467136e-12 | s: 5.860899500375338e-06 | s_nil + z = 5.88472409121521e-06\n",
      "Loss: 5.729891197395902e-06 | GradNorm^2: 2.4187470394820272e-12 | s: 5.8246301633967015e-06 | s_nil + z = 5.778106410954387e-06\n",
      "Loss: 5.701617342882243e-06 | GradNorm^2: 2.64457139907115e-12 | s: 5.5699372864298215e-06 | s_nil + z = 5.566739563034329e-06\n",
      "Loss: 5.6699749560468334e-06 | GradNorm^2: 2.376109154135603e-12 | s: 5.679620477048271e-06 | s_nil + z = 5.697700905610281e-06\n",
      "Loss: 5.640338133341484e-06 | GradNorm^2: 2.334956264219312e-12 | s: 5.60908145794425e-06 | s_nil + z = 5.6161128822243556e-06\n",
      "Loss: 5.611163967439238e-06 | GradNorm^2: 2.3145994549835684e-12 | s: 5.802639859611493e-06 | s_nil + z = 5.792084106968909e-06\n",
      "Loss: 5.583163009483681e-06 | GradNorm^2: 2.407920121982896e-12 | s: 5.640364601363535e-06 | s_nil + z = 5.679697678860618e-06\n",
      "Loss: 5.55406211783273e-06 | GradNorm^2: 2.318121759968705e-12 | s: 5.774879657956551e-06 | s_nil + z = 5.791931843059095e-06\n",
      "Loss: 5.525267486307738e-06 | GradNorm^2: 2.2321968726688216e-12 | s: 5.583844169416847e-06 | s_nil + z = 5.5840753450039915e-06\n",
      "Loss: 5.497583373527831e-06 | GradNorm^2: 2.2524201026598936e-12 | s: 5.715692227437879e-06 | s_nil + z = 5.75209999852111e-06\n",
      "Loss: 5.469808555484845e-06 | GradNorm^2: 2.2226911216988913e-12 | s: 5.417331767937731e-06 | s_nil + z = 5.441981488801817e-06\n",
      "Loss: 5.443774635207396e-06 | GradNorm^2: 2.37965658869491e-12 | s: 5.662628843394695e-06 | s_nil + z = 5.616325402010847e-06\n",
      "Loss: 5.417549348588806e-06 | GradNorm^2: 2.4740084924077258e-12 | s: 5.726737277871939e-06 | s_nil + z = 5.749945964424186e-06\n",
      "Loss: 5.388171925087985e-06 | GradNorm^2: 2.128809370294189e-12 | s: 5.320077378888376e-06 | s_nil + z = 5.337170742740482e-06\n",
      "Loss: 5.361691221127601e-06 | GradNorm^2: 2.117111505827723e-12 | s: 5.394523112781652e-06 | s_nil + z = 5.421027845967097e-06\n",
      "Loss: 5.335800871579125e-06 | GradNorm^2: 2.149961112917908e-12 | s: 5.1633243380335825e-06 | s_nil + z = 5.193197636178815e-06\n",
      "Loss: 5.309137356226986e-06 | GradNorm^2: 2.0587656369198427e-12 | s: 5.292241911915933e-06 | s_nil + z = 5.278205942260694e-06\n",
      "Loss: 5.284666738157602e-06 | GradNorm^2: 2.206357484734381e-12 | s: 5.441447738442832e-06 | s_nil + z = 5.447448917159092e-06\n",
      "Loss: 5.2594005108160116e-06 | GradNorm^2: 2.222884719889694e-12 | s: 5.309202390508848e-06 | s_nil + z = 5.345548941118605e-06\n",
      "Loss: 5.233493392151732e-06 | GradNorm^2: 2.1316840416629516e-12 | s: 5.206925978413382e-06 | s_nil + z = 5.2411377967492845e-06\n",
      "Loss: 5.2073188060589754e-06 | GradNorm^2: 1.9793361274227693e-12 | s: 5.1208452563847435e-06 | s_nil + z = 5.071781185361779e-06\n",
      "Loss: 5.183045201339203e-06 | GradNorm^2: 2.0296751409373744e-12 | s: 5.017580195182249e-06 | s_nil + z = 5.042728594265022e-06\n",
      "Loss: 5.1583474739098065e-06 | GradNorm^2: 1.9996332346506208e-12 | s: 5.077808540082086e-06 | s_nil + z = 5.061435557569827e-06\n",
      "Loss: 5.1337227277714976e-06 | GradNorm^2: 1.9453112200064866e-12 | s: 5.0623265959267496e-06 | s_nil + z = 5.099045010184544e-06\n",
      "Loss: 5.1094893603487255e-06 | GradNorm^2: 1.91434325416579e-12 | s: 5.267518706928764e-06 | s_nil + z = 5.275688518678601e-06\n",
      "Loss: 5.085707030298273e-06 | GradNorm^2: 1.910870188186074e-12 | s: 5.037518468240418e-06 | s_nil + z = 5.068561901293289e-06\n",
      "Loss: 5.061807496513519e-06 | GradNorm^2: 1.868294868798803e-12 | s: 4.992946514562078e-06 | s_nil + z = 4.990750951977541e-06\n",
      "Loss: 5.03894982108472e-06 | GradNorm^2: 1.919876155259392e-12 | s: 5.026895683770821e-06 | s_nil + z = 5.047961031005854e-06\n",
      "Loss: 5.015147875023463e-06 | GradNorm^2: 1.834464582192356e-12 | s: 4.913497589272548e-06 | s_nil + z = 4.915208699489415e-06\n",
      "Loss: 4.992197687825722e-06 | GradNorm^2: 1.826396948671913e-12 | s: 5.089840598034125e-06 | s_nil + z = 5.071223428798621e-06\n",
      "Loss: 4.969546425479659e-06 | GradNorm^2: 1.8194665762449357e-12 | s: 5.089103264644035e-06 | s_nil + z = 5.101181730880755e-06\n",
      "Loss: 4.946818444433769e-06 | GradNorm^2: 1.786819802973325e-12 | s: 4.811446430485789e-06 | s_nil + z = 4.813287874959327e-06\n",
      "Loss: 4.924527559308359e-06 | GradNorm^2: 1.7818106060199507e-12 | s: 4.906678915344454e-06 | s_nil + z = 4.928896833775176e-06\n",
      "Loss: 4.902215815504993e-06 | GradNorm^2: 1.7506475050050187e-12 | s: 5.026615554364344e-06 | s_nil + z = 5.027933000417706e-06\n",
      "Loss: 4.880251854914265e-06 | GradNorm^2: 1.7352124378806342e-12 | s: 4.907346068475892e-06 | s_nil + z = 4.903240257682351e-06\n",
      "Loss: 4.858641164673705e-06 | GradNorm^2: 1.7370486149430874e-12 | s: 4.8960470932251505e-06 | s_nil + z = 4.875045185724625e-06\n",
      "Loss: 4.837209654973019e-06 | GradNorm^2: 1.7346009420152654e-12 | s: 5.069633467637729e-06 | s_nil + z = 5.110334157495955e-06\n",
      "Loss: 4.815970296301329e-06 | GradNorm^2: 1.735759039888661e-12 | s: 4.767529925205088e-06 | s_nil + z = 4.804666599432517e-06\n",
      "Loss: 4.794357463158194e-06 | GradNorm^2: 1.6737354897344955e-12 | s: 4.779919739690136e-06 | s_nil + z = 4.7967550906719e-06\n",
      "Loss: 4.774209240156921e-06 | GradNorm^2: 1.7540490020792487e-12 | s: 4.7624624166243655e-06 | s_nil + z = 4.694720997899804e-06\n",
      "Loss: 4.753392399854167e-06 | GradNorm^2: 1.7431077708251055e-12 | s: 4.8698957624767654e-06 | s_nil + z = 4.76682011452817e-06\n",
      "Loss: 4.731876259751425e-06 | GradNorm^2: 1.6301490923474027e-12 | s: 4.798779366876128e-06 | s_nil + z = 4.832224475788405e-06\n",
      "Loss: 4.711649024439278e-06 | GradNorm^2: 1.6413332736355114e-12 | s: 4.5968488831859205e-06 | s_nil + z = 4.613575718542817e-06\n",
      "Loss: 4.691879011351581e-06 | GradNorm^2: 1.6833886620238903e-12 | s: 4.698939901580135e-06 | s_nil + z = 4.714585974570362e-06\n",
      "Loss: 4.671089249709207e-06 | GradNorm^2: 1.589489875280993e-12 | s: 4.630464604474522e-06 | s_nil + z = 4.648515741778603e-06\n",
      "Loss: 4.651225391152856e-06 | GradNorm^2: 1.582474792737101e-12 | s: 4.711566939821182e-06 | s_nil + z = 4.659446140607704e-06\n",
      "Loss: 4.631381403935258e-06 | GradNorm^2: 1.5605969657588298e-12 | s: 4.583880459664926e-06 | s_nil + z = 4.542507117122341e-06\n",
      "Loss: 4.611839925251584e-06 | GradNorm^2: 1.552380351885346e-12 | s: 4.526847998621339e-06 | s_nil + z = 4.460921315639632e-06\n",
      "Loss: 4.592580607788774e-06 | GradNorm^2: 1.557217339921278e-12 | s: 4.598431396391378e-06 | s_nil + z = 4.61710453142361e-06\n",
      "Loss: 4.573139081783899e-06 | GradNorm^2: 1.521944108403112e-12 | s: 4.5534074122106856e-06 | s_nil + z = 4.586110233545382e-06\n",
      "Loss: 4.5541243424798795e-06 | GradNorm^2: 1.518752915401841e-12 | s: 4.522282455947914e-06 | s_nil + z = 4.487973730524231e-06\n",
      "Loss: 4.535116688914681e-06 | GradNorm^2: 1.4950911827175282e-12 | s: 4.62617735270504e-06 | s_nil + z = 4.629061695009262e-06\n",
      "Loss: 4.516392413677811e-06 | GradNorm^2: 1.4852116815368974e-12 | s: 4.654080335561774e-06 | s_nil + z = 4.666383575206712e-06\n",
      "Loss: 4.49777710940533e-06 | GradNorm^2: 1.470218333510844e-12 | s: 4.4343205020912366e-06 | s_nil + z = 4.458902620123328e-06\n",
      "Loss: 4.479343367707621e-06 | GradNorm^2: 1.4615087175283994e-12 | s: 4.392997612830407e-06 | s_nil + z = 4.395536078484474e-06\n",
      "Loss: 4.461090683625295e-06 | GradNorm^2: 1.45336963023744e-12 | s: 4.4291715424645995e-06 | s_nil + z = 4.449105301437828e-06\n",
      "Loss: 4.4428683555483445e-06 | GradNorm^2: 1.4350145949818042e-12 | s: 4.4643958603322315e-06 | s_nil + z = 4.467363969667959e-06\n",
      "Loss: 4.424936511497072e-06 | GradNorm^2: 1.430641096798851e-12 | s: 4.456517927164876e-06 | s_nil + z = 4.481488240094811e-06\n",
      "Loss: 4.406999017486787e-06 | GradNorm^2: 1.4108637892933923e-12 | s: 4.2884345478494855e-06 | s_nil + z = 4.3091660075073035e-06\n",
      "Loss: 4.3892909762770475e-06 | GradNorm^2: 1.3998096970940037e-12 | s: 4.375069200600768e-06 | s_nil + z = 4.391529368347191e-06\n",
      "Loss: 4.372064185650139e-06 | GradNorm^2: 1.420285566673427e-12 | s: 4.392854980121281e-06 | s_nil + z = 4.389894241667303e-06\n",
      "Loss: 4.354377548323842e-06 | GradNorm^2: 1.3773551077017814e-12 | s: 4.272521643635888e-06 | s_nil + z = 4.30290742650663e-06\n",
      "Loss: 4.3370912219173195e-06 | GradNorm^2: 1.3660846897711539e-12 | s: 4.335198233708982e-06 | s_nil + z = 4.3573141797689785e-06\n",
      "Loss: 4.320423007532036e-06 | GradNorm^2: 1.404647204491314e-12 | s: 4.43819175924444e-06 | s_nil + z = 4.429578491282893e-06\n",
      "Loss: 4.303067026930873e-06 | GradNorm^2: 1.360600066631839e-12 | s: 4.496216056294486e-06 | s_nil + z = 4.499575761221508e-06\n",
      "Loss: 4.2860674938701595e-06 | GradNorm^2: 1.3367271735637989e-12 | s: 4.337509573777016e-06 | s_nil + z = 4.367269461007239e-06\n",
      "Loss: 4.2693048237262615e-06 | GradNorm^2: 1.323631475796782e-12 | s: 4.215054947215563e-06 | s_nil + z = 4.247051771016919e-06\n",
      "Loss: 4.252821052577832e-06 | GradNorm^2: 1.3215280557002959e-12 | s: 4.288318258815434e-06 | s_nil + z = 4.311865427632233e-06\n",
      "Loss: 4.236266023542106e-06 | GradNorm^2: 1.3021836797510697e-12 | s: 4.342618392043277e-06 | s_nil + z = 4.356483405715261e-06\n",
      "Loss: 4.220001949110539e-06 | GradNorm^2: 1.3001366679629339e-12 | s: 4.302551217559054e-06 | s_nil + z = 4.323628008888469e-06\n",
      "Loss: 4.20389891743426e-06 | GradNorm^2: 1.2987569448583179e-12 | s: 4.227048157687271e-06 | s_nil + z = 4.248248775394481e-06\n",
      "Loss: 4.187668275940882e-06 | GradNorm^2: 1.2724224777355765e-12 | s: 4.182950929633541e-06 | s_nil + z = 4.169018007309233e-06\n",
      "Loss: 4.171688583187281e-06 | GradNorm^2: 1.2624778691463509e-12 | s: 4.221391893928167e-06 | s_nil + z = 4.209827823946355e-06\n",
      "Loss: 4.1559348492490726e-06 | GradNorm^2: 1.2621857343814569e-12 | s: 4.331922356882238e-06 | s_nil + z = 4.327165300140046e-06\n",
      "Loss: 4.140204328683128e-06 | GradNorm^2: 1.2493573797730441e-12 | s: 4.285228986294222e-06 | s_nil + z = 4.281708548777019e-06\n",
      "Loss: 4.124587074266997e-06 | GradNorm^2: 1.2352319636532197e-12 | s: 4.143603129239907e-06 | s_nil + z = 4.139992870452747e-06\n",
      "Loss: 4.109166377804105e-06 | GradNorm^2: 1.230158724457249e-12 | s: 4.0902415580439725e-06 | s_nil + z = 4.110824429776061e-06\n",
      "Loss: 4.093737615101978e-06 | GradNorm^2: 1.2152272244846904e-12 | s: 4.072626960562922e-06 | s_nil + z = 4.102022714058684e-06\n",
      "Loss: 4.078982773489891e-06 | GradNorm^2: 1.25351067085974e-12 | s: 4.173921952745519e-06 | s_nil + z = 4.192887977337578e-06\n",
      "Loss: 4.063537873363581e-06 | GradNorm^2: 1.2138517457446147e-12 | s: 4.117868719184374e-06 | s_nil + z = 4.106851558438596e-06\n",
      "Loss: 4.048330615777203e-06 | GradNorm^2: 1.1878529299008217e-12 | s: 3.945878064038327e-06 | s_nil + z = 3.961268607730051e-06\n",
      "Loss: 4.033471325452063e-06 | GradNorm^2: 1.183207884180509e-12 | s: 4.087776016020395e-06 | s_nil + z = 4.101193536457104e-06\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "alpha = 1e-4\n",
    "beta=0.999\n",
    "w = torch.zeros(train_data.shape[1], device=device).requires_grad_()\n",
    "s = torch.tensor(0.0)\n",
    "lmd = 0.01\n",
    "mu = 0.1\n",
    "\n",
    "loss_function = loss_class(w)\n",
    "\n",
    "# save loss and grad size to history\n",
    "hist_psps2_l1 = []\n",
    "loss = loss_function(train_data.to(device), train_target.to(device))\n",
    "g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "print(f\"Loss: {loss.item()} | GradNorm^2: {(torch.linalg.norm(g) ** 2 ).item()}\")\n",
    "hist_psps2_l1.append([loss.item(), (torch.linalg.norm(g) ** 2).item(), s])\n",
    "\n",
    "# preconditioninig matrix\n",
    "Dk = diag_estimate_old(w, g, 100)\n",
    "\n",
    "for step in range(STEPS):\n",
    "\n",
    "    for i, (batch_data, batch_target) in enumerate(train_dataloader):\n",
    "\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_target = batch_target.to(device)\n",
    "\n",
    "        loss = loss_function(batch_data, batch_target)\n",
    "\n",
    "        s_nil = s - (lmd/(2*mu))\n",
    "        z = s - s_nil\n",
    "        \n",
    "        if (i != 0) and s_nil >= loss.item():\n",
    "            continue\n",
    "\n",
    "        g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "\n",
    "        vk = diag_estimate_old(w, g, 1)\n",
    "\n",
    "        # Smoothing and Truncation \n",
    "        Dk = beta * Dk + (1 - beta) * vk\n",
    "        Dk_hat = torch.abs(Dk)\n",
    "        Dk_hat[Dk_hat < alpha] = alpha\n",
    "\n",
    "        Dk_hat_inv = 1 / Dk_hat\n",
    "\n",
    "        gnorm = (g * Dk_hat_inv).dot(g)\n",
    "\n",
    "        f_grad = g.clone().detach()\n",
    "\n",
    "        if gnorm < 1e-8:\n",
    "            continue\n",
    "\n",
    "        t = loss.item() - s_nil\n",
    "        a = torch.dot(f_grad, Dk_hat_inv*f_grad).cpu().detach().numpy()\n",
    "        \n",
    "        AA = 1\n",
    "        BB = 2 + mu * a - 2 * mu * t\n",
    "        CC = 1 + 2 * mu * a - 4 * mu * t\n",
    "        DD = -2 * mu * t\n",
    "\n",
    "        roots = solve(AA, BB, CC, DD)\n",
    "        roots = torch.from_numpy(roots)      \n",
    "        root_star = torch.relu(torch.max(roots))\n",
    "\n",
    "        s = torch.relu(s - ((lmd - root_star)/2 * mu)).item()\n",
    "    \n",
    "        precond = root_star/(1 + root_star) * Dk_hat_inv    \n",
    "        with torch.no_grad():\n",
    "            w.sub_(precond  * f_grad)\n",
    "\n",
    "    loss = loss_function(train_data.to(device), train_target.to(device))\n",
    "    g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "    print(f\"Loss: {loss.item()} | GradNorm^2: {(torch.linalg.norm(g) ** 2 ).item()} | s: {s} | s_nil + z = {s_nil + z}\")\n",
    "    hist_psps2_l1.append([loss.item(), (torch.linalg.norm(g) ** 2).item(), s])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSPS2-L2\n",
    "$w_{t+1}, \\; s_{t+1} = {\\arg\\min}_{w\\in\\mathbb{R}^d,s\\geq\n",
    "    0}\\frac{1}{2}\\|w-w_t\\|_{D}^2+\\mu(s-s_t)^2+\\lambda s^2 \\nonumber \\\\\n",
    "    \\textit{s.t.} \\quad f_i(w_t) +  \\langle  \\nabla  f_i(w_t), w-w_t\\rangle +\\frac{1}{2}\\langle D(w-w^t), w - w^t \\rangle \\leq s \\nonumber.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931471805599453 | GradNorm^2: 0.3195669607542947\n",
      "Loss: 0.16013198383282606 | GradNorm^2: 0.012247448263255791 | s: 0.19640301909294672 | s_nil + z = 0.19640301909294672\n",
      "Loss: 0.13203825285636958 | GradNorm^2: 0.00864781459206309 | s: 0.21521931024640883 | s_nil + z = 0.21521931024640883\n",
      "Loss: 0.10469870631126875 | GradNorm^2: 0.005025258623112497 | s: 0.1703300259840491 | s_nil + z = 0.1703300259840491\n",
      "Loss: 0.09629874630104282 | GradNorm^2: 0.004741101845501772 | s: 0.18052491445703026 | s_nil + z = 0.18052491445703026\n",
      "Loss: 0.090383867777086 | GradNorm^2: 0.00417108126730102 | s: 0.1644211096302518 | s_nil + z = 0.1644211096302518\n",
      "Loss: 0.08406125660688848 | GradNorm^2: 0.0036870091112274884 | s: 0.16093971875494437 | s_nil + z = 0.16093971875494437\n",
      "Loss: 0.07985236232572297 | GradNorm^2: 0.0030429786182739334 | s: 0.1512928894898307 | s_nil + z = 0.1512928894898307\n",
      "Loss: 0.0736977444805535 | GradNorm^2: 0.002600353627447459 | s: 0.14084354198482 | s_nil + z = 0.1483006664770045\n",
      "Loss: 0.07264877665884147 | GradNorm^2: 0.0025892165078833175 | s: 0.15216406513404535 | s_nil + z = 0.15216406513404535\n",
      "Loss: 0.07142825151771007 | GradNorm^2: 0.0025378251028295488 | s: 0.16046351511535975 | s_nil + z = 0.16046351511535975\n",
      "Loss: 0.0690186555475377 | GradNorm^2: 0.002272581781400032 | s: 0.12434745153001929 | s_nil + z = 0.12434745153001929\n",
      "Loss: 0.06333043159549359 | GradNorm^2: 0.0017895733754761248 | s: 0.1160143590672043 | s_nil + z = 0.1160143590672043\n",
      "Loss: 0.060046343620027115 | GradNorm^2: 0.0015922414021052518 | s: 0.10830829916899795 | s_nil + z = 0.09991062136010463\n",
      "Loss: 0.057082498616737115 | GradNorm^2: 0.0016267399526701076 | s: 0.12701150238982303 | s_nil + z = 0.12701150238982303\n",
      "Loss: 0.0548894446365255 | GradNorm^2: 0.001382902894179216 | s: 0.12862203094985636 | s_nil + z = 0.12862203094985636\n",
      "Loss: 0.05363457547124207 | GradNorm^2: 0.0014150818617912403 | s: 0.1131377145626385 | s_nil + z = 0.12341725261611604\n",
      "Loss: 0.050504016671222327 | GradNorm^2: 0.0010668642875894417 | s: 0.11594635829699995 | s_nil + z = 0.11594635829699995\n",
      "Loss: 0.04809780416986497 | GradNorm^2: 0.0010047655159263833 | s: 0.10673562529695674 | s_nil + z = 0.10673562529695674\n",
      "Loss: 0.04700160280609996 | GradNorm^2: 0.0010494581965229794 | s: 0.10710244635819881 | s_nil + z = 0.10710244635819881\n",
      "Loss: 0.046519037133905816 | GradNorm^2: 0.0010337267853032552 | s: 0.11389488856678998 | s_nil + z = 0.11389488856678998\n",
      "Loss: 0.04435128732044149 | GradNorm^2: 0.0008544127517113638 | s: 0.11228526935834242 | s_nil + z = 0.11228526935834242\n",
      "Loss: 0.042760771356616045 | GradNorm^2: 0.0007686143055530829 | s: 0.09347398210506082 | s_nil + z = 0.09723713424547133\n",
      "Loss: 0.04218347839328816 | GradNorm^2: 0.0007356712240433838 | s: 0.08747779983084972 | s_nil + z = 0.09128894694339777\n",
      "Loss: 0.041132380006553435 | GradNorm^2: 0.0007533976990225238 | s: 0.11439491306315633 | s_nil + z = 0.11439491306315633\n",
      "Loss: 0.04047365693420534 | GradNorm^2: 0.0007234589947692878 | s: 0.10647783066479521 | s_nil + z = 0.10647783066479521\n",
      "Loss: 0.03830568870622992 | GradNorm^2: 0.0007033371101321841 | s: 0.1274896213989474 | s_nil + z = 0.1274896213989474\n",
      "Loss: 0.03829177054024853 | GradNorm^2: 0.0007049778280338885 | s: 0.11056451422788145 | s_nil + z = 0.11056451422788145\n",
      "Loss: 0.03684100404747641 | GradNorm^2: 0.0006481644946907452 | s: 0.10279364726794506 | s_nil + z = 0.10279364726794506\n",
      "Loss: 0.036466585934672655 | GradNorm^2: 0.0006343199133193676 | s: 0.10992031216534481 | s_nil + z = 0.10992031216534481\n",
      "Loss: 0.035727246646164994 | GradNorm^2: 0.0006038020161068382 | s: 0.10331758348438477 | s_nil + z = 0.10331758348438477\n",
      "Loss: 0.03422939359367553 | GradNorm^2: 0.00048136294255821766 | s: 0.091613901418973 | s_nil + z = 0.091613901418973\n",
      "Loss: 0.034039857363921296 | GradNorm^2: 0.0005288938099449467 | s: 0.09801021819461625 | s_nil + z = 0.09801021819461625\n",
      "Loss: 0.032837744130753244 | GradNorm^2: 0.00047194362516423554 | s: 0.0978637543144111 | s_nil + z = 0.0978637543144111\n",
      "Loss: 0.032037991379554556 | GradNorm^2: 0.00045784587933286116 | s: 0.0856420990454486 | s_nil + z = 0.0856420990454486\n",
      "Loss: 0.03146011648439299 | GradNorm^2: 0.00046086543752447983 | s: 0.09982706851911526 | s_nil + z = 0.09982706851911526\n",
      "Loss: 0.031058884126901703 | GradNorm^2: 0.0004487590625423117 | s: 0.10527364085600942 | s_nil + z = 0.10527364085600942\n",
      "Loss: 0.031022047947637226 | GradNorm^2: 0.0004739453894511589 | s: 0.0948050729490506 | s_nil + z = 0.0948050729490506\n",
      "Loss: 0.030069323783187902 | GradNorm^2: 0.00046182494760458915 | s: 0.09899340470210415 | s_nil + z = 0.09899340470210415\n",
      "Loss: 0.02953048024305189 | GradNorm^2: 0.0004377217656608699 | s: 0.09335224462977226 | s_nil + z = 0.09335224462977226\n",
      "Loss: 0.02906523421634737 | GradNorm^2: 0.0004323580784087956 | s: 0.09187507392776671 | s_nil + z = 0.09187507392776671\n",
      "Loss: 0.027862376536740396 | GradNorm^2: 0.00038034118361273447 | s: 0.09463703977384544 | s_nil + z = 0.09463703977384544\n",
      "Loss: 0.02761907491851037 | GradNorm^2: 0.0004068227161391489 | s: 0.09924558508332271 | s_nil + z = 0.09924558508332271\n",
      "Loss: 0.026697310788661825 | GradNorm^2: 0.0003175111445139016 | s: 0.07224084037151121 | s_nil + z = 0.06886213550758552\n",
      "Loss: 0.02575129744282115 | GradNorm^2: 0.00030673623633008414 | s: 0.09009596712165008 | s_nil + z = 0.09009596712165008\n",
      "Loss: 0.025424721519985115 | GradNorm^2: 0.00027724193799225164 | s: 0.08997636982966124 | s_nil + z = 0.08997636982966124\n",
      "Loss: 0.024647886293472157 | GradNorm^2: 0.00025128041596919235 | s: 0.09079442836320097 | s_nil + z = 0.09079442836320097\n",
      "Loss: 0.02406856088716183 | GradNorm^2: 0.0002507243026562783 | s: 0.08195923730526994 | s_nil + z = 0.08195923730526994\n",
      "Loss: 0.023683027569505 | GradNorm^2: 0.00022552776235350476 | s: 0.08826325639230749 | s_nil + z = 0.08826325639230749\n",
      "Loss: 0.02297788267475035 | GradNorm^2: 0.00020056388602612596 | s: 0.08441815564112566 | s_nil + z = 0.08441815564112566\n",
      "Loss: 0.022444882658945665 | GradNorm^2: 0.00020906991829364136 | s: 0.06380896966177252 | s_nil + z = 0.06560559886854819\n",
      "Loss: 0.021382599549281095 | GradNorm^2: 0.00017326740429649172 | s: 0.07413332481285767 | s_nil + z = 0.06414109744752958\n",
      "Loss: 0.020993096807772657 | GradNorm^2: 0.00016031472744413626 | s: 0.0665747986492825 | s_nil + z = 0.07233114806533311\n",
      "Loss: 0.020641588012149336 | GradNorm^2: 0.00016379123699257467 | s: 0.06754576682909876 | s_nil + z = 0.06754576682909876\n",
      "Loss: 0.020088552368736766 | GradNorm^2: 0.0001516761037806571 | s: 0.0848846372280498 | s_nil + z = 0.0848846372280498\n",
      "Loss: 0.020077441847288168 | GradNorm^2: 0.00016595728856353266 | s: 0.08019792189697983 | s_nil + z = 0.08019792189697983\n",
      "Loss: 0.019685511452274977 | GradNorm^2: 0.0001741972621145032 | s: 0.08532115867681835 | s_nil + z = 0.08532115867681835\n",
      "Loss: 0.018879871380750226 | GradNorm^2: 0.00014692241712023537 | s: 0.06554969951183322 | s_nil + z = 0.0687702725244941\n",
      "Loss: 0.018732232094808406 | GradNorm^2: 0.00013675415697673513 | s: 0.08461579383517584 | s_nil + z = 0.08461579383517584\n",
      "Loss: 0.01814005259321231 | GradNorm^2: 0.0001195826667536858 | s: 0.06676342290362884 | s_nil + z = 0.06676342290362884\n",
      "Loss: 0.017768854221193408 | GradNorm^2: 0.00011874556097042777 | s: 0.07722346685866427 | s_nil + z = 0.07722346685866427\n",
      "Loss: 0.01733176338014308 | GradNorm^2: 0.00011495485383618425 | s: 0.0660576667393536 | s_nil + z = 0.0660576667393536\n",
      "Loss: 0.01700843495964168 | GradNorm^2: 0.00011201128200850003 | s: 0.07616225856286897 | s_nil + z = 0.07616225856286897\n",
      "Loss: 0.016457056972271556 | GradNorm^2: 9.70093741039901e-05 | s: 0.058979912159316306 | s_nil + z = 0.06321302999705916\n",
      "Loss: 0.01591010001624907 | GradNorm^2: 8.893672002485962e-05 | s: 0.07443725590881399 | s_nil + z = 0.07443725590881399\n",
      "Loss: 0.015646303753249446 | GradNorm^2: 8.734268268175347e-05 | s: 0.06378020568393748 | s_nil + z = 0.05678119951509549\n",
      "Loss: 0.015630662830567528 | GradNorm^2: 9.490566825539934e-05 | s: 0.08171642095970094 | s_nil + z = 0.08171642095970094\n",
      "Loss: 0.014976594414701836 | GradNorm^2: 8.373839050679853e-05 | s: 0.06439840635526924 | s_nil + z = 0.06439840635526924\n",
      "Loss: 0.014191776944319093 | GradNorm^2: 6.3922962952261e-05 | s: 0.05830941719599268 | s_nil + z = 0.05830941719599268\n",
      "Loss: 0.01377251994533667 | GradNorm^2: 5.917306032587191e-05 | s: 0.05470219015482996 | s_nil + z = 0.05676365482650134\n",
      "Loss: 0.013383347534529612 | GradNorm^2: 6.177784059283637e-05 | s: 0.07318423399488447 | s_nil + z = 0.07318423399488447\n",
      "Loss: 0.012970806771237478 | GradNorm^2: 5.745913820353596e-05 | s: 0.06925575629469595 | s_nil + z = 0.06925575629469595\n",
      "Loss: 0.012640700213432157 | GradNorm^2: 5.73269373413612e-05 | s: 0.06389228761337518 | s_nil + z = 0.06389228761337518\n",
      "Loss: 0.012279127431169438 | GradNorm^2: 5.1662805772888836e-05 | s: 0.06929449105925733 | s_nil + z = 0.06926864551018196\n",
      "Loss: 0.012154012584379493 | GradNorm^2: 5.451628254512619e-05 | s: 0.06435796301021247 | s_nil + z = 0.06435796301021247\n",
      "Loss: 0.011826049198352624 | GradNorm^2: 6.12632468124723e-05 | s: 0.07085134763310458 | s_nil + z = 0.07085134763310458\n",
      "Loss: 0.01135174466784933 | GradNorm^2: 5.026846168402441e-05 | s: 0.06471698999780191 | s_nil + z = 0.06471698999780191\n",
      "Loss: 0.010737425619328198 | GradNorm^2: 3.7357215046885947e-05 | s: 0.062449069358249766 | s_nil + z = 0.062449069358249766\n",
      "Loss: 0.010497661887462297 | GradNorm^2: 3.912165531450107e-05 | s: 0.06952992885531648 | s_nil + z = 0.06952992885531648\n",
      "Loss: 0.010390876274218589 | GradNorm^2: 4.046920147650692e-05 | s: 0.07049616639930992 | s_nil + z = 0.07049616639930992\n",
      "Loss: 0.010088510080667346 | GradNorm^2: 3.8091038860586446e-05 | s: 0.05752215628147002 | s_nil + z = 0.05475610072671629\n",
      "Loss: 0.009894086226950257 | GradNorm^2: 3.920911976160584e-05 | s: 0.06714625813550083 | s_nil + z = 0.06714625813550083\n",
      "Loss: 0.009337660657683258 | GradNorm^2: 2.815661593642466e-05 | s: 0.05985431038742426 | s_nil + z = 0.05985431038742426\n",
      "Loss: 0.008990544068746135 | GradNorm^2: 2.5833725789098396e-05 | s: 0.05350422450686479 | s_nil + z = 0.058759592341720554\n",
      "Loss: 0.008497594855276468 | GradNorm^2: 2.258143063961441e-05 | s: 0.0634787105662424 | s_nil + z = 0.0634787105662424\n",
      "Loss: 0.008148257548391435 | GradNorm^2: 2.034984711030895e-05 | s: 0.05035811479628911 | s_nil + z = 0.05448218439763299\n",
      "Loss: 0.007837912354926566 | GradNorm^2: 1.8621379942495136e-05 | s: 0.06042787171558257 | s_nil + z = 0.06042787171558257\n",
      "Loss: 0.007752677704482351 | GradNorm^2: 2.0340471995814932e-05 | s: 0.06905199848235555 | s_nil + z = 0.06905199848235555\n",
      "Loss: 0.0074945773700426 | GradNorm^2: 1.8034455159899548e-05 | s: 0.061008424691546076 | s_nil + z = 0.06058321088038468\n",
      "Loss: 0.007153962548676433 | GradNorm^2: 1.554810250547615e-05 | s: 0.06540530172288603 | s_nil + z = 0.06540530172288603\n",
      "Loss: 0.006764399720742196 | GradNorm^2: 1.2368202068496044e-05 | s: 0.053918511264091425 | s_nil + z = 0.05201000038363044\n",
      "Loss: 0.006472159772800256 | GradNorm^2: 1.161027790534457e-05 | s: 0.0528480359458975 | s_nil + z = 0.0529852477849279\n",
      "Loss: 0.006254732741969766 | GradNorm^2: 1.0566363936094953e-05 | s: 0.04970814784485531 | s_nil + z = 0.053088278290198056\n",
      "Loss: 0.0059661988863511685 | GradNorm^2: 9.855017257855883e-06 | s: 0.05176931538806674 | s_nil + z = 0.04991073733973236\n",
      "Loss: 0.005707415530641904 | GradNorm^2: 8.565894581607268e-06 | s: 0.052826992195946464 | s_nil + z = 0.04988065229411263\n",
      "Loss: 0.005386759765897555 | GradNorm^2: 6.986925952374357e-06 | s: 0.05181504803291552 | s_nil + z = 0.05306202280950909\n",
      "Loss: 0.005115640626684955 | GradNorm^2: 8.294387385194303e-06 | s: 0.05737014927462995 | s_nil + z = 0.05737014927462995\n",
      "Loss: 0.004961766063778207 | GradNorm^2: 7.61602433811194e-06 | s: 0.05311493054017398 | s_nil + z = 0.05566267224178156\n",
      "Loss: 0.004710790490257553 | GradNorm^2: 6.832782122790772e-06 | s: 0.057150191476411344 | s_nil + z = 0.057150191476411344\n",
      "Loss: 0.0044319710272670105 | GradNorm^2: 5.22626168492529e-06 | s: 0.048649033454543814 | s_nil + z = 0.051287980538800036\n",
      "Loss: 0.004228265905501806 | GradNorm^2: 4.681009847352721e-06 | s: 0.0493424840706866 | s_nil + z = 0.04747145326617889\n",
      "Loss: 0.004098217341520192 | GradNorm^2: 5.5922636078718955e-06 | s: 0.06226653850653462 | s_nil + z = 0.06226653850653462\n",
      "Loss: 0.003874911053646997 | GradNorm^2: 3.697321001363439e-06 | s: 0.05221361662406946 | s_nil + z = 0.049121186743011386\n",
      "Loss: 0.003658270590270114 | GradNorm^2: 3.2717062398263685e-06 | s: 0.0529925514176383 | s_nil + z = 0.0529925514176383\n",
      "Loss: 0.0034543483382078193 | GradNorm^2: 2.631903224031926e-06 | s: 0.04950645341924305 | s_nil + z = 0.04930483311338253\n",
      "Loss: 0.003279797893091627 | GradNorm^2: 2.4608743545321204e-06 | s: 0.04968305788811572 | s_nil + z = 0.049188565166096536\n",
      "Loss: 0.003091713163133809 | GradNorm^2: 2.095870394419101e-06 | s: 0.04791546194223496 | s_nil + z = 0.04741035437887353\n",
      "Loss: 0.0029089163920064847 | GradNorm^2: 1.8133348189759219e-06 | s: 0.04785218808159142 | s_nil + z = 0.047954563028821895\n",
      "Loss: 0.0027394493869569853 | GradNorm^2: 1.620152682896822e-06 | s: 0.048090019953477794 | s_nil + z = 0.047923609877664086\n",
      "Loss: 0.002583516065938643 | GradNorm^2: 1.510217840147424e-06 | s: 0.048425130718695586 | s_nil + z = 0.04838983014557285\n",
      "Loss: 0.0024613516487572928 | GradNorm^2: 1.5452138372692037e-06 | s: 0.047934158578415285 | s_nil + z = 0.051755151996814845\n",
      "Loss: 0.0023156603378558277 | GradNorm^2: 1.1810507779503388e-06 | s: 0.04806458096581194 | s_nil + z = 0.04738617341514864\n",
      "Loss: 0.0021863500244487153 | GradNorm^2: 1.0217341813388971e-06 | s: 0.04689425680106954 | s_nil + z = 0.04699981599040546\n",
      "Loss: 0.00206101546768323 | GradNorm^2: 9.286934859031864e-07 | s: 0.04741520791398827 | s_nil + z = 0.04806209423756099\n",
      "Loss: 0.0019465346260810847 | GradNorm^2: 8.831752641453165e-07 | s: 0.04908418962744786 | s_nil + z = 0.04704858708890427\n",
      "Loss: 0.0018352269552479516 | GradNorm^2: 7.249970961515715e-07 | s: 0.047588528364139605 | s_nil + z = 0.04682384402762625\n",
      "Loss: 0.0017378052347545411 | GradNorm^2: 7.338627655996452e-07 | s: 0.047592454885907874 | s_nil + z = 0.04815731922128508\n",
      "Loss: 0.0016405513010166585 | GradNorm^2: 7.179667474223546e-07 | s: 0.05109571073528249 | s_nil + z = 0.04720993387885758\n",
      "Loss: 0.001547823812137122 | GradNorm^2: 5.625742460052865e-07 | s: 0.04647023277431927 | s_nil + z = 0.04736618002437107\n",
      "Loss: 0.001458159322306843 | GradNorm^2: 3.9674111355054617e-07 | s: 0.04729373102601875 | s_nil + z = 0.04743428083135378\n",
      "Loss: 0.0013759953795487374 | GradNorm^2: 3.6062316307633196e-07 | s: 0.04722944444195923 | s_nil + z = 0.04671153967561079\n",
      "Loss: 0.0013016573534366282 | GradNorm^2: 3.491997942598833e-07 | s: 0.04684041087493568 | s_nil + z = 0.04663059030956825\n",
      "Loss: 0.0012322870187983757 | GradNorm^2: 3.1157134471540253e-07 | s: 0.046756149396795364 | s_nil + z = 0.046252770610609284\n",
      "Loss: 0.0011649434800728089 | GradNorm^2: 2.5122624868866765e-07 | s: 0.04698105498215073 | s_nil + z = 0.04740230565091384\n",
      "Loss: 0.0011032458166628121 | GradNorm^2: 2.2702153958241072e-07 | s: 0.04611808044523204 | s_nil + z = 0.04653559366949023\n",
      "Loss: 0.0010459736678973116 | GradNorm^2: 2.0951179247870463e-07 | s: 0.047635196463690015 | s_nil + z = 0.046631441034432994\n",
      "Loss: 0.000991387757710019 | GradNorm^2: 1.5962871988341565e-07 | s: 0.04680882223092868 | s_nil + z = 0.04772600904049315\n",
      "Loss: 0.0009416673987184324 | GradNorm^2: 1.4896409364088543e-07 | s: 0.04709736149031266 | s_nil + z = 0.04700569989147056\n",
      "Loss: 0.0008949949493562007 | GradNorm^2: 1.3206627513219885e-07 | s: 0.04618417086387827 | s_nil + z = 0.04663674089030461\n",
      "Loss: 0.000851266910083998 | GradNorm^2: 1.0977707817718555e-07 | s: 0.04648665120010899 | s_nil + z = 0.046683931175619385\n",
      "Loss: 0.0008121665573415171 | GradNorm^2: 1.2352002238264857e-07 | s: 0.04618524071082387 | s_nil + z = 0.046762152934467796\n",
      "Loss: 0.0007740829060626113 | GradNorm^2: 1.0233075934976876e-07 | s: 0.04668526687741132 | s_nil + z = 0.04647939838112057\n",
      "Loss: 0.0007385320218919115 | GradNorm^2: 9.588050250239919e-08 | s: 0.04654266432176576 | s_nil + z = 0.04625077281788021\n",
      "Loss: 0.0007041598440469308 | GradNorm^2: 7.577448347929337e-08 | s: 0.046639821524278736 | s_nil + z = 0.046234100787573826\n",
      "Loss: 0.0006730029017795695 | GradNorm^2: 7.02036576570505e-08 | s: 0.04623020941291515 | s_nil + z = 0.046209925615026924\n",
      "Loss: 0.00064342987466091 | GradNorm^2: 5.990333483480252e-08 | s: 0.04658414062183237 | s_nil + z = 0.046322346777533656\n",
      "Loss: 0.0006159224537003463 | GradNorm^2: 5.228355333698867e-08 | s: 0.0462327966672351 | s_nil + z = 0.04639260114077869\n",
      "Loss: 0.0005902178105900788 | GradNorm^2: 5.065197446270181e-08 | s: 0.04602514414748936 | s_nil + z = 0.04686979241456214\n",
      "Loss: 0.000565542273254646 | GradNorm^2: 4.151416992512487e-08 | s: 0.04603643556581314 | s_nil + z = 0.04634648787041075\n",
      "Loss: 0.0005433318238889901 | GradNorm^2: 4.469717717978706e-08 | s: 0.04633824492039855 | s_nil + z = 0.04609095435727642\n",
      "Loss: 0.0005213898920393686 | GradNorm^2: 3.5674634246563975e-08 | s: 0.04604990449508278 | s_nil + z = 0.046210949884500786\n",
      "Loss: 0.0005011512185412722 | GradNorm^2: 3.4898745847501616e-08 | s: 0.04627642601136839 | s_nil + z = 0.046013878721717506\n",
      "Loss: 0.0004817015883383506 | GradNorm^2: 2.8757683770586745e-08 | s: 0.04618991104885288 | s_nil + z = 0.046111147632333746\n",
      "Loss: 0.00046416368436508296 | GradNorm^2: 2.7253427387292606e-08 | s: 0.04597487060682086 | s_nil + z = 0.04609020450514393\n",
      "Loss: 0.00044739550954623394 | GradNorm^2: 2.5090963035912474e-08 | s: 0.04613571207137084 | s_nil + z = 0.045968157340685156\n",
      "Loss: 0.0004317158724631801 | GradNorm^2: 2.2086671968704157e-08 | s: 0.046075171537120284 | s_nil + z = 0.04604711271461053\n",
      "Loss: 0.00041732033733915796 | GradNorm^2: 2.0961845920410296e-08 | s: 0.04583064516206366 | s_nil + z = 0.047338417748366265\n",
      "Loss: 0.000403527380866883 | GradNorm^2: 1.9265839801157934e-08 | s: 0.046170900937444546 | s_nil + z = 0.0460482181988184\n",
      "Loss: 0.00039053266727058646 | GradNorm^2: 1.7360606585885555e-08 | s: 0.04592707939469177 | s_nil + z = 0.04590192574808614\n",
      "Loss: 0.0003782554936260034 | GradNorm^2: 1.6014189303136255e-08 | s: 0.046053882445506206 | s_nil + z = 0.04600525637796803\n",
      "Loss: 0.0003666115547288124 | GradNorm^2: 1.4978798798926194e-08 | s: 0.04594113692123067 | s_nil + z = 0.046411550691870966\n",
      "Loss: 0.0003557947317509393 | GradNorm^2: 1.502939468972845e-08 | s: 0.04593193991205394 | s_nil + z = 0.046316202293183034\n",
      "Loss: 0.00034517523865776643 | GradNorm^2: 1.2939094895442969e-08 | s: 0.04629688685048896 | s_nil + z = 0.046119543907830164\n",
      "Loss: 0.00033560030225143474 | GradNorm^2: 1.473226786280231e-08 | s: 0.04633073476667962 | s_nil + z = 0.04595999617757619\n",
      "Loss: 0.0003262554023874595 | GradNorm^2: 1.4042241722625106e-08 | s: 0.046130708684739466 | s_nil + z = 0.04599029718869274\n",
      "Loss: 0.00031717291912811336 | GradNorm^2: 1.1562469818790399e-08 | s: 0.04607939192001981 | s_nil + z = 0.04628440900403902\n",
      "Loss: 0.00030874781768727613 | GradNorm^2: 1.0316196940131999e-08 | s: 0.04594292624997398 | s_nil + z = 0.0459779184552111\n",
      "Loss: 0.00030081088065062836 | GradNorm^2: 1.008087554665994e-08 | s: 0.0459962287007076 | s_nil + z = 0.04605035799803155\n",
      "Loss: 0.00029334029882808343 | GradNorm^2: 1.1049481874821979e-08 | s: 0.046420581921716476 | s_nil + z = 0.04615424967717558\n",
      "Loss: 0.00028626886561145394 | GradNorm^2: 1.1740538444127306e-08 | s: 0.04593529728886342 | s_nil + z = 0.04598676223270171\n",
      "Loss: 0.00027926185231399113 | GradNorm^2: 1.1188587150608999e-08 | s: 0.04607190493879149 | s_nil + z = 0.04614326639996708\n",
      "Loss: 0.0002721832811170421 | GradNorm^2: 9.151009545775659e-09 | s: 0.04611532430370549 | s_nil + z = 0.04625892129005635\n",
      "Loss: 0.0002654595197156702 | GradNorm^2: 7.050164755581674e-09 | s: 0.04613169154255007 | s_nil + z = 0.04590845457050054\n",
      "Loss: 0.0002592419103020558 | GradNorm^2: 6.7376827803398304e-09 | s: 0.046065594170747716 | s_nil + z = 0.046146897088966216\n",
      "Loss: 0.00025390590832826386 | GradNorm^2: 9.960358864324464e-09 | s: 0.0471170730588074 | s_nil + z = 0.04623388728167516\n",
      "Loss: 0.00024829261362671037 | GradNorm^2: 9.519610631430832e-09 | s: 0.04656268791101873 | s_nil + z = 0.04596160549145765\n",
      "Loss: 0.000242263479989218 | GradNorm^2: 5.8260459436312256e-09 | s: 0.046123565424715476 | s_nil + z = 0.04626646800697066\n",
      "Loss: 0.00023704309323038615 | GradNorm^2: 5.460288151193963e-09 | s: 0.04602591467433058 | s_nil + z = 0.0461098843692951\n",
      "Loss: 0.00023232181272713203 | GradNorm^2: 6.5545694908799035e-09 | s: 0.04602896917411641 | s_nil + z = 0.04593036183097388\n",
      "Loss: 0.00022745386326362868 | GradNorm^2: 6.532726216429233e-09 | s: 0.04599403250747784 | s_nil + z = 0.04613146525645829\n",
      "Loss: 0.00022244847962220695 | GradNorm^2: 5.0935431157072755e-09 | s: 0.04619834652619509 | s_nil + z = 0.04607476067960764\n",
      "Loss: 0.00021789793812089143 | GradNorm^2: 4.661645753546732e-09 | s: 0.04588835913368054 | s_nil + z = 0.046082212489676116\n",
      "Loss: 0.00021342839275645346 | GradNorm^2: 4.220737087320144e-09 | s: 0.04596041210099825 | s_nil + z = 0.045907128302365464\n",
      "Loss: 0.00020925770560060468 | GradNorm^2: 4.096873589659801e-09 | s: 0.04600984267751177 | s_nil + z = 0.04622844167152559\n",
      "Loss: 0.0002051891742546418 | GradNorm^2: 4.045461101146359e-09 | s: 0.045978138800235796 | s_nil + z = 0.046135819904781536\n",
      "Loss: 0.00020128899761665865 | GradNorm^2: 4.059391163702086e-09 | s: 0.0459252341479089 | s_nil + z = 0.04594279158807231\n",
      "Loss: 0.00019740474661892013 | GradNorm^2: 3.5914841366492803e-09 | s: 0.046084894628898844 | s_nil + z = 0.04608598532572486\n",
      "Loss: 0.00019371355919963094 | GradNorm^2: 3.3524127222833354e-09 | s: 0.04606181902233667 | s_nil + z = 0.04602008765131513\n",
      "Loss: 0.00019025556296208874 | GradNorm^2: 3.5718798013953303e-09 | s: 0.04597379789434573 | s_nil + z = 0.046034416251647975\n",
      "Loss: 0.0001867405137823625 | GradNorm^2: 3.170156458796618e-09 | s: 0.04607675190355657 | s_nil + z = 0.04600900620441887\n",
      "Loss: 0.0001833988786938038 | GradNorm^2: 2.9878549333062997e-09 | s: 0.045958309509493395 | s_nil + z = 0.04595479392763292\n",
      "Loss: 0.00018028578820291456 | GradNorm^2: 3.0886907908584174e-09 | s: 0.04590478835063284 | s_nil + z = 0.04594330324326038\n",
      "Loss: 0.00017717747288533765 | GradNorm^2: 2.7359990043433014e-09 | s: 0.046014602929938704 | s_nil + z = 0.04596959570782946\n",
      "Loss: 0.000174268955488336 | GradNorm^2: 2.862152876755636e-09 | s: 0.04592919767155199 | s_nil + z = 0.046192823198795724\n",
      "Loss: 0.0001714110104350629 | GradNorm^2: 2.6463728592479528e-09 | s: 0.045996764476426096 | s_nil + z = 0.045992351332529234\n",
      "Loss: 0.00016870390191436296 | GradNorm^2: 2.859914159483024e-09 | s: 0.045996733125563134 | s_nil + z = 0.04587697636072994\n",
      "Loss: 0.00016664364686633465 | GradNorm^2: 5.169610149084455e-09 | s: 0.045904853726536535 | s_nil + z = 0.04597036662718318\n",
      "Loss: 0.00016324683979407524 | GradNorm^2: 2.2965547085519116e-09 | s: 0.045909616296854194 | s_nil + z = 0.04589123167700348\n",
      "Loss: 0.00016072156106344292 | GradNorm^2: 2.4051010998937534e-09 | s: 0.04591828821794079 | s_nil + z = 0.046151297152011175\n",
      "Loss: 0.00015827238312381888 | GradNorm^2: 2.4456575728953594e-09 | s: 0.04615189986347668 | s_nil + z = 0.04593054447065446\n",
      "Loss: 0.00015584395357307035 | GradNorm^2: 2.311760578737662e-09 | s: 0.046026185903067084 | s_nil + z = 0.04588566744056263\n",
      "Loss: 0.00015345381778653617 | GradNorm^2: 2.0750315052182086e-09 | s: 0.04597436031312546 | s_nil + z = 0.045987645764616364\n",
      "Loss: 0.0001511572491600068 | GradNorm^2: 1.9420197265395392e-09 | s: 0.045924490720727934 | s_nil + z = 0.04613492334908489\n",
      "Loss: 0.00014896467882906286 | GradNorm^2: 1.9411010729038935e-09 | s: 0.04593262480681388 | s_nil + z = 0.045879971773905535\n",
      "Loss: 0.000146824430410543 | GradNorm^2: 1.916065186292972e-09 | s: 0.046020958094054674 | s_nil + z = 0.046063172231836\n",
      "Loss: 0.0001447185972733725 | GradNorm^2: 1.7702600849387267e-09 | s: 0.04590896217508563 | s_nil + z = 0.045884035865180195\n",
      "Loss: 0.00014269868422943295 | GradNorm^2: 1.693505046502736e-09 | s: 0.045900760110060695 | s_nil + z = 0.045885165296877486\n",
      "Loss: 0.0001407403250152192 | GradNorm^2: 1.641300398954061e-09 | s: 0.04587269586401071 | s_nil + z = 0.0458736317145836\n",
      "Loss: 0.0001388611773198796 | GradNorm^2: 1.6677727581192208e-09 | s: 0.04588167379824881 | s_nil + z = 0.045964972933722655\n",
      "Loss: 0.00013705454869490435 | GradNorm^2: 1.747971747832044e-09 | s: 0.045874290843672665 | s_nil + z = 0.04609592957070861\n",
      "Loss: 0.00013537211763764266 | GradNorm^2: 2.045340743758265e-09 | s: 0.04647344074437142 | s_nil + z = 0.04589613519616809\n",
      "Loss: 0.00013362976570085923 | GradNorm^2: 2.016553335646034e-09 | s: 0.045952292379407414 | s_nil + z = 0.04587008738392988\n",
      "Loss: 0.0001317755648707789 | GradNorm^2: 1.5053557053387057e-09 | s: 0.04586847578380519 | s_nil + z = 0.046282186765947954\n",
      "Loss: 0.00013009359106283934 | GradNorm^2: 1.3863612443767035e-09 | s: 0.045910200432276504 | s_nil + z = 0.04604585919897835\n",
      "Loss: 0.000128479950145029 | GradNorm^2: 1.3490061312602192e-09 | s: 0.04590050614732684 | s_nil + z = 0.0460992448051917\n",
      "Loss: 0.00012691120673867222 | GradNorm^2: 1.313645235950205e-09 | s: 0.04592699401188344 | s_nil + z = 0.045925690340421876\n",
      "Loss: 0.00012539853493426608 | GradNorm^2: 1.3114197803667198e-09 | s: 0.045951592734098566 | s_nil + z = 0.04587636656990996\n",
      "Loss: 0.00012394267868539105 | GradNorm^2: 1.364585824619764e-09 | s: 0.04597747866096823 | s_nil + z = 0.045885002437089054\n",
      "Loss: 0.00012245023591740632 | GradNorm^2: 1.2305185830254237e-09 | s: 0.04601804836296561 | s_nil + z = 0.04594437179872208\n",
      "Loss: 0.00012103009335881613 | GradNorm^2: 1.1874379059680264e-09 | s: 0.045940969466995545 | s_nil + z = 0.045897475681078825\n",
      "Loss: 0.00011964721968112273 | GradNorm^2: 1.158994763822399e-09 | s: 0.04594088646490735 | s_nil + z = 0.04589577237782631\n",
      "Loss: 0.00011829679844638904 | GradNorm^2: 1.1315437808865415e-09 | s: 0.045899574551118864 | s_nil + z = 0.045958531197936914\n",
      "Loss: 0.00011698767766389342 | GradNorm^2: 1.137739096922751e-09 | s: 0.045926599150434746 | s_nil + z = 0.04594775017541436\n",
      "Loss: 0.0001157137544167783 | GradNorm^2: 1.1434249201421493e-09 | s: 0.04592227285316415 | s_nil + z = 0.045921827349538855\n",
      "Loss: 0.00011442564179684343 | GradNorm^2: 1.053239822204928e-09 | s: 0.04611245370610408 | s_nil + z = 0.045933143075890144\n",
      "Loss: 0.00011320161032863425 | GradNorm^2: 1.0410150347600238e-09 | s: 0.04596091154820893 | s_nil + z = 0.04594176602936523\n",
      "Loss: 0.00011199212882021537 | GradNorm^2: 1.0070294206345208e-09 | s: 0.04596328790642759 | s_nil + z = 0.045911096811011826\n",
      "Loss: 0.00011084302840367252 | GradNorm^2: 1.055235378726915e-09 | s: 0.04598850151387005 | s_nil + z = 0.04589269526720106\n",
      "Loss: 0.00010967169152286646 | GradNorm^2: 9.767534126849741e-10 | s: 0.04591757716421659 | s_nil + z = 0.046266624120282825\n",
      "Loss: 0.00010858042293052908 | GradNorm^2: 1.0314394829958268e-09 | s: 0.045890313353809054 | s_nil + z = 0.04585580387855896\n",
      "Loss: 0.0001074649502149415 | GradNorm^2: 9.72354601322374e-10 | s: 0.04589107454938848 | s_nil + z = 0.046004896057165366\n",
      "Loss: 0.00010639141964595968 | GradNorm^2: 9.541031209197798e-10 | s: 0.04592557876874581 | s_nil + z = 0.04591265151227881\n",
      "Loss: 0.00010532222458030888 | GradNorm^2: 9.031070400194962e-10 | s: 0.04600448894101214 | s_nil + z = 0.04590319960002833\n",
      "Loss: 0.00010431515716640002 | GradNorm^2: 9.464332203572962e-10 | s: 0.04590969466477523 | s_nil + z = 0.04591866892877049\n",
      "Loss: 0.0001033617901523116 | GradNorm^2: 1.068181603186513e-09 | s: 0.04590090640390923 | s_nil + z = 0.04594570059416269\n",
      "Loss: 0.00010228636229556085 | GradNorm^2: 8.514528473913294e-10 | s: 0.045946358809566316 | s_nil + z = 0.045952961053408924\n",
      "Loss: 0.0001013088849078688 | GradNorm^2: 8.155868517745757e-10 | s: 0.0459231382593768 | s_nil + z = 0.04590340367741572\n",
      "Loss: 0.00010035541296312065 | GradNorm^2: 8.004546610526838e-10 | s: 0.045948272559009934 | s_nil + z = 0.04586482819707751\n",
      "Loss: 9.94304228273043e-05 | GradNorm^2: 8.080121777742091e-10 | s: 0.04594418278758826 | s_nil + z = 0.045891791449479274\n",
      "Loss: 9.851142479109761e-05 | GradNorm^2: 7.821908286484095e-10 | s: 0.04596068316585278 | s_nil + z = 0.04592716850984297\n",
      "Loss: 9.76288964381601e-05 | GradNorm^2: 8.0123605946857e-10 | s: 0.0459914348073712 | s_nil + z = 0.04587132861281929\n",
      "Loss: 9.67348702311883e-05 | GradNorm^2: 7.572088388065728e-10 | s: 0.0459237171264992 | s_nil + z = 0.04585135056155305\n",
      "Loss: 9.586743763155168e-05 | GradNorm^2: 7.355717437811108e-10 | s: 0.04592388370188618 | s_nil + z = 0.04587871364893528\n",
      "Loss: 9.503991641166515e-05 | GradNorm^2: 7.685470522535066e-10 | s: 0.04587699274896475 | s_nil + z = 0.04586877830796048\n",
      "Loss: 9.422127219707364e-05 | GradNorm^2: 7.837996328912075e-10 | s: 0.04585936347836759 | s_nil + z = 0.04591419270557658\n",
      "Loss: 9.337432623533897e-05 | GradNorm^2: 7.083737056213303e-10 | s: 0.04587150843500961 | s_nil + z = 0.045850843273694264\n",
      "Loss: 9.25690847023915e-05 | GradNorm^2: 6.921066172893252e-10 | s: 0.04588406955458235 | s_nil + z = 0.045893895983784544\n",
      "Loss: 9.179982067912713e-05 | GradNorm^2: 7.187296867982698e-10 | s: 0.045863942903253636 | s_nil + z = 0.04587964746468514\n",
      "Loss: 9.100801061330298e-05 | GradNorm^2: 6.677693464467391e-10 | s: 0.04601095010504469 | s_nil + z = 0.04597327720617126\n",
      "Loss: 9.024762742963562e-05 | GradNorm^2: 6.5417691324655e-10 | s: 0.04590461675506792 | s_nil + z = 0.04594018165895946\n",
      "Loss: 8.949825361617638e-05 | GradNorm^2: 6.395031270443689e-10 | s: 0.04590768707510832 | s_nil + z = 0.045854033482432346\n",
      "Loss: 8.87592309319841e-05 | GradNorm^2: 6.213342073955188e-10 | s: 0.0460024737784987 | s_nil + z = 0.04586747083887665\n",
      "Loss: 8.806792796618392e-05 | GradNorm^2: 6.741061464003884e-10 | s: 0.04600160441960275 | s_nil + z = 0.04592242909259463\n",
      "Loss: 8.734054102224477e-05 | GradNorm^2: 6.276497409260092e-10 | s: 0.045912522311427464 | s_nil + z = 0.045899565096407295\n",
      "Loss: 8.663104706814824e-05 | GradNorm^2: 5.939246098523691e-10 | s: 0.0458838880612027 | s_nil + z = 0.04584854024316824\n",
      "Loss: 8.5942763206217e-05 | GradNorm^2: 5.786125780605777e-10 | s: 0.04591251349117426 | s_nil + z = 0.04626531340022107\n",
      "Loss: 8.527474704500448e-05 | GradNorm^2: 5.869129698997081e-10 | s: 0.045901264731056185 | s_nil + z = 0.04588969567510559\n",
      "Loss: 8.460238192012743e-05 | GradNorm^2: 5.605051649330587e-10 | s: 0.04586365088592806 | s_nil + z = 0.04589110894096476\n",
      "Loss: 8.394974700448746e-05 | GradNorm^2: 5.50768372364994e-10 | s: 0.04587764796598327 | s_nil + z = 0.045893275446250596\n",
      "Loss: 8.331572237680004e-05 | GradNorm^2: 5.626542926565288e-10 | s: 0.045909247674296945 | s_nil + z = 0.04587188646575332\n",
      "Loss: 8.273181468172679e-05 | GradNorm^2: 6.491427199882068e-10 | s: 0.04595853094219782 | s_nil + z = 0.045893959583453485\n",
      "Loss: 8.205038509369985e-05 | GradNorm^2: 5.259837926009516e-10 | s: 0.04585677705955358 | s_nil + z = 0.045899491775875184\n",
      "Loss: 8.14450855422839e-05 | GradNorm^2: 5.322539793100634e-10 | s: 0.04589647473783525 | s_nil + z = 0.0458915411731359\n",
      "Loss: 8.083746691939825e-05 | GradNorm^2: 5.17598233710709e-10 | s: 0.04586367833931446 | s_nil + z = 0.04586263231558353\n",
      "Loss: 8.02517276069707e-05 | GradNorm^2: 5.259948602868162e-10 | s: 0.04591364550405422 | s_nil + z = 0.04587491500492259\n",
      "Loss: 7.965591991021496e-05 | GradNorm^2: 4.974242887365379e-10 | s: 0.04591995113459139 | s_nil + z = 0.045912350567858556\n",
      "Loss: 7.907862724226475e-05 | GradNorm^2: 4.865623427936963e-10 | s: 0.04584896197420096 | s_nil + z = 0.04593980590449997\n",
      "Loss: 7.852882824814447e-05 | GradNorm^2: 5.123782822551268e-10 | s: 0.04588134065202581 | s_nil + z = 0.045878816964086\n",
      "Loss: 7.795099461277673e-05 | GradNorm^2: 4.727949396519116e-10 | s: 0.04587180107167942 | s_nil + z = 0.04585616185597204\n",
      "Loss: 7.740004939093488e-05 | GradNorm^2: 4.6565271509474717e-10 | s: 0.04597240266081271 | s_nil + z = 0.04587655038137989\n",
      "Loss: 7.68566860979017e-05 | GradNorm^2: 4.604177752132898e-10 | s: 0.04588307810321813 | s_nil + z = 0.0459869429114692\n",
      "Loss: 7.633822285723963e-05 | GradNorm^2: 4.82541919865073e-10 | s: 0.045938868505127795 | s_nil + z = 0.04591764148377026\n",
      "Loss: 7.580701299094645e-05 | GradNorm^2: 4.700299263075806e-10 | s: 0.045895904904988474 | s_nil + z = 0.04592979555745759\n",
      "Loss: 7.53629210309061e-05 | GradNorm^2: 5.961026436573551e-10 | s: 0.04600478270923302 | s_nil + z = 0.04601748138457899\n",
      "Loss: 7.482847082093424e-05 | GradNorm^2: 5.494464809451708e-10 | s: 0.04586515263921464 | s_nil + z = 0.04588335876760302\n",
      "Loss: 7.42648664206049e-05 | GradNorm^2: 4.4180875102198337e-10 | s: 0.04593361816966107 | s_nil + z = 0.04596412563508582\n",
      "Loss: 7.376162252212162e-05 | GradNorm^2: 4.2679403121411626e-10 | s: 0.04587022938715145 | s_nil + z = 0.04591215438420934\n",
      "Loss: 7.328233629996096e-05 | GradNorm^2: 4.415978691936905e-10 | s: 0.045957227742899384 | s_nil + z = 0.045868025457752135\n",
      "Loss: 7.279472488804291e-05 | GradNorm^2: 4.3102207819897033e-10 | s: 0.04595341486840639 | s_nil + z = 0.04587832169361428\n",
      "Loss: 7.230304340730216e-05 | GradNorm^2: 4.0406140682963263e-10 | s: 0.0460147376325496 | s_nil + z = 0.04594613288569287\n",
      "Loss: 7.183162215804401e-05 | GradNorm^2: 3.986173922602826e-10 | s: 0.045882358531495866 | s_nil + z = 0.04601143535080269\n",
      "Loss: 7.136726070610904e-05 | GradNorm^2: 3.9470039330787684e-10 | s: 0.045990825147551956 | s_nil + z = 0.04593917912275686\n",
      "Loss: 7.091081462687332e-05 | GradNorm^2: 3.9387886157803833e-10 | s: 0.04590337782136705 | s_nil + z = 0.045857383140646975\n",
      "Loss: 7.045413524507827e-05 | GradNorm^2: 3.83158250125245e-10 | s: 0.045987970255146206 | s_nil + z = 0.04588164383521044\n",
      "Loss: 7.000754819872926e-05 | GradNorm^2: 3.8093707973012865e-10 | s: 0.04590131179061096 | s_nil + z = 0.04589149738862402\n",
      "Loss: 6.956594162019871e-05 | GradNorm^2: 3.764077297241245e-10 | s: 0.046010517192786336 | s_nil + z = 0.045881212953504544\n",
      "Loss: 6.913807714490113e-05 | GradNorm^2: 3.859225460456989e-10 | s: 0.0459631338383301 | s_nil + z = 0.04585359258226885\n",
      "Loss: 6.869756186365311e-05 | GradNorm^2: 3.6450642445160005e-10 | s: 0.04588342779479519 | s_nil + z = 0.04584993527687547\n",
      "Loss: 6.82717301004558e-05 | GradNorm^2: 3.589260425883275e-10 | s: 0.04592042722331868 | s_nil + z = 0.04598865419878144\n",
      "Loss: 6.785549168645913e-05 | GradNorm^2: 3.59085749296187e-10 | s: 0.04585030016783025 | s_nil + z = 0.04591935704587097\n",
      "Loss: 6.744115820784162e-05 | GradNorm^2: 3.543525969690372e-10 | s: 0.04592017478804231 | s_nil + z = 0.04586354173333294\n",
      "Loss: 6.703012114027881e-05 | GradNorm^2: 3.458906013886207e-10 | s: 0.045877281096670275 | s_nil + z = 0.045962045023835\n",
      "Loss: 6.662579423649231e-05 | GradNorm^2: 3.4460221007378456e-10 | s: 0.04585648665548162 | s_nil + z = 0.045957864368498656\n",
      "Loss: 6.622740788859452e-05 | GradNorm^2: 3.4133430522772556e-10 | s: 0.04587249166323602 | s_nil + z = 0.04590803695849721\n",
      "Loss: 6.584336968811963e-05 | GradNorm^2: 3.518242232230993e-10 | s: 0.045854538313038304 | s_nil + z = 0.04586659288376925\n",
      "Loss: 6.544272614883681e-05 | GradNorm^2: 3.2898350446519225e-10 | s: 0.045895353331949815 | s_nil + z = 0.04584481330971325\n",
      "Loss: 6.506502479058345e-05 | GradNorm^2: 3.3485780679732447e-10 | s: 0.04586471179389838 | s_nil + z = 0.04588397707106374\n",
      "Loss: 6.467934103454653e-05 | GradNorm^2: 3.212521358135407e-10 | s: 0.04586820470578773 | s_nil + z = 0.045903467956784495\n",
      "Loss: 6.430437174014503e-05 | GradNorm^2: 3.172220118820117e-10 | s: 0.04586206930207895 | s_nil + z = 0.04585303871409226\n",
      "Loss: 6.393313151314896e-05 | GradNorm^2: 3.134566168696125e-10 | s: 0.045881317387480226 | s_nil + z = 0.04585093772906073\n",
      "Loss: 6.356669271790278e-05 | GradNorm^2: 3.1035034904222667e-10 | s: 0.045906943518785874 | s_nil + z = 0.04588140717778348\n",
      "Loss: 6.320527296518111e-05 | GradNorm^2: 3.07105493415685e-10 | s: 0.04586963198437052 | s_nil + z = 0.045877784339511084\n",
      "Loss: 6.28500077172869e-05 | GradNorm^2: 3.0712373230629565e-10 | s: 0.0458783914454751 | s_nil + z = 0.04586287957659515\n",
      "Loss: 6.249552618950206e-05 | GradNorm^2: 3.0114490626238234e-10 | s: 0.04584394170057677 | s_nil + z = 0.04592302078270016\n",
      "Loss: 6.214849270189982e-05 | GradNorm^2: 3.0057118909267696e-10 | s: 0.04584761091649404 | s_nil + z = 0.045919425100565125\n",
      "Loss: 6.180980412829816e-05 | GradNorm^2: 3.067844477979872e-10 | s: 0.045890387806713305 | s_nil + z = 0.04593082694116995\n",
      "Loss: 6.146139983637572e-05 | GradNorm^2: 2.9296467899607137e-10 | s: 0.045879455694808925 | s_nil + z = 0.045878968634637335\n",
      "Loss: 6.112097441132695e-05 | GradNorm^2: 2.860764289854179e-10 | s: 0.04587373915868708 | s_nil + z = 0.04593003915642377\n",
      "Loss: 6.0786912210105024e-05 | GradNorm^2: 2.82597773752673e-10 | s: 0.04588797264729723 | s_nil + z = 0.04591543426171536\n",
      "Loss: 6.047792834157766e-05 | GradNorm^2: 3.0872302914302455e-10 | s: 0.04586818242814316 | s_nil + z = 0.04584335542002209\n",
      "Loss: 6.013377376575288e-05 | GradNorm^2: 2.813853444503666e-10 | s: 0.04588020880115629 | s_nil + z = 0.045878489574607224\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "alpha = 1e-4\n",
    "beta=0.999\n",
    "w = torch.zeros(train_data.shape[1], device=device).requires_grad_()\n",
    "s = torch.tensor(0.0)\n",
    "lmd = 0.01\n",
    "mu = 0.1\n",
    "delta = lmd + mu\n",
    "\n",
    "loss_function = loss_class(w)\n",
    "\n",
    "# save loss and grad size to history\n",
    "hist_psps2_l2 = []\n",
    "loss = loss_function(train_data.to(device), train_target.to(device))\n",
    "g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "print(f\"Loss: {loss.item()} | GradNorm^2: {(torch.linalg.norm(g) ** 2 ).item()}\")\n",
    "hist_psps2_l2.append([loss.item(), (torch.linalg.norm(g) ** 2).item(), s])\n",
    "\n",
    "# preconditioninig matrix\n",
    "Dk = diag_estimate_old(w, g, 100)\n",
    "\n",
    "for step in range(STEPS):\n",
    "\n",
    "    for i, (batch_data, batch_target) in enumerate(train_dataloader):\n",
    "\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_target = batch_target.to(device)\n",
    "\n",
    "        loss = loss_function(batch_data, batch_target)\n",
    "\n",
    "        s_nil = s - (lmd/(2*mu))\n",
    "        z = s - s_nil\n",
    "        \n",
    "        if (i != 0) and s_nil >= loss.item():\n",
    "            continue\n",
    "\n",
    "        g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "\n",
    "        vk = diag_estimate_old(w, g, 1)\n",
    "\n",
    "        # Smoothing and Truncation \n",
    "        Dk = beta * Dk + (1 - beta) * vk\n",
    "        Dk_hat = torch.abs(Dk)\n",
    "        Dk_hat[Dk_hat < alpha] = alpha\n",
    "\n",
    "        Dk_hat_inv = 1 / Dk_hat\n",
    "\n",
    "        gnorm = (g * Dk_hat_inv).dot(g)\n",
    "\n",
    "        f_grad = g.clone().detach()\n",
    "\n",
    "        if gnorm < 1e-8:\n",
    "            continue\n",
    "\n",
    "        t = loss.item() - s_nil\n",
    "        a = torch.dot(f_grad, Dk_hat_inv*f_grad).cpu().detach().numpy()\n",
    "        \n",
    "        AA = 1\n",
    "        BB = 2 + delta * a - 2 * delta * t\n",
    "        CC = 1 + 2 * delta * a - 4 * delta * t\n",
    "        DD = -2 * delta * t\n",
    "\n",
    "        roots = solve(AA, BB, CC, DD)\n",
    "        roots = torch.from_numpy(roots)      \n",
    "        root_star = torch.relu(torch.max(roots))\n",
    "\n",
    "        s = (1/delta) * (mu * s + (root_star/2)).item()\n",
    "        \n",
    "        precond = root_star/(1 + root_star) * Dk_hat_inv    \n",
    "        with torch.no_grad():\n",
    "            w.sub_(precond  * f_grad)\n",
    "\n",
    "    loss = loss_function(train_data.to(device), train_target.to(device))\n",
    "    g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "    print(f\"Loss: {loss.item()} | GradNorm^2: {(torch.linalg.norm(g) ** 2 ).item()} | s: {s} | s_nil + z = {s_nil + z}\")\n",
    "    hist_psps2_l2.append([loss.item(), (torch.linalg.norm(g) ** 2).item(), s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1728x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 6))\n",
    "\n",
    "\n",
    "ax1.semilogy([x[0] for x in hist_psps2])\n",
    "ax1.semilogy([x[0] for x in hist_psps2_l1])\n",
    "ax1.semilogy([x[0] for x in hist_psps2_l2])\n",
    "ax1.semilogy([x[0] for x in hist_sgd])\n",
    "ax1.semilogy([x[0] for x in hist_adam])\n",
    "\n",
    "\n",
    "ax2.semilogy([x[1] for x in hist_psps2])\n",
    "ax2.semilogy([x[1] for x in hist_psps2_l1])\n",
    "ax2.semilogy([x[1] for x in hist_psps2_l2])\n",
    "ax2.semilogy([x[1] for x in hist_sgd])\n",
    "ax2.semilogy([x[1] for x in hist_adam])\n",
    "\n",
    "\n",
    "ax3.semilogy([0 for x in hist_psps2])\n",
    "ax3.semilogy([x[2] for x in hist_psps2_l1])\n",
    "ax3.semilogy([x[2] for x in hist_psps2_l2])\n",
    "ax3.semilogy([0 for x in hist_sgd])\n",
    "ax3.semilogy([0 for x in hist_adam])\n",
    "\n",
    "\n",
    "ax1.set_ylabel('loss', fontsize=20)\n",
    "ax2.set_ylabel('GradNorm^2', fontsize=20)\n",
    "ax3.set_ylabel('s', fontsize=20)\n",
    "\n",
    "fig.text(0.5, 0.04, 'epochs', ha='center', va='center', )\n",
    "fig.suptitle(f\"Dataset: {dataset_name} Loss: {loss_class}\")\n",
    "fig.legend(['PSPS2', 'PSPS2-L1', 'PSPS2-L2', 'SGD', 'Aadm'])\n",
    "plt.tight_layout()\n",
    "fig.show()\n",
    "\n",
    "# fig.savefig(f\"../plots/psps2_{dataset_name}_logreg_scale:{scale_range}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "from datasets import get_dataset\n",
    "\n",
    "from loss_fns import LogisticRegression, NLLSQ\n",
    "from optimizers import SPS\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Hutch\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "class SPS2(Optimizer):\n",
    "\n",
    "    def __init__(self, params, preconditioner=\"none\", slack_method=\"none\", lmd=0.01, mu=0.1):\n",
    "        defaults = dict(s=0.0)\n",
    "        self.lmd = lmd\n",
    "        self.lmd_hat = 1.0 / (1.0 + self.lmd)\n",
    "        \n",
    "        self.s = 0.0\n",
    "        self.mu = mu\n",
    "        self.delta = lmd + mu\n",
    "        \n",
    "        if slack_method == \"L1\":\n",
    "            self.update = self.update_L1\n",
    "        elif slack_method == \"L2\":\n",
    "            self.update = self.update_L2\n",
    "        else:\n",
    "            self.update = self.update_sps\n",
    "\n",
    "        self.preconditioner = None\n",
    "        if preconditioner == \"hutch\":\n",
    "            self.preconditioner = Hutch()\n",
    "            \n",
    "        self.preconditioner_initialized = False\n",
    "\n",
    "        # TO-DO: Think of something better\n",
    "        self.replay_buffer = []\n",
    "\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        for group in self.param_groups:\n",
    "            weights = list(group[\"params\"])\n",
    "            \n",
    "            loss = closure()\n",
    "            grad = torch.autograd.grad(loss, weights, create_graph=True)\n",
    "            loss = loss.item()\n",
    "\n",
    "            if self.preconditioner:\n",
    "                if not self.preconditioner_initialized:\n",
    "                    self.preconditioner.init(self, grad, 100)\n",
    "                    self.preconditioner_initialized = True\n",
    "                self.preconditioner.step(self, grad, 1)\n",
    "            else:\n",
    "                self.init_empty_precond()\n",
    "                self.preconditioner_initialized = True\n",
    "            \n",
    "            gnorm_square = self.calc_grad_norm(grad).item()\n",
    "            \n",
    "            if gnorm_square < 1e-13:\n",
    "                continue \n",
    "\n",
    "            self.update(gnorm_square, loss)\n",
    "\n",
    "            self.replay_buffer.append({\n",
    "                \"loss\": loss,\n",
    "                \"grad_norm_sq\": gnorm_square,\n",
    "                \"slack\": self.s,\n",
    "            }) \n",
    "                          \n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_sps(self, gnorm_square, loss):\n",
    "\n",
    "        det = 1 - (2 * loss / gnorm_square )\n",
    "        if det < 1e-15:\n",
    "            return\n",
    "        else:\n",
    "            t = torch.sqrt(det)/det\n",
    "            root1 = -1 + t\n",
    "            root2 = -1 - t\n",
    "            root = torch.maximum(root1, root2)\n",
    "            precond = root/(1 + root)\n",
    "\n",
    "        for group in self.param_groups: \n",
    "            for p in group['params']:\n",
    "                p.sub_(self.state[p]['scaled_grad'].mul(precond))\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_L1(self, gnorm_square, loss):\n",
    "\n",
    "            s_nil = self.s - (self.lmd/(2*self.mu))\n",
    "            z = self.s - s_nil\n",
    "\n",
    "            t = loss - s_nil\n",
    "            AA = 1\n",
    "            BB = 2 + self.mu * gnorm_square - 2 * self.mu * t\n",
    "            CC = 1 + 2 * self.mu * gnorm_square - 4 * self.mu * t\n",
    "            DD = -2 * self.mu * t\n",
    "\n",
    "            roots = solve(AA, BB, CC, DD)\n",
    "            roots = torch.from_numpy(roots)\n",
    "            root_star = torch.relu(torch.max(roots))\n",
    "\n",
    "            self.s = torch.relu(self.s - ((self.lmd - root_star)/2 * self.mu)).item()\n",
    "            \n",
    "            precond = root_star/(1 + root_star)\n",
    "\n",
    "            for group in self.param_groups:\n",
    "                for p in group['params']:\n",
    "                    p.sub_(self.state[p]['scaled_grad'].mul(precond))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_L2(self, gnorm_square ,loss):\n",
    "        s_nil = self.s - (self.lmd/(2*self.mu))\n",
    "        z = self.s - s_nil\n",
    "\n",
    "        t = loss - s_nil\n",
    "\n",
    "        AA = 1\n",
    "        BB = 2 + self.delta * gnorm_square - 2 * self.delta * t\n",
    "        CC = 1 + 2 * self.delta * gnorm_square - 4 * self.delta * t\n",
    "        DD = -2 * self.delta * t\n",
    "\n",
    "        roots = solve(AA, BB, CC, DD)\n",
    "        roots = torch.from_numpy(roots)       \n",
    "        root_star = torch.relu(torch.max(roots))\n",
    "\n",
    "        self.s = (1/(self.delta)) * (self.mu * self.s + (root_star/2)).item()\n",
    "\n",
    "        precond = root_star/(1 + root_star) \n",
    "        \n",
    "        for group in self.param_groups: \n",
    "            for p in group['params']:\n",
    "                p.sub_(self.state[p]['scaled_grad'].mul(precond))\n",
    "            \n",
    "\n",
    "    def init_empty_precond(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                self.state[p]['Dk'] = torch.ones_like(p) \n",
    "                self.state[p]['DkhatInv'] = torch.ones_like(p)\n",
    "\n",
    "    def calc_grad_norm(self, grad):\n",
    "        for group in self.param_groups: \n",
    "            gnorm_square = 0.\n",
    "            for p, g in zip(group['params'], grad):\n",
    "                g_detached = g.detach().clone()\n",
    "                self.state[p]['scaled_grad'] = self.state[p]['DkhatInv'].mul(g_detached)\n",
    "                gnorm_sq = self.state[p]['scaled_grad'].mul(g_detached).sum()\n",
    "                gnorm_square += gnorm_sq\n",
    "\n",
    "        return gnorm_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "BS = 128\n",
    "trainset = torchvision.datasets.MNIST(root='../datasets', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=BS, shuffle=False)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='../datasets', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters 970\n"
     ]
    }
   ],
   "source": [
    "from nn_models import SmallLeNet\n",
    "from torch.functional import F\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "model = SmallLeNet().to(device)\n",
    "\n",
    "w0 = [w+0.0 for w in model.parameters()]\n",
    "d = np.sum([w.numel() for w in model.parameters()])\n",
    "print (\"Total parameters\",d)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_model(model, loss_fn, data_loader):\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss += loss_fn(outputs, labels).item() / len(data_loader)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()  \n",
    "    \n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, criterion, train_loader, test_loader, epochs, optimizer_class, **optimizer_kwargs):\n",
    "\n",
    "    optimizer = optimizer_class(model.parameters(), **optimizer_kwargs)\n",
    "\n",
    "    slack = 0\n",
    "\n",
    "    hist = []\n",
    "    with torch.no_grad():\n",
    "        train_loss, train_acc = eval_model(model, criterion, train_loader) \n",
    "        print(f\"Epoch[{0}] Train Loss: {train_loss} | Train Acc: {train_acc}\")\n",
    "\n",
    "        test_loss, test_acc = eval_model(model, criterion, test_loader)\n",
    "        print(f\"Epoch[{0}] Test Loss: {test_loss} | Test Acc: {test_acc}\")\n",
    "\n",
    "        hist.append([train_loss, train_acc, test_loss, test_acc, slack])\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            \n",
    "            # images = images.reshape(-1, images.shape[2] * images.shape[3]).to(device)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            def closure():\n",
    "                outputs = model(images)\n",
    "                return criterion(outputs, labels)\n",
    "            \n",
    "            loss = closure()\n",
    "\n",
    "            if isinstance(optimizer, SPS2):     \n",
    "                optimizer.step(closure) \n",
    "                slack = optimizer.replay_buffer[-1][\"slack\"]\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                train_loss, train_acc = eval_model(model, criterion, train_loader) \n",
    "                print(f\"Epoch[{epoch + 1}] Train Loss: {train_loss} | Train Acc: {train_acc}\")\n",
    "\n",
    "                test_loss, test_acc = eval_model(model, criterion, test_loader)\n",
    "                print(f\"Epoch[{epoch + 1}] Test Loss: {test_loss} | Test Acc: {test_acc}\")\n",
    "\n",
    "                hist.append([train_loss, train_acc, test_loss, test_acc, slack])\n",
    "        \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0] Train Loss: 2.3019664888816433 | Train Acc: 12.926666666666666\n",
      "Epoch[0] Test Loss: 6.876575052773815 | Test Acc: 12.926666666666666\n",
      "Epoch[1] Train Loss: 2.3014224280354996 | Train Acc: 10.441666666666666\n",
      "Epoch[1] Test Loss: 6.874949800946807 | Test Acc: 10.441666666666666\n",
      "Epoch[11] Train Loss: 2.301307879455556 | Train Acc: 11.236666666666666\n",
      "Epoch[11] Test Loss: 6.874607614424554 | Test Acc: 11.236666666666666\n",
      "Epoch[21] Train Loss: 0.662201900174952 | Train Acc: 77.72666666666667\n",
      "Epoch[21] Test Loss: 1.978170007528996 | Test Acc: 77.72666666666667\n",
      "Epoch[31] Train Loss: 0.22736503590059448 | Train Acc: 93.08333333333333\n",
      "Epoch[31] Test Loss: 0.6791987378176987 | Test Acc: 93.08333333333333\n",
      "Epoch[41] Train Loss: 0.15938084733383004 | Train Acc: 95.13333333333334\n",
      "Epoch[41] Test Loss: 0.4761122127360913 | Test Acc: 95.13333333333334\n",
      "Epoch[51] Train Loss: 0.14485859505780754 | Train Acc: 95.44\n",
      "Epoch[51] Test Loss: 0.43273045275230404 | Test Acc: 95.44\n",
      "Epoch[61] Train Loss: 0.13882838095857886 | Train Acc: 95.54666666666667\n",
      "Epoch[61] Test Loss: 0.4147166284686209 | Test Acc: 95.54666666666667\n",
      "Epoch[71] Train Loss: 0.14163830794812984 | Train Acc: 95.40333333333334\n",
      "Epoch[71] Test Loss: 0.42311061418899826 | Test Acc: 95.40333333333334\n",
      "Epoch[81] Train Loss: 0.13536984421875695 | Train Acc: 95.59666666666666\n",
      "Epoch[81] Test Loss: 0.4043850760420188 | Test Acc: 95.59666666666666\n",
      "Epoch[91] Train Loss: 0.1269398140737338 | Train Acc: 95.86333333333333\n",
      "Epoch[91] Test Loss: 0.37920237452599476 | Test Acc: 95.86333333333333\n",
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         625699200 function calls (617806890 primitive calls) in 4025.573 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "   161458 2893.137    0.018 2893.137    0.018 {method 'item' of 'torch._C._TensorBase' objects}\n",
      "    93900  242.704    0.003  242.704    0.003 {method 'run_backward' of 'torch._C._EngineBase' objects}\n",
      "  7320000   67.734    0.000  472.331    0.000 functional.py:106(to_tensor)\n",
      "  7696000   62.983    0.000   62.983    0.000 {method 'div' of 'torch._C._TensorBase' objects}\n",
      "14686900/7366900   62.568    0.000  183.509    0.000 {built-in method numpy.array}\n",
      "  7434436   61.682    0.000   61.682    0.000 {method 'to' of 'torch._C._TensorBase' objects}\n",
      "  7320000   57.504    0.000  695.423    0.000 mnist.py:130(__getitem__)\n",
      "  7320000   36.919    0.000  140.286    0.000 Image.py:2764(fromarray)\n",
      "  7320000   27.368    0.000   80.192    0.000 Image.py:694(tobytes)\n",
      "  7320000   21.691    0.000   21.691    0.000 {method 'view' of 'torch._C._TensorBase' objects}\n",
      "  7320000   21.471    0.000  142.540    0.000 Image.py:661(__array__)\n",
      "  7320000   20.328    0.000   20.328    0.000 {method 'permute' of 'torch._C._TensorBase' objects}\n",
      " 14640000   19.420    0.000   28.924    0.000 Image.py:533(_new)\n",
      "  7320000   18.549    0.000  101.113    0.000 Image.py:2711(frombuffer)\n",
      "   416472   17.929    0.000   17.929    0.000 {built-in method conv2d}\n",
      "  7320000   15.033    0.000   53.218    0.000 Image.py:2634(new)\n",
      "  7320000   14.829    0.000   22.132    0.000 Image.py:409(_getencoder)\n",
      "  7320000   14.514    0.000   25.587    0.000 utils.py:526(_log_api_usage_once)\n",
      "  1877600   13.972    0.000   13.972    0.000 {method 'mul' of 'torch._C._TensorBase' objects}\n",
      "  7366900   12.751    0.000   12.751    0.000 {built-in method from_numpy}\n",
      " 21960000   12.537    0.000   12.537    0.000 Image.py:497(__init__)\n",
      " 66895273   11.835    0.000   12.191    0.000 {built-in method builtins.isinstance}\n",
      "  7320000   11.791    0.000  484.123    0.000 transforms.py:127(__call__)\n",
      "  7320000   11.298    0.000   11.298    0.000 {method 'encode' of 'ImagingEncoder' objects}\n",
      "  7320000   10.651    0.000   10.651    0.000 {built-in method PIL._imaging.fill}\n",
      " 14640000   10.018    0.000   13.588    0.000 Image.py:2616(_check_size)\n",
      "  7320001    8.310    0.000    8.310    0.000 {built-in method torch._C._log_api_usage_once}\n",
      "  7320000    8.125    0.000   10.672    0.000 Image.py:242(_conv_type_shape)\n",
      "    47000    7.836    0.000   13.688    0.000 utils.py:5(<listcomp>)\n",
      "  7320000    7.650    0.000    7.650    0.000 {built-in method PIL._imaging.map_buffer}\n",
      "   761518    7.556    0.000    7.556    0.000 {method 'sum' of 'torch._C._TensorBase' objects}\n",
      "    57218    7.463    0.000    7.463    0.000 {built-in method stack}\n",
      "   751200    7.379    0.000    7.379    0.000 {method 'mul_' of 'torch._C._TensorBase' objects}\n",
      "  7320000    7.027    0.000    7.027    0.000 {method 'numpy' of 'torch._C._TensorBase' objects}\n",
      " 14640000    6.759    0.000    6.759    0.000 Image.py:510(__getattr__)\n",
      "  7320000    6.744    0.000    8.645    0.000 Image.py:788(load)\n",
      "  7320000    6.484    0.000  490.607    0.000 transforms.py:93(__call__)\n",
      "59018257/58976863    6.067    0.000    6.175    0.000 {built-in method builtins.len}\n",
      " 36600000    6.045    0.000    6.045    0.000 Image.py:529(size)\n",
      "  7320000    5.283    0.000    9.972    0.000 _trace.py:999(is_tracing)\n",
      "    57218    4.966    0.000  700.389    0.012 fetch.py:49(<listcomp>)\n",
      "    46900    4.820    0.000   18.372    0.000 3077094892.py:139(calc_grad_norm)\n",
      "   751200    4.779    0.000    4.779    0.000 {method 'add_' of 'torch._C._TensorBase' objects}\n",
      "   406154    4.733    0.000    4.733    0.000 {built-in method relu}\n",
      "  7320000    4.515    0.000    7.508    0.000 Image.py:1225(getbands)\n",
      "    47000    4.147    0.000  193.350    0.004 utils.py:7(hvp_from_grad)\n",
      "   312354    4.102    0.000    4.102    0.000 {built-in method max_pool2d}\n",
      "  7320000    3.841    0.000    3.841    0.000 {built-in method torch._C._is_tracing}\n",
      "   375200    3.798    0.000    3.798    0.000 {method 'clone' of 'torch._C._TensorBase' objects}\n",
      "    46900    3.449    0.000  241.233    0.005 utils.py:29(step)\n",
      "  7320000    3.334    0.000    3.334    0.000 {built-in method PIL._imaging.raw_encoder}\n",
      "   375200    3.332    0.000    3.332    0.000 {built-in method abs}\n",
      "  7320000    3.321    0.000    3.321    0.000 {built-in method builtins.max}\n",
      "   375208    3.299    0.000    3.299    0.000 {built-in method zeros_like}\n",
      "   376000    3.160    0.000    3.160    0.000 {built-in method rand_like}\n",
      "  7320000    3.015    0.000    3.810    0.000 functional_pil.py:15(_is_pil_image)\n",
      "  7320000    2.993    0.000    2.993    0.000 ImageMode.py:33(getmode)\n",
      "   375200    2.876    0.000    2.876    0.000 {built-in method clamp}\n",
      "  7320000    2.856    0.000    2.856    0.000 {built-in method torch._C.get_default_dtype}\n",
      "  7320000    2.816    0.000    4.199    0.000 functional.py:96(_is_numpy)\n",
      "  7320001    2.693    0.000    2.693    0.000 {built-in method builtins.getattr}\n",
      "   376000    2.692    0.000    2.692    0.000 {built-in method round}\n",
      "    46901    2.687    0.000  228.438    0.005 utils.py:42(diag_estimate)\n",
      "    46900    2.667    0.000   11.425    0.000 3077094892.py:84(update_L1)\n",
      "  7320000    2.586    0.000    2.586    0.000 {method 'contiguous' of 'torch._C._TensorBase' objects}\n",
      "  7320000    2.542    0.000    2.542    0.000 {method 'setimage' of 'ImagingEncoder' objects}\n",
      " 14780812    2.530    0.000    2.530    0.000 {method 'append' of 'list' objects}\n",
      "   375200    2.442    0.000    2.442    0.000 {built-in method reciprocal}\n",
      "    57340    2.357    0.000    4.007    0.000 sampler.py:224(__iter__)\n",
      " 14686900    2.299    0.000    2.299    0.000 {method 'get' of 'dict' objects}\n",
      "    46900    2.223    0.000 3244.844    0.069 optimizer.py:83(wrapper)\n",
      "    57340    2.219    0.000  720.188    0.013 dataloader.py:568(_next_data)\n",
      "   375200    2.218    0.000    2.218    0.000 {method 'sub_' of 'torch._C._TensorBase' objects}\n",
      "   104118    2.169    0.000   36.213    0.000 nn_models.py:18(forward)\n",
      " 14640000    2.030    0.000    2.030    0.000 {method 'copy' of 'dict' objects}\n",
      " 14780833    1.972    0.000    1.972    0.000 _jit_internal.py:957(is_scripting)\n",
      "  7320000    1.958    0.000    1.958    0.000 {method 'startswith' of 'str' objects}\n",
      "  7320000    1.901    0.000    1.901    0.000 {method 'pixel_access' of 'ImagingCore' objects}\n",
      "  7320000    1.847    0.000    1.847    0.000 Image.py:658(__init__)\n",
      "        1    1.835    1.835 4025.573 4025.573 1497878416.py:1(train_nn)\n",
      "   751200    1.642    0.000    1.642    0.000 {method 'detach' of 'torch._C._TensorBase' objects}\n",
      "   104118    1.510    0.000    1.510    0.000 {built-in method torch._C._nn.nll_loss_nd}\n",
      "   151140    1.506    0.000    1.506    0.000 {built-in method torch._ops.profiler._record_function_enter}\n",
      "    46900    1.504    0.000 3241.183    0.069 3077094892.py:34(step)\n",
      "  7320000    1.399    0.000    1.824    0.000 collate.py:167(<genexpr>)\n",
      "171654/57218    1.368    0.000   13.211    0.000 collate.py:84(default_collate)\n",
      "  7320000    1.352    0.000    1.352    0.000 {method 'join' of 'bytes' objects}\n",
      "   104118    1.288    0.000    1.288    0.000 {method 'log_softmax' of 'torch._C._TensorBase' objects}\n",
      "520590/104118    1.259    0.000   36.835    0.000 module.py:1104(_call_impl)\n",
      "    93900    1.177    0.000    1.177    0.000 {built-in method ones_like}\n",
      "  2626448    1.164    0.000    1.855    0.000 _tensor.py:706(__hash__)\n",
      "    57218    1.132    0.000    1.132    0.000 {built-in method tensor}\n",
      "   151140    0.963    0.000    0.963    0.000 {built-in method zeros}\n",
      "  1249416    0.896    0.000    0.896    0.000 module.py:1172(__getattr__)\n",
      "   416472    0.849    0.000   19.724    0.000 conv.py:446(forward)\n",
      "   312354    0.833    0.000    5.309    0.000 functional.py:769(_max_pool2d)\n",
      "   208236    0.803    0.000    0.803    0.000 {method 'squeeze' of 'torch._C._TensorBase' objects}\n",
      "   151140    0.755    0.000    0.755    0.000 {built-in method torch._ops.profiler._record_function_exit}\n",
      "    57218    0.656    0.000    2.480    0.000 {built-in method builtins.all}\n",
      "    57218    0.656    0.000    0.656    0.000 {built-in method max}\n",
      "    93900    0.632    0.000  245.271    0.003 __init__.py:177(grad)\n",
      "   416472    0.604    0.000   18.533    0.000 conv.py:438(_conv_forward)\n",
      "    93800    0.601    0.000    0.601    0.000 {built-in method rsub}\n",
      "    46900    0.522    0.000    2.148    0.000 optimizer.py:189(zero_grad)\n",
      "  3740914    0.502    0.000    0.502    0.000 {built-in method torch._C._has_torch_function_unary}\n",
      "   140833    0.501    0.000    0.535    0.000 grad_mode.py:121(__init__)\n",
      "   531030    0.491    0.000    0.491    0.000 {built-in method torch._C._get_tracing_state}\n",
      "    93900    0.472    0.000    1.740    0.000 __init__.py:30(_make_grads)\n",
      "   312354    0.455    0.000    5.832    0.000 _jit_internal.py:412(fn)\n",
      "    57340    0.452    0.000  722.187    0.013 dataloader.py:526(__next__)\n",
      "    46900    0.440    0.000    1.042    0.000 utils.py:93(solve)\n",
      "       22    0.436    0.020  146.479    6.658 658914316.py:1(eval_model)\n",
      "   281666    0.384    0.000    0.531    0.000 grad_mode.py:215(__init__)\n",
      "   151140    0.377    0.000    1.883    0.000 profiler.py:435(__enter__)\n",
      "  2626448    0.375    0.000    0.375    0.000 {built-in method builtins.id}\n",
      "   151140    0.348    0.000    1.103    0.000 profiler.py:439(__exit__)\n",
      "   312354    0.343    0.000    4.556    0.000 functional.py:1431(relu)\n",
      "    46922    0.343    0.000  158.869    0.003 grad_mode.py:24(decorate_context)\n",
      "   104118    0.318    0.000    1.927    0.000 functional.py:2607(nll_loss)\n",
      "   140833    0.285    0.000    0.616    0.000 grad_mode.py:126(__enter__)\n",
      "    57218    0.274    0.000  713.875    0.012 fetch.py:47(fetch)\n",
      "   312354    0.269    0.000    0.269    0.000 typing.py:306(inner)\n",
      "   151140    0.263    0.000    1.225    0.000 profiler.py:426(__init__)\n",
      "    46901    0.249    0.000    3.548    0.000 utils.py:43(<listcomp>)\n",
      "   375200    0.231    0.000    0.281    0.000 _tensor.py:1092(grad)\n",
      "    93800    0.224    0.000   35.325    0.000 1497878416.py:26(closure)\n",
      "   140833    0.207    0.000    0.490    0.000 grad_mode.py:130(__exit__)\n",
      "   422499    0.160    0.000    0.160    0.000 {built-in method torch._C.is_grad_enabled}\n",
      "    93800    0.160    0.000    0.780    0.000 _tensor.py:600(__rsub__)\n",
      "   104118    0.155    0.000    1.460    0.000 functional.py:1886(log_softmax)\n",
      "    57218    0.149    0.000    9.212    0.000 collate.py:172(<listcomp>)\n",
      "   114436    0.141    0.000    0.299    0.000 {built-in method _abc._abc_instancecheck}\n",
      "    93800    0.127    0.000    0.929    0.000 _tensor.py:26(wrapped)\n",
      "    57218    0.107    0.000    0.132    0.000 {built-in method _abc._abc_subclasscheck}\n",
      "    46900    0.101    0.000    0.101    0.000 utils.py:161(findF)\n",
      "    47023    0.084    0.000    0.084    0.000 {method 'format' of 'str' objects}\n",
      "    46922    0.078    0.000    0.261    0.000 grad_mode.py:82(clone)\n",
      "   281666    0.071    0.000    0.071    0.000 {built-in method torch._C._set_grad_enabled}\n",
      "    47000    0.070    0.000   13.757    0.000 utils.py:4(rademacher)\n",
      "    93900    0.068    0.000    0.068    0.000 __init__.py:77(_tensor_or_tensors_to_tuple)\n",
      "   104118    0.064    0.000    0.064    0.000 _reduction.py:7(get_enum)\n",
      "   114558    0.058    0.000    4.066    0.000 {built-in method builtins.next}\n",
      "   312354    0.057    0.000    0.057    0.000 __init__.py:79(annotate)\n",
      "   114436    0.057    0.000    0.356    0.000 abc.py:117(__instancecheck__)\n",
      "   187700    0.056    0.000    0.056    0.000 {built-in method torch._C._has_torch_function}\n",
      "   197918    0.054    0.000    0.054    0.000 {built-in method torch._C._has_torch_function_variadic}\n",
      "    46900    0.054    0.000    0.054    0.000 utils.py:171(findH)\n",
      "   187504    0.053    0.000    0.053    0.000 {built-in method math.sqrt}\n",
      "    46900    0.047    0.000    0.047    0.000 utils.py:166(findG)\n",
      "    57340    0.041    0.000    4.094    0.000 dataloader.py:520(_next_index)\n",
      "    93900    0.040    0.000    0.040    0.000 {method 'numel' of 'torch._C._TensorBase' objects}\n",
      "   104118    0.038    0.000    0.038    0.000 {built-in method builtins.hasattr}\n",
      "    10440    0.034    0.000    0.054    0.000 _tensor.py:676(__len__)\n",
      "    57218    0.025    0.000    0.157    0.000 abc.py:121(__subclasscheck__)\n",
      "    57218    0.025    0.000    0.025    0.000 _collections_abc.py:315(__subclasshook__)\n",
      "    10318    0.019    0.000    0.142    0.000 dataloader.py:386(__len__)\n",
      "    57462    0.016    0.000    0.016    0.000 {built-in method builtins.iter}\n",
      "      122    0.016    0.000    0.016    0.000 {method 'random_' of 'torch._C._TensorBase' objects}\n",
      "    57218    0.014    0.000    0.014    0.000 worker.py:83(get_worker_info)\n",
      "    10318    0.013    0.000    0.104    0.000 sampler.py:234(__len__)\n",
      "    10318    0.010    0.000    0.010    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n",
      "    10440    0.008    0.000    0.012    0.000 dataloader.py:374(_index_sampler)\n",
      "    10440    0.006    0.000    0.070    0.000 mnist.py:152(__len__)\n",
      "    10318    0.005    0.000    0.082    0.000 sampler.py:68(__len__)\n",
      "    10440    0.005    0.000    0.005    0.000 {method 'dim' of 'torch._C._TensorBase' objects}\n",
      "    10562    0.004    0.000    0.004    0.000 dataloader.py:370(_auto_collation)\n",
      "      122    0.002    0.000    0.023    0.000 dataloader.py:494(__init__)\n",
      "      122    0.002    0.000    0.002    0.000 {built-in method empty}\n",
      "       66    0.001    0.000    0.001    0.000 socket.py:480(send)\n",
      "      122    0.001    0.000    0.025    0.000 dataloader.py:560(__init__)\n",
      "      122    0.001    0.000    0.002    0.000 dataloader.py:51(create_fetcher)\n",
      "      122    0.001    0.000    0.027    0.000 dataloader.py:355(__iter__)\n",
      "       66    0.001    0.000    0.002    0.000 iostream.py:208(schedule)\n",
      "      122    0.001    0.000    0.026    0.000 dataloader.py:309(_get_iterator)\n",
      "      122    0.000    0.000    0.003    0.000 sampler.py:65(__iter__)\n",
      "       44    0.000    0.000    0.003    0.000 iostream.py:502(write)\n",
      "      122    0.000    0.000    0.001    0.000 fetch.py:44(__init__)\n",
      "       22    0.000    0.000    0.003    0.000 {built-in method builtins.print}\n",
      "       66    0.000    0.000    0.001    0.000 threading.py:1162(is_alive)\n",
      "       66    0.000    0.000    0.000    0.000 iostream.py:97(_event_pipe)\n",
      "      122    0.000    0.000    0.000    0.000 fetch.py:8(__init__)\n",
      "        1    0.000    0.000 4025.573 4025.573 {built-in method builtins.exec}\n",
      "       66    0.000    0.000    0.000    0.000 threading.py:1095(_wait_for_tstate_lock)\n",
      "       44    0.000    0.000    0.000    0.000 iostream.py:420(_is_master_process)\n",
      "       44    0.000    0.000    0.001    0.000 iostream.py:439(_schedule_flush)\n",
      "       66    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "        1    0.000    0.000 4025.573 4025.573 <string>:1(<module>)\n",
      "        1    0.000    0.000    5.697    5.697 utils.py:21(init)\n",
      "       48    0.000    0.000    0.000    0.000 {built-in method math.acos}\n",
      "        1    0.000    0.000    0.000    0.000 optimizer.py:243(add_param_group)\n",
      "       96    0.000    0.000    0.000    0.000 {built-in method math.cos}\n",
      "       44    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
      "        9    0.000    0.000    0.000    0.000 module.py:1501(_named_members)\n",
      "     14/6    0.000    0.000    0.000    0.000 module.py:1668(named_modules)\n",
      "       66    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
      "        1    0.000    0.000    0.000    0.000 optimizer.py:33(__init__)\n",
      "       66    0.000    0.000    0.000    0.000 threading.py:546(is_set)\n",
      "       48    0.000    0.000    0.000    0.000 {built-in method math.sin}\n",
      "        1    0.000    0.000    0.000    0.000 3077094892.py:6(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 optimizer.py:78(_hook_for_profile)\n",
      "        9    0.000    0.000    0.000    0.000 module.py:1538(named_parameters)\n",
      "       13    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}\n",
      "        9    0.000    0.000    0.000    0.000 module.py:1514(parameters)\n",
      "        5    0.000    0.000    0.000    0.000 module.py:1559(<lambda>)\n",
      "       10    0.000    0.000    0.000    0.000 {method 'items' of 'collections.OrderedDict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 utils.py:17(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'isdisjoint' of 'set' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}"
     ]
    }
   ],
   "source": [
    "model = SmallLeNet().to(device)\n",
    "%prun train_nn(model, F.nll_loss, train_loader, test_loader, EPOCHS, SPS2, preconditioner=\"hutch\", slack_method=\"L1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0] Train Loss: 2.3069560328657155 | Train Acc: 9.751666666666667\n",
      "Epoch[0] Test Loss: 6.891480123656184 | Test Acc: 9.751666666666667\n",
      "Epoch[1] Train Loss: 2.3034729687890527 | Train Acc: 9.751666666666667\n",
      "Epoch[1] Test Loss: 6.881075301669206 | Test Acc: 9.751666666666667\n",
      "Epoch[11] Train Loss: 2.2993483481158776 | Train Acc: 11.236666666666666\n",
      "Epoch[11] Test Loss: 6.868753982588193 | Test Acc: 11.236666666666666\n",
      "Epoch[21] Train Loss: 1.1616235388869802 | Train Acc: 63.48\n",
      "Epoch[21] Test Loss: 3.47007286457321 | Test Acc: 63.48\n",
      "Epoch[31] Train Loss: 0.5999340752588639 | Train Acc: 81.22166666666666\n",
      "Epoch[31] Test Loss: 1.7921597534802975 | Test Acc: 81.22166666666666\n",
      "Epoch[41] Train Loss: 0.44289743806796494 | Train Acc: 85.97333333333333\n",
      "Epoch[41] Test Loss: 1.323050308623411 | Test Acc: 85.97333333333333\n",
      "Epoch[51] Train Loss: 0.3666342368423777 | Train Acc: 88.42\n",
      "Epoch[51] Test Loss: 1.0952322106947463 | Test Acc: 88.42\n",
      "Epoch[61] Train Loss: 0.28845369996063447 | Train Acc: 90.96833333333333\n",
      "Epoch[61] Test Loss: 0.8616865304556527 | Test Acc: 90.96833333333333\n",
      "Epoch[71] Train Loss: 0.21125009656659918 | Train Acc: 93.41166666666666\n",
      "Epoch[71] Test Loss: 0.6310592056671015 | Test Acc: 93.41166666666666\n",
      "Epoch[81] Train Loss: 0.18637777462138164 | Train Acc: 94.12\n",
      "Epoch[81] Test Loss: 0.5567590846969944 | Test Acc: 94.12\n",
      "Epoch[91] Train Loss: 0.17548830303207794 | Train Acc: 94.45833333333333\n",
      "Epoch[91] Test Loss: 0.5242293893123855 | Test Acc: 94.45833333333333\n",
      "Epoch[101] Train Loss: 0.16921253776451944 | Train Acc: 94.675\n",
      "Epoch[101] Test Loss: 0.5054820395640743 | Test Acc: 94.675\n",
      "Epoch[111] Train Loss: 0.16144649203565273 | Train Acc: 95.00166666666667\n",
      "Epoch[111] Test Loss: 0.48228283289631285 | Test Acc: 95.00166666666667\n",
      "Epoch[121] Train Loss: 0.15623333050711763 | Train Acc: 95.16\n",
      "Epoch[121] Test Loss: 0.4667097580117081 | Test Acc: 95.16\n",
      "Epoch[131] Train Loss: 0.1490089527706787 | Train Acc: 95.39166666666667\n",
      "Epoch[131] Test Loss: 0.44512865509202726 | Test Acc: 95.39166666666667\n",
      "Epoch[141] Train Loss: 0.14229170866471183 | Train Acc: 95.57666666666667\n",
      "Epoch[141] Test Loss: 0.4250624927627385 | Test Acc: 95.57666666666667\n",
      "Epoch[151] Train Loss: 0.13830898721591078 | Train Acc: 95.62833333333333\n",
      "Epoch[151] Test Loss: 0.4131650637214145 | Test Acc: 95.62833333333333\n",
      "Epoch[161] Train Loss: 0.1329522120853151 | Train Acc: 95.755\n",
      "Epoch[161] Test Loss: 0.3971629775032658 | Test Acc: 95.755\n",
      "Epoch[171] Train Loss: 0.12510119284926596 | Train Acc: 96.02666666666667\n",
      "Epoch[171] Test Loss: 0.3737099327790179 | Test Acc: 96.02666666666667\n",
      "Epoch[181] Train Loss: 0.11921141791435311 | Train Acc: 96.18166666666667\n",
      "Epoch[181] Test Loss: 0.35611563695434106 | Test Acc: 96.18166666666667\n",
      "Epoch[191] Train Loss: 0.11590041398797503 | Train Acc: 96.31\n",
      "Epoch[191] Test Loss: 0.3462248035691733 | Test Acc: 96.31\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.random.manual_seed(42)\n",
    "\n",
    "model = SmallLeNet().to(device)\n",
    "\n",
    "small_lenet_sgd = train_nn(\n",
    "    model, \n",
    "    F.nll_loss,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    EPOCHS,\n",
    "    SGD,\n",
    "    lr=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0] Train Loss: 2.3069560328657155 | Train Acc: 9.751666666666667\n",
      "Epoch[0] Test Loss: 6.891480123656184 | Test Acc: 9.751666666666667\n",
      "Epoch[1] Train Loss: 0.4315668099918042 | Train Acc: 86.65\n",
      "Epoch[1] Test Loss: 1.2892027636060899 | Test Acc: 86.65\n",
      "Epoch[11] Train Loss: 0.2960347473971168 | Train Acc: 90.86833333333334\n",
      "Epoch[11] Test Loss: 0.884333098912407 | Test Acc: 90.86833333333334\n",
      "Epoch[21] Train Loss: 0.2732307922834956 | Train Acc: 91.53333333333333\n",
      "Epoch[21] Test Loss: 0.8162117298150281 | Test Acc: 91.53333333333333\n",
      "Epoch[31] Train Loss: 0.2519844343288064 | Train Acc: 92.07666666666667\n",
      "Epoch[31] Test Loss: 0.7527433101924224 | Test Acc: 92.07666666666667\n",
      "Epoch[41] Train Loss: 0.2385178731775062 | Train Acc: 92.48166666666667\n",
      "Epoch[41] Test Loss: 0.7125151752882187 | Test Acc: 92.48166666666667\n",
      "Epoch[51] Train Loss: 0.23594708019087388 | Train Acc: 92.53333333333333\n",
      "Epoch[51] Test Loss: 0.704835545283566 | Test Acc: 92.53333333333333\n",
      "Epoch[61] Train Loss: 0.23493600723998675 | Train Acc: 92.595\n",
      "Epoch[61] Test Loss: 0.7018152063411074 | Test Acc: 92.595\n",
      "Epoch[71] Train Loss: 0.23789691159642004 | Train Acc: 92.385\n",
      "Epoch[71] Test Loss: 0.7106602008835723 | Test Acc: 92.385\n",
      "Epoch[81] Train Loss: 0.21926671338968723 | Train Acc: 93.14666666666666\n",
      "Epoch[81] Test Loss: 0.6550069336290661 | Test Acc: 93.14666666666666\n",
      "Epoch[91] Train Loss: 0.2203056333555075 | Train Acc: 93.025\n",
      "Epoch[91] Test Loss: 0.6581104588772804 | Test Acc: 93.025\n",
      "Epoch[101] Train Loss: 0.18241150983611265 | Train Acc: 94.25833333333334\n",
      "Epoch[101] Test Loss: 0.5449108160072414 | Test Acc: 94.25833333333334\n",
      "Epoch[111] Train Loss: 0.1629628415824572 | Train Acc: 94.98166666666667\n",
      "Epoch[111] Test Loss: 0.48681256498199016 | Test Acc: 94.98166666666667\n",
      "Epoch[121] Train Loss: 0.18446760563492887 | Train Acc: 94.40833333333333\n",
      "Epoch[121] Test Loss: 0.5510529111005195 | Test Acc: 94.40833333333333\n",
      "Epoch[131] Train Loss: 0.1615702501607457 | Train Acc: 94.98333333333333\n",
      "Epoch[131] Test Loss: 0.4826525307349668 | Test Acc: 94.98333333333333\n",
      "Epoch[141] Train Loss: 0.16488918402767233 | Train Acc: 94.925\n",
      "Epoch[141] Test Loss: 0.4925670529234293 | Test Acc: 94.925\n",
      "Epoch[151] Train Loss: 0.14662991620459598 | Train Acc: 95.41\n",
      "Epoch[151] Test Loss: 0.43802185159207346 | Test Acc: 95.41\n",
      "Epoch[161] Train Loss: 0.15681669926419947 | Train Acc: 95.10333333333334\n",
      "Epoch[161] Test Loss: 0.4684524328338182 | Test Acc: 95.10333333333334\n",
      "Epoch[171] Train Loss: 0.14402229021645097 | Train Acc: 95.53666666666666\n",
      "Epoch[171] Test Loss: 0.4302321917930921 | Test Acc: 95.53666666666666\n",
      "Epoch[181] Train Loss: 0.14167648673351008 | Train Acc: 95.51833333333333\n",
      "Epoch[181] Test Loss: 0.42322466419118626 | Test Acc: 95.51833333333333\n",
      "Epoch[191] Train Loss: 0.1407515215888832 | Train Acc: 95.61833333333334\n",
      "Epoch[191] Test Loss: 0.4204615517527784 | Test Acc: 95.61833333333334\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "\n",
    "model = SmallLeNet().to(device)\n",
    "\n",
    "small_lenet_adam = train_nn(\n",
    "    model, \n",
    "    F.nll_loss,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    EPOCHS,\n",
    "    Adam,\n",
    "    lr=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0] Train Loss: 2.3069560328657155 | Train Acc: 9.751666666666667\n",
      "Epoch[0] Test Loss: 6.891480123656184 | Test Acc: 9.751666666666667\n",
      "Epoch[1] Train Loss: 0.8981172030557297 | Train Acc: 71.255\n",
      "Epoch[1] Test Loss: 2.682910625688772 | Test Acc: 71.255\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.62/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mmanual_seed(\u001b[39m42\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.62/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m SmallLeNet()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.62/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m small_lenet_sps_l1_d \u001b[39m=\u001b[39m train_nn(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.62/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     model, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.62/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     F\u001b[39m.\u001b[39;49mnll_loss,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.62/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     train_loader,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.62/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     test_loader,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.62/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     EPOCHS,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.62/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     SPS2,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.62/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     preconditioner\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhutch\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.62/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     slack_method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL1\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.62/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m )\n",
      "\u001b[1;32m/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb Cell 32\u001b[0m in \u001b[0;36mtrain_nn\u001b[0;34m(model, criterion, train_loader, test_loader, epochs, optimizer_class, **optimizer_kwargs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.62/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.62/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(optimizer, SPS2):     \n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.62/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure) \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.62/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     slack \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mreplay_buffer[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mslack\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.62/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/sps2/lib/python3.10/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb Cell 32\u001b[0m in \u001b[0;36mSPS2.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.62/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_empty_precond()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.62/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreconditioner_initialized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.62/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m gnorm_square \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcalc_grad_norm(grad)\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.62/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mif\u001b[39;00m gnorm_square \u001b[39m<\u001b[39m \u001b[39m1e-13\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.62/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "\n",
    "model = SmallLeNet().to(device)\n",
    "\n",
    "small_lenet_sps_l1_d = train_nn(\n",
    "    model, \n",
    "    F.nll_loss,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    EPOCHS,\n",
    "    SPS2,\n",
    "    preconditioner=\"hutch\",\n",
    "    slack_method=\"L1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0] Train Loss: 2.3069560328657155 | Train Acc: 9.751666666666667\n",
      "Epoch[0] Test Loss: 6.891480123656184 | Test Acc: 9.751666666666667\n",
      "Epoch[1] Train Loss: 0.842130427713662 | Train Acc: 72.07166666666667\n",
      "Epoch[1] Test Loss: 2.515663506991766 | Test Acc: 72.07166666666667\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mmanual_seed(\u001b[39m42\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m SmallLeNet()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m small_lenet_sps_l2_d \u001b[39m=\u001b[39m train_nn(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     model, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     F\u001b[39m.\u001b[39;49mnll_loss,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     train_loader,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     test_loader,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     EPOCHS,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     SPS2,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     preconditioner\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhutch\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     slack_method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL2\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m )\n",
      "\u001b[1;32m/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb Cell 33\u001b[0m in \u001b[0;36mtrain_nn\u001b[0;34m(model, criterion, train_loader, test_loader, epochs, optimizer_class, **optimizer_kwargs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(optimizer, SPS2):     \n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure) \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     slack \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mreplay_buffer[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mslack\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/sps2/lib/python3.10/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb Cell 33\u001b[0m in \u001b[0;36mSPS2.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_empty_precond()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreconditioner_initialized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m gnorm_square \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcalc_grad_norm(grad)\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mif\u001b[39;00m gnorm_square \u001b[39m<\u001b[39m \u001b[39m1e-13\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "\n",
    "model = SmallLeNet().to(device)\n",
    "\n",
    "small_lenet_sps_l2_d = train_nn(\n",
    "    model, \n",
    "    F.nll_loss,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    EPOCHS,\n",
    "    SPS2,\n",
    "    preconditioner=\"hutch\",\n",
    "    slack_method=\"L2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0] Train Loss: 2.3069560328657155 | Train Acc: 9.751666666666667\n",
      "Epoch[0] Test Loss: 6.891480123656184 | Test Acc: 9.751666666666667\n",
      "Epoch[1] Train Loss: 2.300363980207623 | Train Acc: 11.236666666666666\n",
      "Epoch[1] Test Loss: 6.871787940875 | Test Acc: 11.236666666666666\n",
      "Epoch[11] Train Loss: 1.8784015689477014 | Train Acc: 41.638333333333335\n",
      "Epoch[11] Test Loss: 5.611276024436133 | Test Acc: 41.638333333333335\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mmanual_seed(\u001b[39m42\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m SmallLeNet()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m small_lenet_sps_l1 \u001b[39m=\u001b[39m train_nn(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     model, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     F\u001b[39m.\u001b[39;49mnll_loss,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     train_loader,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     test_loader,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     EPOCHS,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     SPS2,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     preconditioner\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mnone\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     slack_method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL1\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m )\n",
      "\u001b[1;32m/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb Cell 34\u001b[0m in \u001b[0;36mtrain_nn\u001b[0;34m(model, criterion, train_loader, test_loader, epochs, optimizer_class, **optimizer_kwargs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(optimizer, SPS2):     \n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure) \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     slack \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mreplay_buffer[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mslack\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/sps2/lib/python3.10/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb Cell 34\u001b[0m in \u001b[0;36mSPS2.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m grad \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgrad(loss, weights, create_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreconditioner:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.135/home/farshed.abdukhakimov/projects/sps2/notebooks/sps2.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreconditioner_initialized:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "\n",
    "model = SmallLeNet().to(device)\n",
    "\n",
    "small_lenet_sps_l1 = train_nn(\n",
    "    model, \n",
    "    F.nll_loss,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    EPOCHS,\n",
    "    SPS2,\n",
    "    preconditioner=\"none\",\n",
    "    slack_method=\"L1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35d98a01b571b9c5855d990661b06f31354de19c91463a2a0e2c023e458877c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
