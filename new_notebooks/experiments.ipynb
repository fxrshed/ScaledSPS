{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "from momo import Momo\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import get_dataset    \n",
    "from loss_functions import logreg\n",
    "from utils import solve\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/farshed.abdukhakimov/projects/sps2/datasets\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(0)\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "# training \n",
    "EPOCHS = 5\n",
    "loss_function = logreg\n",
    "\n",
    "train_data, train_target = get_dataset(name=\"mushrooms\", batch_size=batch_size, percentage=1.0, scale=0)\n",
    "train_data = train_data.to(torch.get_default_dtype())\n",
    "train_target = train_target.to(torch.get_default_dtype())\n",
    "train_load = data_utils.TensorDataset(train_data, train_target)\n",
    "train_dataloader = DataLoader(train_load, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_optimizer(optimizer, lr):\n",
    "    # parameters\n",
    "    w = torch.zeros(train_data.shape[1], device=device).requires_grad_()\n",
    "    opt = optimizer([w], lr=lr)\n",
    "\n",
    "    # save loss and grad size to history\n",
    "    hist = []\n",
    "    loss = loss_function(w, train_data.to(device), train_target.to(device))\n",
    "    g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "    print(f\"Loss: {loss.item()} | GradNorm^2: {(torch.linalg.norm(g) ** 2 ).item()}\")\n",
    "    hist.append([loss.item(), (torch.linalg.norm(g) ** 2).item()])\n",
    "\n",
    "    def compute_loss(w, data, target):\n",
    "        loss = loss_function(w, data, target)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    for step in range(EPOCHS):\n",
    "        for i, (batch_data, batch_target) in enumerate(train_dataloader):\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_target = batch_target.to(device)\n",
    "            opt.zero_grad()\n",
    "            if isinstance(opt, Momo):\n",
    "                closure = lambda: compute_loss(w, batch_data, batch_target)\n",
    "                opt.step(closure=closure)\n",
    "            else:\n",
    "                loss = compute_loss(w, batch_data, batch_target)\n",
    "                opt.step()\n",
    "\n",
    "        loss = loss_function(w, train_data.to(device), train_target.to(device))\n",
    "        g, = torch.autograd.grad(loss, w, create_graph=True)\n",
    "        print(f\"Epoch: [{step}/{EPOCHS}] | Loss: {loss.item()} | GradNorm^2: {(torch.linalg.norm(g) ** 2 ).item()}\")\n",
    "        hist.append([loss.item(), (torch.linalg.norm(g) ** 2).item()])\n",
    "\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931471805599453 | GradNorm^2: 6.057377027322154\n",
      "Epoch: [0/5] | Loss: 0.0067412308688859415 | GradNorm^2: 0.0005006705468246634\n",
      "Epoch: [1/5] | Loss: 0.003482427450427738 | GradNorm^2: 0.00013121430252664953\n",
      "Epoch: [2/5] | Loss: 0.0023583187148119174 | GradNorm^2: 5.954730152242412e-05\n",
      "Epoch: [3/5] | Loss: 0.0017869995997133105 | GradNorm^2: 3.394032275300847e-05\n",
      "Epoch: [4/5] | Loss: 0.0014403064839403094 | GradNorm^2: 2.1924316228493653e-05\n"
     ]
    }
   ],
   "source": [
    "hist_sgd = train_optimizer(SGD, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931471805599453 | GradNorm^2: 6.057377027322154\n",
      "Epoch: [0/5] | Loss: 0.001048931754401928 | GradNorm^2: 1.1569714907356672e-05\n",
      "Epoch: [1/5] | Loss: 0.0004181265076955042 | GradNorm^2: 1.7957530172969797e-06\n",
      "Epoch: [2/5] | Loss: 0.0002672548630970547 | GradNorm^2: 7.255459723659826e-07\n",
      "Epoch: [3/5] | Loss: 0.00019798505709332553 | GradNorm^2: 3.952586921289689e-07\n",
      "Epoch: [4/5] | Loss: 0.0001574049080086827 | GradNorm^2: 2.4846185995680393e-07\n"
     ]
    }
   ],
   "source": [
    "hist_momo = train_optimizer(Momo, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sps2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
